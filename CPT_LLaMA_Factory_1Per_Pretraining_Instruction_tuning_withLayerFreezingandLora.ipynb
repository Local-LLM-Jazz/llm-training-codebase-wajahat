{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDSWCksGyd-C"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5fpFiF0myfT",
        "outputId": "025b737e-0f77-4def-ccaa-74e4afe52a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/'\n",
            "/home/ai-research-lab/MCSProjectbyWajahatAliBasharat\n"
          ]
        }
      ],
      "source": [
        "# %cd /content/\n",
        "# %rm -rf LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr16vtixyXn1",
        "outputId": "c293473c-8c7a-461f-9d2b-40b941644a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 360, done.\u001b[K\n",
            "remote: Counting objects: 100% (360/360), done.\u001b[K\n",
            "remote: Compressing objects: 100% (279/279), done.\u001b[K\n",
            "^Cceiving objects:  11% (43/360), 1.85 MiB | 61.00 KiB/s   \n",
            "[Errno 2] No such file or directory: 'LLaMA-Factory'\n",
            "/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/                      \u001b[01;34mevaluation\u001b[0m/             MANIFEST.in       \u001b[01;34mscripts\u001b[0m/\n",
            "CITATION.cff                 \u001b[01;34mexamples\u001b[0m/               pyproject.toml    setup.py\n",
            "CPT_LayerFreezing_1per.json  LICENSE                 README.md         \u001b[01;34msrc\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/                        \u001b[01;34mllama3-3b_freeze_1per\u001b[0m/  README_zh.md      \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdocker\u001b[0m/                      Makefile                requirements.txt\n",
            "Obtaining file:///home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (4.52.4)\n",
            "Requirement already satisfied: datasets<=3.6.0,>=2.16.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: accelerate<=1.7.0,>=0.34.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (1.7.0)\n",
            "Requirement already satisfied: peft<=0.15.2,>=0.14.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.15.2)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.21.1)\n",
            "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (5.31.0)\n",
            "Requirement already satisfied: scipy in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (1.15.3)\n",
            "Requirement already satisfied: einops in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (6.31.1)\n",
            "Requirement already satisfied: uvicorn in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.34.3)\n",
            "Requirement already satisfied: fastapi in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (2.3.6)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (3.10.3)\n",
            "Requirement already satisfied: fire in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: omegaconf in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<=2.10.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (2.10.6)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: av in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (14.4.0)\n",
            "Requirement already satisfied: librosa in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.8.14)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (2.7.1+cu128)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.22.1+cu128)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from llamafactory==0.9.4.dev0) (0.46.0)\n",
            "Requirement already satisfied: psutil in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.4.dev0) (5.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.4.dev0) (0.33.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.4.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.12.13)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.10.18)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.0.0)\n",
            "Requirement already satisfied: pydub in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.12.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (2.27.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.4.dev0) (1.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->llamafactory==0.9.4.dev0) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (14.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (1.7.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.20.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (9.7.1.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.13.0.11)\n",
            "Requirement already satisfied: triton==3.3.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from triton==3.3.1->torch>=2.0.0->llamafactory==0.9.4.dev0) (78.1.1)\n",
            "Requirement already satisfied: certifi in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fire->llamafactory==0.9.4.dev0) (3.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (0.61.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (1.7.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (5.2.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.3.8)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.22)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=27649 sha256=147cccadd03ef139d7b1fdb4358fd43aa3b8b7f527f3d140e4c750a2a1747fd3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kp8sk4xf/wheels/cf/a9/0c/32cd471f87066cab45c5f9dfe72e5de5820c26f8a450cb49e1\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.4.dev0\n",
            "    Uninstalling llamafactory-0.9.4.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.4.dev0\n",
            "Successfully installed llamafactory-0.9.4.dev0\n",
            "Requirement already satisfied: datasets in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Ignoring transformers: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.52.4)\n",
            "Requirement already satisfied: datasets<=3.6.0,>=2.16.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: accelerate<=1.7.0,>=0.34.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: peft<=0.15.2,>=0.14.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.15.2)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.21.1)\n",
            "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (5.31.0)\n",
            "Requirement already satisfied: scipy in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.15.3)\n",
            "Requirement already satisfied: einops in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (6.31.1)\n",
            "Requirement already satisfied: uvicorn in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.34.3)\n",
            "Requirement already satisfied: fastapi in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.3.6)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.10.3)\n",
            "Requirement already satisfied: fire in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.7.0)\n",
            "Requirement already satisfied: omegaconf in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (2.3.0)\n",
            "Requirement already satisfied: packaging in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (25.0)\n",
            "Requirement already satisfied: pyyaml in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (1.26.4)\n",
            "Requirement already satisfied: pydantic<=2.10.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2.10.6)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (2.3.0)\n",
            "Requirement already satisfied: av in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (14.4.0)\n",
            "Requirement already satisfied: librosa in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.8.14)\n",
            "Requirement already satisfied: filelock in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (0.33.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: psutil in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (5.9.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (2.7.1+cu128)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (3.10.18)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (11.0.0)\n",
            "Requirement already satisfied: pydub in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (4.12.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (15.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<=2.10.6->-r requirements.txt (line 23)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<=2.10.6->-r requirements.txt (line 23)) (2.27.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 24)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 24)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 24)) (2025.2)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 27)) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 27)) (14.0.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 27)) (1.7.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (3.12.13)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (1.1.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (1.5.4)\n",
            "Requirement already satisfied: h11>=0.8 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 14)) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.2.3)\n",
            "Requirement already satisfied: termcolor in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (3.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from omegaconf->-r requirements.txt (line 19)) (4.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (0.61.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (1.7.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (5.2.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 26)) (1.1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 3)) (1.20.1)\n",
            "Requirement already satisfied: certifi in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 26)) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from pooch>=1.1->librosa->-r requirements.txt (line 26)) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 24)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 27)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 27)) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 27)) (0.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 26)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 26)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 26)) (2.22)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (9.7.1.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (1.13.0.11)\n",
            "Requirement already satisfied: triton==3.3.1 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from triton==3.3.1->torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (78.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<=1.7.0,>=0.34.0->-r requirements.txt (line 4)) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip3 install -e .[torch,bitsandbytes]\n",
        "!pip3 install datasets\n",
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFrMN3nOmyfV",
        "outputId": "fbacb358-ef6d-4ad7-9323-8dd9ba90a980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA version: 12.8\n",
            "CUDA available: True\n",
            "Device name: NVIDIA GeForce RTX 4090\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA version:\", torch.version.cuda)           # e.g. '12.1'\n",
        "print(\"CUDA available:\", torch.cuda.is_available())  # should be True\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0)) # e.g. 'NVIDIA GeForce RTX 4090'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "1c5407a70de8419693df1d22d5c8bdbb",
            "fb79b552a71f460c890e843a8b692fff",
            "62fa9b71e2e548b3a4cced2ff123aaac",
            "802dbb4314f246218d59a6fd4863d4bc",
            "f8d6e5dd15cb462da8d97c99dcd13fb9",
            "4cbc9094d6744182ab2ea61740393088",
            "c20cbf9318794caa97433b70a49a723b",
            "7984b2e1c4e04fb38a23bd06c6b920ef",
            "ff4c6269b65f4433906a447e5d420c3a",
            "a713851005ff41a8948d03631ac19577",
            "8af622609f484e9499e85e3f7acfe2c6",
            "116c038fc0764a118b3381aac5e0828d",
            "34959170cecd47ba945da1d7fd052e82",
            "c682d4a8f72f409c8be7343fb8599dcd",
            "ac45a9297c2a4c618cd5c8562faabdfc",
            "1e294998453a46eba8ae9382fd63e4ee",
            "c7239048f09e42b9b6ac6b45a9f50d8b",
            "c0af717f5e7048079f07f59c76288646",
            "17639ae42259456dbca3ae2be58619cd",
            "fc263c7861dd4feaa6bb4e9e2080c481",
            "fd8c409c333842c08fd3765cc83f5c99"
          ]
        },
        "id": "fVsfo6nVynF3",
        "outputId": "4ac8239a-14d6-4c38-9092-1b919aa7b5db"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd8c409c333842c08fd3765cc83f5c99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "##login-info\n",
        "# !pip3 install ipywidgets\n",
        "from huggingface_hub import login, notebook_login\n",
        "notebook_login()\n",
        "# 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\n",
        "# wiki_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yilCA43-UAIK"
      },
      "source": [
        "# DataSet Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "49faa7001ac54cf5a2e6c9c92f462402"
          ]
        },
        "id": "10tQKtBYmyfc",
        "outputId": "625c8b1a-b51d-4216-fb45-49767094b53f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49faa7001ac54cf5a2e6c9c92f462402",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples written: 64078\n",
            "\n",
            "First 200 characters of the first sample:\n",
            "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typi\n",
            "\n",
            " Correct: Raw text only.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the absolute path to the 'data' directory\n",
        "data_dir = '/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data'\n",
        "# Ensure the 'data' directory exists\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Define the output file path within the 'data' directory\n",
        "# output_path = os.path.join(data_dir, 'wiki_1percent.json')\n",
        "output_path = os.path.join(data_dir, 'wiki_1percent.json')\n",
        "# Load the dataset with streaming\n",
        "dataset = load_dataset(\n",
        "    'wikimedia/wikipedia',\n",
        "    '20231101.en',\n",
        "    split='train',\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Estimate total size and compute 1%\n",
        "total_samples = dataset.info.splits['train'].num_examples\n",
        "sample_size = int(0.01 * total_samples)\n",
        "# sample_size = int(0.05 * total_samples)\n",
        "# Collect raw text samples\n",
        "text_samples = []\n",
        "for i, example in enumerate(dataset):\n",
        "    if i >= sample_size:\n",
        "        break\n",
        "    text_samples.append(example['text'])\n",
        "\n",
        "# Format as a list of {\"text\": ...}\n",
        "formatted_data = [{'text': t} for t in text_samples]\n",
        "\n",
        "# Save to JSON with UTF-8 encoding\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(formatted_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Verification\n",
        "with open(output_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    print(f'Total samples written: {len(data)}\\n')\n",
        "    first = data[0]['text'][:200]\n",
        "    print('First 200 characters of the first sample:')\n",
        "    print(first)\n",
        "    if '### Title:' in first or 'Wikipedia Article' in first:\n",
        "        print('\\n WARNING: Detected Wikipedia formattingexpected raw text only.')\n",
        "    else:\n",
        "        print('\\n Correct: Raw text only.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_y_DLhKmyfc",
        "outputId": "d034ffb0-098f-46b1-a569-130588325fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning Markdown files in: /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/Markdown/\n",
            "Collected 37 markdown documents and saved to /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/markdown_docs.jsonl\n",
            "\n",
            "First 3 lines of markdown_docs.jsonl:\n",
            "{\"text\": \"## [AS PASSED BY THE MAJLIS-E-SHOORA (PARLIAMENT)]\\n\\n## BILL\\n\\nto provide for setting up of the Pakistan International Airlines Corporation into a public limited company .\\n\\nWHEREAS it for   conversion of the Pakistan International Airlines   Corporation   into a 1984 (XLVII of 1984) and to deal with ancillary matters;\\n\\nIt is hereby enacted as follows:\\n\\n- 1 Short title, extent and commencement.- (l) This Act may be called the Pakistan International Airlines Corporation (Conversion) Act. 2016.\\n- It extends to the whole of Pakistan\\n- It shall come into force at once.\\n- 2 Definitions.- In this Act.  unless there is anything repugnant in the subject or context.\\n- (a) \\\"arrangement\\\" means an arrangement in writing between the Company and any relevant entity setting forth the terms. conditions and manner of transfer of o' Or more assets of the Company to 2 relevant entity along with the consideration for the same, which transfer is subject to be provisions of section 4;\\n- (b) \\\"assets includes all properties. rights and entitlements of every description and nature whatsoever. whether present or future. actual or contingent. and tangible or intangible, in Pakistan or elsewhere and includes but not limited to property held on trust. both movable and immovable. benefits. claims.   receivables, cash balances; documents, investments, privileges and powers;\\n- \\\"Company' mcans Pakistan International Airlines Corporation Limited incorporated under the Companies Ordinance:\\n- \\\"Companies Ordinance\\\" means the   Companies   Ordinance. 1984 (XLVII of 1984);\\n- \\\"Company request\\\" mneans a to the Federal Government to issue an order pursuant to section 4 to effect transfer to a relevant entity of specified assets in terms of the relevant arrangenment, provided nevertheless; such request may only be made once the Company has to that extent complied with the provisions of sub-section (3) of section 196 of the Companies Ordinance and the applicable code of corporate governance;\\n- \\\"conversion with all its cognate expressions means, in accordance with the provisions of this Act. the conversion of the Corporation into a Company:\\n- \\\"commencing date\\\" means the date of promulgation of this Act;\\n- h \\\"Corporation means the Pakistan International Airlines Corporation established under the Pakistan International Airlines Corporation Act. 1956 (XIX of 1956):\\n- \\\"liabilities includes all borrowings; duties. obligations; loans encumbrances of every   description and nature whatsoever in Pakistan elsewhere; whether present or future. actual or contingent, and disclosed or undisclosed; O\\n- \\\"order' means any order issued by the Federal  Government  pursuant to subsection (1 ) of section \\\"orders' shall be construed accordingly;\\n- \\\"PIAC Act\\\" means the Pakistan   International (XIX of 1956):\\n- proceedings includes any  suit. arbitration Or   administrative proceedings applications. appeals. awards, reviews or revisions filed or pending;\\n- m relevant entity' means any body corporate or company owned or controlled by the Federal Government or the Federal Government itself:\\n- (n) \\\"specified assets' means the assets specified in the relevant arrangements;\\n- 'undertakings ' include all projects; ventures and operations undertaken by the Corporation; individually or collectively . in collaboration with some other person; and\\n- \\\"validity period\\\" means   the period   starting   from the  commencing date   and ending on the second anniversary of the commencing date.. or on such earlier date as may be notified by the Federal Government in the official Gazette.\\n- 3 Conversion of Corporation into a Company - (i) The Corporation shall be deemed to have been converted into a company limited by shares with effect from the commencing date public\\n- 2) As and from the commending date,\\n- (a) the Company shall be deemed to hold and own all assets and liabilities of the Corporation  without any conveyance; alienation assignment  and withou. any further act. deed or registration and without   discharging Or invalidating any contract; and\\n- (b without prejudice to the generality of the foregoing clause; the Company shall;\\n- be entitled [0 the benefit of all notitications. licenses.   permissions, sanctions;   authorizations.  concessions;   decrees, orders and benefits whatsoever issued or granted in favour of the Corporation as on the commencing date, including but not limited to the permission connected with the of the securities of the Corporation on the relevant stock exchanges; and listing\\n- (ii) be deemed to have taken over and shall be entitled to enforce, all rights, licenses; and concessions and to have assumed all liabilities of the Corporation and shall be liable to pay and discharge all liabilities of every description and nature whatsoever of the Corporation. grants\\n- (3 The shareholders of the Company shall be deemed without any fresh issuance of shares   to own and hold the same number of fully shares   with such rights   and privileges (including as to class; kind and face value) as owned and Company shall be deemed to be equivalent to the authorized of the Corporation as on the commencing date and no fee or charges shall be payable in this regard. paid they capital\\n- All   proceedings of every description and nature   whatsoever by Or against  or relating to the Corporation pending on the commencing date in any court; tribunal, or other authority shall be continued, defended, prosecuted and enforced by or against or relating to the Corporation; and the same shall not abate, be discontinued, prejudiced or otherwise affected by the provisions of this Act.\\n- (5 The Company shall be deemed to be the successor-in-interest of the Corporation; and the name of the Company shall be deemed to have been substituted for the name of the   Corporation of attorney , consents;   undertakings; leases, grants, concessions, records of Central or documents of every description and nature   whatsoever relating to the Corporation and no objection shall be entertained by any court; tribunal or authority in regard to such substitution or on the ground that any such contract; agreement or document as aforesaid was, or with; the name of the Corporation and not the Company.\\n- (6) All  employees of the Corporation   shall be deemed to be employees of the Company on the same remuneration and other conditions of service, rights and privileges including but not  limited to the provisions as to their pension; provident   fund and gratuity; as the case may and other matters as were applicable to them before the conversion; including all existing retirement benefits of the employees whether funded or non-funded: be,\\n\\n## Provided that\\n\\n- Notwithstanding anything contained in this Act or nay other law; Or any decision of any court or tribunal, the employees of the Company shall continue to be govemed by non-statutory contractual terms, conditions, rules   and regulations which shall not   acquire;, Or be deemed to have acquired or be treated as having acquired, statutory status;\\n- No person deemed to be employed by the Company under this section shall be entitled to any compensation or benefit as a consequence of the conversion of the Corporation into a Company;\\n- (iii) The salaries; emoluments and all other terms of service of employees; whether permanent or contractual, shall not be changed to their disadvantage; and\\n- (iv) Pensions and other of the Corporation to   retired employees shall not be changed to their disadvantage.\\n- Notwithstanding the provisions of section 146 of the Companies Ordinance, the Company shall, upon conversion; continue all business and   undertakings of the Corporation as were carried on immediately to the commencing date. being prior\\n- 4 Power to pass orders for the transfer of assets.- (1) During the validity period and subject to a transfer of specified assets to a relevant entity substantially on the terms set forth in the relevant arrangement. prior\\n- (2) The orders shall binding on the Company; the relevant and any other person having any right; claim or liability in relation to the Company or any relevant entity . entity\\n- (3) As and from the date specified in the order, the specified assets shall, by virtue and to the extent provided in the relevant order, stand transferred and vest in, the relevant without nay conveyance; alienation or assignment and without any further act, deed Or registration and without discharging or invalidating any contract, and be subject to the terms of the relevant order in all cases. to, entity ,\\n- Representation on the Board of Directors and all other rights and privileges of shareholders of the Company; or any of its subsidiary companies carrying on air-transport business; shall be proportionate to their share-holding.\\n- Explanation: - Management control of the Company and any of its subsidiary companies in the above circumstances shall continue to vest in the majority shareholder; which shall be the Federal Government and whose share shall not be less than one percent. fifty\\n- 5 The Federal Government shall carry out or cause to be carried out valuation of the assets of the Company, and its subsidiary companies carrying on air-transport business, by a recognized valuator before transferring any shares of these companies to a third party .\\n- The Public Procurement Regulatory Authority Ordinance; 2002 (Ordinance XXII of 2002) and rules framed thereunder; as presently applicable, shall continue to apply to all transactions under this Act.\\n- 5 Guarantees to remain in force Notwithstanding the repeal of the PIAC Act; all guarantees   given by the Federal Government to any   person;   including   foreign or local institutions; to secure any of the liabilities of the Corporation shall remain in full force and affect as though they were given on behalf of the Company.\\n- 6 The Federal Government may, by notification in the Official Gazette, waive any duty, fee or any other charge that may be under any Federal law for the time in force. payable tax, being\\n- 7 Name and Headquarters of Company - (1) The name of the Company shall not be changed without the consent; in writing; of the Federal Government.\\n- (2) The Headquarters of the Company and any of its subsidiary companies carrying on air-transport business shall be at Karachi.\\n- 8 No or Neither the conversion nor the transfer of any asset of the Company through an order shall given rise to any or loss under the Income Tax Ordinance; 2001 (XLXof 2001) loss gain gain\\n- 9 Act to override - The provisions of this Act and the orders issued hereunder shall have effect notwithstanding anything to the contrary contained in any other law for the time in force. being\\n- 10. difficulty arises during the validity in giving effect to any provision of this Act; the Federal Government may, be notification in the official Gazette; make such provisions as may appear to it to be necessary for the purpose of removing the difficulty . period\\n- 11 Repeal - (1) The PIAC Act is hereby repealed.\\n- On repeal of PIAC Act under sub-section (l), nothing contained in the said Act shall be applicable to the Company; its shareholders Or any other person that may have had interest in the Corporation immediately to the conversion. prior\"}\n",
            "{\"text\": \"<!-- image -->\\n\\nTerminal-1 JIAP , Karachi-75200 Tel: (92-21) 9924-2033 Fax: (92-21) 9924-2032\\n\\n## HEADQUARTERS PAKISTAN CIVIL AVIATION AUTHORITY\\n\\ndairtransport@caapakistan com pk\\n\\nAuqust 30 2021\\n\\n## IMPLEMENTATION OF STANDARD OPERATING PROCEDURES PRIVATEAIRCRAFFLGHTS\\n\\nReference our letter of even number dated August 13, 2021 containing Categorized   Country Lists and   guidelines   concerning inbound travel to Pakistan.\\n\\n- 2. The instructions contained in the above-mentioned letter are hereby extended till September 30, 2021. Consequenlly; all Pakistanis whose return to Pakistan from Category C Countries is scheduled till September 30, 2021, may travel to Pakistan without grant of a special exemption and while in possession of valid negative PCR Test Result conducted within the 72 hours to commencement of travel to Pakistan; subject to conformance with all other applicable rules regulations . Inbound travel to Pakistan from Category  Countries for all other passengers will continue to remain banned. prior\\n- 3. or all passengers arriving in Pakistan will, however, be subjected to and Quarantine stipulations as per procedure in-vogue. Any Testing\\n- 4 All concerned are directed to ensure strict compliance.\\n\\n<!-- image -->\\n\\n(IRFANSABIR)\\n\\nAirCommodore (Retd)\\n\\nDirector AT &amp; ER\\n\\nAll Scheduled and Charter Airline Operators All Ground Handling Agents All Private Operators All Authorized Flight Permission Agents\\n\\n## Copyto:\\n\\n- Additional Deputy DGCAA (Reg) HQCAA; Karachi\\n- Joint Secretary II, Aviation Division; Islamabad\\n- Director Airport Services, HQCAA; Karachi\\n- All Chief Operating OfficerslAirport Managers\\n- PSO to DGCAA, HQCAA, Karachi\\n- Director Operations; Airports Security Force, ASF HQ; Karachi\"}\n",
            "{\"text\": \"<!-- image -->\\n\\n## HEADQUARTERS PAKISTAN CIVIL AVIATION AUTHORITY\\n\\nTerminal-1 JIAP , Karachi-75200\\n\\nTel: (92-21) 9924-2033\\n\\nFax: (92-21) 9924-2032\\n\\ndairtransport @caapakistan compk\\n\\nJuly 27 2021\\n\\n## PRIVATE AIRCRAFT FLIGHTS\\n\\nReference is made to our letters of even number dated June 12 and July 12, 2021 providing guidelines on inbound travel to Pakistan from Categorized Country Lists .\\n\\n- 2 The instructions and guidelines coriained in our above-referred letters are hereby extended upto August 31, 2021\\\\_ Consequently, all Pakistanis whose return to Pakistan from Category C Countries is scheduled till August 38, 2021 may travel to Pakistan without grant of a special exemption and while in possession of a valid negative PCR Test Result conducted within the 72 hours to commencement of travel to Pakistan; subject to conformance with all other applicable rules / regulations. Inbound travel to Pakistan from Category C Countries for all other passengers will continue to remain banned. prior\\n- 3 or all passengers arriving in Pakistan will, however; be subjected to Testing and Quarantine stipulations as per procedure in-vogue. Any\\n- 4 All concerned are directed t0 ensure strict compliance.\\n\\n(IRFAN SABIR)\\n\\nAir Comhodore (Retd.)\\n\\nDirector AT &amp; ER\\n\\n<!-- image -->\\n\\nAll Scheduled and Charter Air Carriers All Private Operators All Authorized Flight Permission Agents All Ground Handling Agents\\n\\n## Copy to:\\n\\n- Joint Secretary II, Aviation Division; Islamabad\\n- PSO to DG CAA; HQCAA, Karachi\\n- Additional Deputy DGCAA, HQCAA, Karachi\\n- All Chief Operating Officers/Airport Managers\\n- Director Operations, ASF, ASF HQ; Karachi\\n\\n<!-- image -->\\n\\n## HEADQUARTERS PAKISTAN CIVIL AVIATION AUTHORITY\\n\\nTerminal-1 JIAP , Karachi-75200\\n\\nTel: (92-21) 9924-2033\\n\\nFax: (92-21) 9924-2032\\n\\ndairtransport @caapakistan compk\\n\\nRef: HQCAA10895O1AINR\\n\\nJuly 12 2021\\n\\n## IMPLEMENIATION OF STANDARD OPERATING PROCEDURES PRIVATE AIRCRAFT FLIGHTS\\n\\nReference is made to our letter of even number dated June 12 and 29, 2021  providing  guidelines on inbound travel to Pakistan from Categorized Country Lists.\\n\\n- 2 The instructions and guidelines coriained in our above-referred letters are hereby extended upto July 31, 2021. Consequently; all Pakistanis whose return to Pakistan from Category C Countries is scheduled till July 31, 2021 may travel to Pakistan wilhout grant of a special   exemption and while in possession of a valid negative PCR Test Result conducted within the 72 hours to commencement of travel to Pakistan, subject to conformance with all other applicable rules / regulations. 'bound travel to Pakistan from Category  Countries for all other passengers will continue to remain banned. prior\\n- 3 or all passengers arriving in Pakistan will, however; be subjected t0 Testing and Quarantine stipulations as per procedure in-vogue. Any\\n- 4 All concerned are directed t0 ensure strict compliance\\n\\nAir Copimodore\\n\\n(IRFAN SABIR) (Retd:)\\n\\nDirector AT &amp; ER\\n\\n<!-- image -->\\n\\nAll Scheduled and Charter Air Carriers\\n\\nAll Private Operators All Authorized Flight Permission Agents All Ground Handling Agents\\n\\n## Copy\\n\\n- Joint Secretary II, Aviation Division; Islamabad\\n- PSO to DG CAA; HQCAA; Karachi\\n- Additional Deputy DGCAA, HQCAA, Karachi\\n- All Chief Operating Officers/Airport Managers\\n- Director Operations; ASF, ASF HQ, Karachi\\n\\n<!-- image -->\\n\\n## HEADQUARTERS PAKISTAN CIVIL AVIATION AUTHORITY\\n\\nTerminal-1 JIAP , Karachi-75200\\n\\nTel: (92-21) 9924-2033\\n\\nFax: (92-21) 9924-2032\\n\\ndairtransport@caapakistan com pk\\n\\nJune 29 2021\\n\\n## IMPLEMENTATION OF STANDARD OPERATING PROCEDURES PRIVATE AIRCRAFTFLIGHIS\\n\\nReference our letter of even number dated June 12, 2021 containing Categorized   Country Lists and guidelines   concerning inbound travel to Pakistan.\\n\\n- 2 The instructions and guidelines contained in our above-referred letter are hereby extended upto July 15, 2021. Consequently; all Pakistanis may return to Pakistan from Category C Countries upto July 15, 2021 while in possession of valid negative PCR Test Result conducted within the 72 hours to commencement of travel to Pakistan; subject to conformance with all other applicable rules regulations. Inbound travel t0 Pakistan from Category  Countries for all other passengers will continue to remain banned. prior\\n- 3 or all passengers arriving in Pakistan will, however; be subjected to Testing and Quarantine stipulations as per procedure in-vogue. Any\\n- 4 All concerned are directed to ensure strict compliance.\\n\\n<!-- image -->\\n\\nAll Scheduled and Charter Airline Operators All Ground Handling Agents All Private Operators All Authorized Flight Permission Agents\\n\\n- Joint Secretary II, Aviation Division; Islamabad\\n- Director Security, HQCAA, Karachi\\n- Additional Deputy DGCAA (Reg), HQCAA; Karachi\\n- PSO to DGCAA, HQCAA, Karachi\\n- All Chief Operating OfficerslAirport Managers\\n\\n<!-- image -->\\n\\n## HEADQUARTERS PAKISTAN CIVIL AVIATION AUTHORITY\\n\\nTerminal-1 JIAP , Karachi-75200\\n\\nTel: (92-21) 9924-2033\\n\\nFax: (92-21) 9924-2032\\n\\ndairtransport@caapakistan com pk\\n\\nJune 12 2021\\n\\n## IMPLEMENTATION OF STANDARD OPERATING PROCEDURES PRIVATE AIRCRAFTFLIGHIS\\n\\nReference our   letter of even number  dated 22 and 23, 2021 containing Categorized Country Lists and guidelines concerning inbound travel to Pakistan. May\\n\\n- 2 Revised Category C Country List duly approved by the Competent Authority is enclosed herewith and will become effective immediately. Accordingly; travel to Pakistan from Category C Countries will   continue to remain banned and only allowed subject to grant of special exemption from the NCOC Exemptions Committee. being\\n- 3. Notwithstanding the foregoing; all Pakistanis who have scheduled return to Pakistan from Category C Countries in the month of June; 2021 will be allowed to travel to Pakistan and will be exempted from the inbound travel ban placed on Category C Countries. Such passengers will; however be subjected to Testing and Quarantine stipulations upon arrival in Pakistan as per procedure in-vogue.\\n- 4 . All concerned are directed to ensure strict compliance.\\n\\n<!-- image -->\\n\\nAll Scheduled and Charter Airline Operators All Ground Handling Agents All Private Operators All Authorized Flight Permission Agents\\n\\n## Copy\\n\\n- Additional Deputy DGCAA (Reg) HQCAA, Karachi\\n- Joint Secretary II, Aviation Division; Islamabad\\n- Director Security; HQCAA, Karachi\\n- All Chief Operating OfficerslAirport Managers\\n- PSO t0 DGCAA, HQCAA, Karachi\\n- Director Operations, Airports Security Force; ASF HQ; Karachi\\n\\n## UPDATED LISI JUNE 12 2021 VALID WITH IMMEDIATEEFFECI\\n\\nINTERNATIONAL TRAVELLERS FROM ANY COUNTRY ARE REQUIRED TO POSSESS VALID NEGATIVE PCR TEST CERTIFICATION CONDUCTED WITHIN THE 72 HOURS PRIOR TO COMMENCEMENT OF TRAVEL TO PAKISTAN\\n\\n## CATEGORYC\\n\\nINTERNATIONAL TRAVEL TO PAKISTAN FROM CATEGORY c COUNTRIES MENTIONED BELOW IS BANNED AND ONLY ALLOWED SUBJECT TO EXEMPTION BY COMMITTEE AS PER PROCEDURE IN VOGUE\\n\\n|   Sr . | Country            |\\n|--------|--------------------|\\n|     01 | Arqentina          |\\n|     02 | Bangladesh         |\\n|     03 | Bhutan             |\\n|     04 | Bolivia            |\\n|     05 | Brazil             |\\n|     06 | Chile              |\\n|     07 | Colombia           |\\n|     08 | Costa Rica         |\\n|     09 | Dominican Republic |\\n|     10 | Ecuador            |\\n|     11 | India              |\\n|     12 | Indonesia          |\\n|     13 | Iran               |\\n|     14 | Iraq               |\\n|     15 | Maldives           |\\n|     16 | Mexico             |\\n|     17 | Namibia            |\\n|     18 | Nepal              |\\n|     19 | Paraquay           |\\n|     20 | Peru               |\\n|     21 | Philippines        |\\n|     22 | South Africa       |\\n|     23 | Sri Lanka          |\\n|     24 | Trinidadand Tobago |\\n|     25 | Tunisia            |\\n|     26 | Uruquay            |\\n\\nNote: Pakistani whose return from Category C countries is scheduled in June 2021 have been excluded from exemption process. However, will undergo already emplaced inbound testing / Quarantine protocols. they\"}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the directory containing your Markdown files\n",
        "# --- MAKE SURE THIS PATH IS EXACTLY CORRECT ---\n",
        "markdown_dir = '/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/Markdown/'\n",
        "\n",
        "# Define the output JSONL file path within the 'data' directory\n",
        "# (It's good practice to keep processed data directly in 'data' or a 'processed_data' subfolder)\n",
        "output_jsonl_path = '/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/markdown_docs.jsonl'\n",
        "\n",
        "# Ensure the parent directory for output_jsonl_path exists\n",
        "os.makedirs(os.path.dirname(output_jsonl_path), exist_ok=True)\n",
        "\n",
        "\n",
        "# Collect text from Markdown files\n",
        "markdown_samples = []\n",
        "# Check if the markdown_dir exists\n",
        "if not os.path.exists(markdown_dir):\n",
        "    print(f\"Error: Markdown directory not found at {markdown_dir}\")\n",
        "else:\n",
        "    print(f\"Scanning Markdown files in: {markdown_dir}\")\n",
        "    for filename in os.listdir(markdown_dir):\n",
        "        if filename.endswith(\".md\"):\n",
        "            filepath = os.path.join(markdown_dir, filename)\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                    if content.strip(): # Only add non-empty files after stripping whitespace\n",
        "                        markdown_samples.append({'text': content})\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filepath}: {e}\")\n",
        "\n",
        "# Save to JSONL\n",
        "if markdown_samples:\n",
        "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in markdown_samples:\n",
        "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
        "    print(f\"Collected {len(markdown_samples)} markdown documents and saved to {output_jsonl_path}\")\n",
        "\n",
        "    # Optional: Verify first few lines\n",
        "    print(\"\\nFirst 3 lines of markdown_docs.jsonl:\")\n",
        "    with open(output_jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= 3:\n",
        "                break\n",
        "            print(line.strip())\n",
        "else:\n",
        "    print(f\"No Markdown files found or processed in {markdown_dir}. Check the directory and file extensions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HTUZpZ0myfd",
        "outputId": "2a5fb1d7-6c77-4384-b364-e68dad31c242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded existing /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/dataset_info.json successfully.\n",
            "\n",
            "Successfully updated /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/dataset_info.json with 'wiki_1percent' and 'markdown_docs' entries.\n",
            "\n",
            "--- Current content of dataset_info.json ---\n",
            "{\n",
            "  \"identity\": {\n",
            "    \"file_name\": \"identity.json\"\n",
            "  },\n",
            "  \"alpaca_en_demo\": {\n",
            "    \"file_name\": \"alpaca_en_demo.json\"\n",
            "  },\n",
            "  \"alpaca_zh_demo\": {\n",
            "    \"file_name\": \"alpaca_zh_demo.json\"\n",
            "  },\n",
            "  \"glaive_toolcall_en_demo\": {\n",
            "    \"file_name\": \"glaive_toolcall_en_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"tools\": \"tools\"\n",
            "    }\n",
            "  },\n",
            "  \"glaive_toolcall_zh_demo\": {\n",
            "    \"file_name\": \"glaive_toolcall_zh_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"tools\": \"tools\"\n",
            "    }\n",
            "  },\n",
            "  \"mllm_demo\": {\n",
            "    \"file_name\": \"mllm_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"images\": \"images\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"mllm_audio_demo\": {\n",
            "    \"file_name\": \"mllm_audio_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"audios\": \"audios\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"mllm_video_demo\": {\n",
            "    \"file_name\": \"mllm_video_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"videos\": \"videos\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"mllm_video_audio_demo\": {\n",
            "    \"file_name\": \"mllm_video_audio_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"videos\": \"videos\",\n",
            "      \"audios\": \"audios\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"alpaca_en\": {\n",
            "    \"hf_hub_url\": \"llamafactory/alpaca_en\",\n",
            "    \"ms_hub_url\": \"llamafactory/alpaca_en\",\n",
            "    \"om_hub_url\": \"HaM/alpaca_en\"\n",
            "  },\n",
            "  \"alpaca_zh\": {\n",
            "    \"hf_hub_url\": \"llamafactory/alpaca_zh\",\n",
            "    \"ms_hub_url\": \"llamafactory/alpaca_zh\"\n",
            "  },\n",
            "  \"alpaca_gpt4_en\": {\n",
            "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_en\",\n",
            "    \"ms_hub_url\": \"llamafactory/alpaca_gpt4_en\"\n",
            "  },\n",
            "  \"alpaca_gpt4_zh\": {\n",
            "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
            "    \"ms_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
            "    \"om_hub_url\": \"State_Cloud/alpaca-gpt4-data-zh\"\n",
            "  },\n",
            "  \"glaive_toolcall_en\": {\n",
            "    \"hf_hub_url\": \"llamafactory/glaive_toolcall_en\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"tools\": \"tools\"\n",
            "    }\n",
            "  },\n",
            "  \"glaive_toolcall_zh\": {\n",
            "    \"hf_hub_url\": \"llamafactory/glaive_toolcall_zh\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"tools\": \"tools\"\n",
            "    }\n",
            "  },\n",
            "  \"lima\": {\n",
            "    \"hf_hub_url\": \"llamafactory/lima\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"guanaco\": {\n",
            "    \"hf_hub_url\": \"JosephusCheung/GuanacoDataset\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/GuanacoDataset\"\n",
            "  },\n",
            "  \"belle_2m\": {\n",
            "    \"hf_hub_url\": \"BelleGroup/train_2M_CN\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/train_2M_CN\"\n",
            "  },\n",
            "  \"belle_1m\": {\n",
            "    \"hf_hub_url\": \"BelleGroup/train_1M_CN\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/train_1M_CN\"\n",
            "  },\n",
            "  \"belle_0.5m\": {\n",
            "    \"hf_hub_url\": \"BelleGroup/train_0.5M_CN\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/train_0.5M_CN\"\n",
            "  },\n",
            "  \"belle_dialog\": {\n",
            "    \"hf_hub_url\": \"BelleGroup/generated_chat_0.4M\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/generated_chat_0.4M\"\n",
            "  },\n",
            "  \"belle_math\": {\n",
            "    \"hf_hub_url\": \"BelleGroup/school_math_0.25M\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/school_math_0.25M\"\n",
            "  },\n",
            "  \"belle_multiturn\": {\n",
            "    \"script_url\": \"belle_multiturn\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"ultra_chat\": {\n",
            "    \"script_url\": \"ultra_chat\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"open_platypus\": {\n",
            "    \"hf_hub_url\": \"garage-bAInd/Open-Platypus\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/Open-Platypus\"\n",
            "  },\n",
            "  \"codealpaca\": {\n",
            "    \"hf_hub_url\": \"sahil2801/CodeAlpaca-20k\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/CodeAlpaca-20k\"\n",
            "  },\n",
            "  \"alpaca_cot\": {\n",
            "    \"hf_hub_url\": \"QingyiSi/Alpaca-CoT\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/Alpaca-CoT\"\n",
            "  },\n",
            "  \"openorca\": {\n",
            "    \"hf_hub_url\": \"Open-Orca/OpenOrca\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/OpenOrca\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"question\",\n",
            "      \"response\": \"response\",\n",
            "      \"system\": \"system_prompt\"\n",
            "    }\n",
            "  },\n",
            "  \"slimorca\": {\n",
            "    \"hf_hub_url\": \"Open-Orca/SlimOrca\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"mathinstruct\": {\n",
            "    \"hf_hub_url\": \"TIGER-Lab/MathInstruct\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/MathInstruct\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"instruction\",\n",
            "      \"response\": \"output\"\n",
            "    }\n",
            "  },\n",
            "  \"firefly\": {\n",
            "    \"hf_hub_url\": \"YeungNLP/firefly-train-1.1M\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"input\",\n",
            "      \"response\": \"target\"\n",
            "    }\n",
            "  },\n",
            "  \"wikiqa\": {\n",
            "    \"hf_hub_url\": \"wiki_qa\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"question\",\n",
            "      \"response\": \"answer\"\n",
            "    }\n",
            "  },\n",
            "  \"webqa\": {\n",
            "    \"hf_hub_url\": \"suolyer/webqa\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/webqa\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"input\",\n",
            "      \"response\": \"output\"\n",
            "    }\n",
            "  },\n",
            "  \"webnovel\": {\n",
            "    \"hf_hub_url\": \"zxbsmk/webnovel_cn\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/webnovel_cn\"\n",
            "  },\n",
            "  \"nectar_sft\": {\n",
            "    \"hf_hub_url\": \"AstraMindAI/SFT-Nectar\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/SFT-Nectar\"\n",
            "  },\n",
            "  \"deepctrl\": {\n",
            "    \"ms_hub_url\": \"deepctrl/deepctrl-sft-data\"\n",
            "  },\n",
            "  \"adgen_train\": {\n",
            "    \"hf_hub_url\": \"HasturOfficial/adgen\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/adgen\",\n",
            "    \"split\": \"train\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"content\",\n",
            "      \"response\": \"summary\"\n",
            "    }\n",
            "  },\n",
            "  \"adgen_eval\": {\n",
            "    \"hf_hub_url\": \"HasturOfficial/adgen\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/adgen\",\n",
            "    \"split\": \"validation\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"content\",\n",
            "      \"response\": \"summary\"\n",
            "    }\n",
            "  },\n",
            "  \"sharegpt_hyper\": {\n",
            "    \"hf_hub_url\": \"totally-not-an-llm/sharegpt-hyperfiltered-3k\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"sharegpt4\": {\n",
            "    \"hf_hub_url\": \"shibing624/sharegpt_gpt4\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/sharegpt_gpt4\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"ultrachat_200k\": {\n",
            "    \"hf_hub_url\": \"HuggingFaceH4/ultrachat_200k\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/ultrachat_200k\",\n",
            "    \"split\": \"train_sft\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"agent_instruct\": {\n",
            "    \"hf_hub_url\": \"THUDM/AgentInstruct\",\n",
            "    \"ms_hub_url\": \"ZhipuAI/AgentInstruct\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"lmsys_chat\": {\n",
            "    \"hf_hub_url\": \"lmsys/lmsys-chat-1m\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/lmsys-chat-1m\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversation\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"evol_instruct\": {\n",
            "    \"hf_hub_url\": \"WizardLM/WizardLM_evol_instruct_V2_196k\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/WizardLM_evol_instruct_V2_196k\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"glaive_toolcall_100k\": {\n",
            "    \"hf_hub_url\": \"hiyouga/glaive-function-calling-v2-sharegpt\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"tools\": \"tools\"\n",
            "    }\n",
            "  },\n",
            "  \"cosmopedia\": {\n",
            "    \"hf_hub_url\": \"HuggingFaceTB/cosmopedia\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"prompt\",\n",
            "      \"response\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"stem_zh\": {\n",
            "    \"hf_hub_url\": \"hfl/stem_zh_instruction\"\n",
            "  },\n",
            "  \"ruozhiba_gpt4\": {\n",
            "    \"hf_hub_url\": \"hfl/ruozhiba_gpt4_turbo\"\n",
            "  },\n",
            "  \"neo_sft\": {\n",
            "    \"hf_hub_url\": \"m-a-p/neo_sft_phase2\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"magpie_pro_300k\": {\n",
            "    \"hf_hub_url\": \"Magpie-Align/Magpie-Pro-300K-Filtered\",\n",
            "    \"formatting\": \"sharegpt\"\n",
            "  },\n",
            "  \"magpie_ultra\": {\n",
            "    \"hf_hub_url\": \"argilla/magpie-ultra-v0.1\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"instruction\",\n",
            "      \"response\": \"response\"\n",
            "    }\n",
            "  },\n",
            "  \"web_instruct\": {\n",
            "    \"hf_hub_url\": \"TIGER-Lab/WebInstructSub\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"question\",\n",
            "      \"response\": \"answer\"\n",
            "    }\n",
            "  },\n",
            "  \"openo1_sft\": {\n",
            "    \"hf_hub_url\": \"llamafactory/OpenO1-SFT\",\n",
            "    \"ms_hub_url\": \"llamafactory/OpenO1-SFT\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"prompt\",\n",
            "      \"response\": \"response\"\n",
            "    }\n",
            "  },\n",
            "  \"open_thoughts\": {\n",
            "    \"hf_hub_url\": \"llamafactory/OpenThoughts-114k\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\",\n",
            "      \"system_tag\": \"system\"\n",
            "    }\n",
            "  },\n",
            "  \"open_r1_math\": {\n",
            "    \"hf_hub_url\": \"llamafactory/OpenR1-Math-94k\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\",\n",
            "      \"system_tag\": \"system\"\n",
            "    }\n",
            "  },\n",
            "  \"chinese_r1_distill\": {\n",
            "    \"hf_hub_url\": \"Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT\",\n",
            "    \"ms_hub_url\": \"liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT\"\n",
            "  },\n",
            "  \"llava_1k_en\": {\n",
            "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n",
            "    \"subset\": \"en\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"images\": \"images\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"llava_1k_zh\": {\n",
            "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n",
            "    \"subset\": \"zh\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"images\": \"images\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"llava_150k_en\": {\n",
            "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n",
            "    \"subset\": \"en\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"images\": \"images\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"llava_150k_zh\": {\n",
            "    \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n",
            "    \"subset\": \"zh\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"images\": \"images\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"pokemon_cap\": {\n",
            "    \"hf_hub_url\": \"llamafactory/pokemon-gpt4o-captions\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"images\": \"images\"\n",
            "    }\n",
            "  },\n",
            "  \"mllm_pt_demo\": {\n",
            "    \"hf_hub_url\": \"BUAADreamer/mllm_pt_demo\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"images\": \"images\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"oasst_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/oasst_de\"\n",
            "  },\n",
            "  \"dolly_15k_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/dolly-15k_de\"\n",
            "  },\n",
            "  \"alpaca-gpt4_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/alpaca-gpt4_de\"\n",
            "  },\n",
            "  \"openschnabeltier_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/openschnabeltier_de\"\n",
            "  },\n",
            "  \"evol_instruct_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/evol-instruct_de\"\n",
            "  },\n",
            "  \"dolphin_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/dolphin_de\"\n",
            "  },\n",
            "  \"booksum_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/booksum_de\"\n",
            "  },\n",
            "  \"airoboros_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/airoboros-3.0_de\"\n",
            "  },\n",
            "  \"ultrachat_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/ultra-chat_de\"\n",
            "  },\n",
            "  \"dpo_en_demo\": {\n",
            "    \"file_name\": \"dpo_en_demo.json\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\"\n",
            "    }\n",
            "  },\n",
            "  \"dpo_zh_demo\": {\n",
            "    \"file_name\": \"dpo_zh_demo.json\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\"\n",
            "    }\n",
            "  },\n",
            "  \"dpo_mix_en\": {\n",
            "    \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n",
            "    \"subset\": \"en\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\"\n",
            "    }\n",
            "  },\n",
            "  \"dpo_mix_zh\": {\n",
            "    \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n",
            "    \"subset\": \"zh\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\"\n",
            "    }\n",
            "  },\n",
            "  \"ultrafeedback\": {\n",
            "    \"hf_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n",
            "    \"ms_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n",
            "    \"ranking\": true,\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"instruction\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\"\n",
            "    }\n",
            "  },\n",
            "  \"coig_p\": {\n",
            "    \"hf_hub_url\": \"m-a-p/COIG-P\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\"\n",
            "    }\n",
            "  },\n",
            "  \"rlhf_v\": {\n",
            "    \"hf_hub_url\": \"llamafactory/RLHF-V\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\",\n",
            "      \"images\": \"images\"\n",
            "    }\n",
            "  },\n",
            "  \"vlfeedback\": {\n",
            "    \"hf_hub_url\": \"Zhihui/VLFeedback\",\n",
            "    \"ranking\": true,\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"conversations\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\",\n",
            "      \"images\": \"images\"\n",
            "    }\n",
            "  },\n",
            "  \"rlaif_v\": {\n",
            "    \"hf_hub_url\": \"openbmb/RLAIF-V-Dataset\",\n",
            "    \"ranking\": true,\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"question\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\",\n",
            "      \"images\": \"image\"\n",
            "    }\n",
            "  },\n",
            "  \"orca_pairs\": {\n",
            "    \"hf_hub_url\": \"Intel/orca_dpo_pairs\",\n",
            "    \"ranking\": true,\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"question\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\",\n",
            "      \"system\": \"system\"\n",
            "    }\n",
            "  },\n",
            "  \"hh_rlhf_en\": {\n",
            "    \"script_url\": \"hh_rlhf_en\",\n",
            "    \"ranking\": true,\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"instruction\",\n",
            "      \"chosen\": \"chosen\",\n",
            "      \"rejected\": \"rejected\",\n",
            "      \"history\": \"history\"\n",
            "    }\n",
            "  },\n",
            "  \"nectar_rm\": {\n",
            "    \"hf_hub_url\": \"AstraMindAI/RLAIF-Nectar\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/RLAIF-Nectar\",\n",
            "    \"ranking\": true\n",
            "  },\n",
            "  \"orca_dpo_de\": {\n",
            "    \"hf_hub_url\": \"mayflowergmbh/intel_orca_dpo_pairs_de\",\n",
            "    \"ranking\": true\n",
            "  },\n",
            "  \"kto_en_demo\": {\n",
            "    \"file_name\": \"kto_en_demo.json\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"messages\",\n",
            "      \"kto_tag\": \"label\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"kto_mix_en\": {\n",
            "    \"hf_hub_url\": \"argilla/kto-mix-15k\",\n",
            "    \"formatting\": \"sharegpt\",\n",
            "    \"columns\": {\n",
            "      \"messages\": \"completion\",\n",
            "      \"kto_tag\": \"label\"\n",
            "    },\n",
            "    \"tags\": {\n",
            "      \"role_tag\": \"role\",\n",
            "      \"content_tag\": \"content\",\n",
            "      \"user_tag\": \"user\",\n",
            "      \"assistant_tag\": \"assistant\"\n",
            "    }\n",
            "  },\n",
            "  \"ultrafeedback_kto\": {\n",
            "    \"hf_hub_url\": \"argilla/ultrafeedback-binarized-preferences-cleaned-kto\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/ultrafeedback-binarized-preferences-cleaned-kto\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"prompt\",\n",
            "      \"response\": \"completion\",\n",
            "      \"kto_tag\": \"label\"\n",
            "    }\n",
            "  },\n",
            "  \"wiki_demo\": {\n",
            "    \"file_name\": \"wiki_demo.txt\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"c4_demo\": {\n",
            "    \"file_name\": \"c4_demo.jsonl\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"refinedweb\": {\n",
            "    \"hf_hub_url\": \"tiiuae/falcon-refinedweb\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"content\"\n",
            "    }\n",
            "  },\n",
            "  \"redpajama_v2\": {\n",
            "    \"hf_hub_url\": \"togethercomputer/RedPajama-Data-V2\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"raw_content\"\n",
            "    },\n",
            "    \"subset\": \"default\"\n",
            "  },\n",
            "  \"wikipedia_en\": {\n",
            "    \"hf_hub_url\": \"olm/olm-wikipedia-20221220\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/olm-wikipedia-20221220\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"wikipedia_zh\": {\n",
            "    \"hf_hub_url\": \"pleisto/wikipedia-cn-20230720-filtered\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/wikipedia-cn-20230720-filtered\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"completion\"\n",
            "    }\n",
            "  },\n",
            "  \"pile\": {\n",
            "    \"hf_hub_url\": \"monology/pile-uncopyrighted\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/pile\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"skypile\": {\n",
            "    \"hf_hub_url\": \"Skywork/SkyPile-150B\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/SkyPile-150B\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"fineweb\": {\n",
            "    \"hf_hub_url\": \"HuggingFaceFW/fineweb\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"fineweb_edu\": {\n",
            "    \"hf_hub_url\": \"HuggingFaceFW/fineweb-edu\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"the_stack\": {\n",
            "    \"hf_hub_url\": \"bigcode/the-stack\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/the-stack\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"content\"\n",
            "    }\n",
            "  },\n",
            "  \"starcoder_python\": {\n",
            "    \"hf_hub_url\": \"bigcode/starcoderdata\",\n",
            "    \"ms_hub_url\": \"AI-ModelScope/starcoderdata\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"content\"\n",
            "    },\n",
            "    \"folder\": \"python\"\n",
            "  },\n",
            "  \"wiki_1percent\": {\n",
            "    \"file_name\": \"wiki_1percent.json\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  },\n",
            "  \"markdown_docs\": {\n",
            "    \"file_name\": \"markdown_docs.jsonl\",\n",
            "    \"columns\": {\n",
            "      \"prompt\": \"text\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the base directory of your LLaMA-Factory installation\n",
        "llama_factory_base_dir = '/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory'\n",
        "data_dir = os.path.join(llama_factory_base_dir, 'data')\n",
        "dataset_info_path = os.path.join(data_dir, 'dataset_info.json')\n",
        "\n",
        "# Define the new entry for your markdown data\n",
        "# This assumes you ran Step 1 and created markdown_docs.jsonl in the 'data' directory\n",
        "new_markdown_entry = {\n",
        "    \"file_name\": \"markdown_docs.jsonl\",\n",
        "    \"columns\": {\n",
        "        \"prompt\": \"text\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the updated entry for wiki_1percent\n",
        "# This ensures it has the correct 'columns' and 'formatting' for pre-training\n",
        "updated_wiki_entry = {\n",
        "    \"file_name\": \"wiki_1percent.json\",\n",
        "    \"columns\": {\n",
        "        \"prompt\": \"text\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Load existing dataset_info.json\n",
        "existing_data = {}\n",
        "if os.path.exists(dataset_info_path):\n",
        "    try:\n",
        "        with open(dataset_info_path, 'r', encoding='utf-8') as f:\n",
        "            existing_data = json.load(f)\n",
        "        print(f\"Loaded existing {dataset_info_path} successfully.\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Warning: Could not decode existing {dataset_info_path} ({e}). \"\n",
        "              \"Starting with an empty configuration, any previous entries might be lost if not valid JSON.\")\n",
        "        existing_data = {}\n",
        "else:\n",
        "    print(f\"No existing {dataset_info_path} found. Creating a new one.\")\n",
        "    # Ensure the 'data' directory exists if it's not there for some reason\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Add or update the entries\n",
        "existing_data[\"wiki_1percent\"] = updated_wiki_entry\n",
        "existing_data[\"markdown_docs\"] = new_markdown_entry\n",
        "\n",
        "# Save the combined content back to the file\n",
        "with open(dataset_info_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(existing_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nSuccessfully updated {dataset_info_path} with 'wiki_1percent' and 'markdown_docs' entries.\")\n",
        "print(\"\\n--- Current content of dataset_info.json ---\")\n",
        "with open(dataset_info_path, 'r', encoding='utf-8') as f:\n",
        "    print(f.read())\n",
        "print(\"---------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3R15oYDUkWH"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# args = dict(\n",
        "#     # model settings\n",
        "#     model_name_or_path=\"meta-llama/LLaMA-3.2-3B-Instruct\",\n",
        "#     trust_remote_code=True,\n",
        "\n",
        "\n",
        "#     # training stage\n",
        "#     stage=\"pt\",                      # pretraining / CPT\n",
        "#     do_train=True,\n",
        "#     finetuning_type=\"freeze\",       # pure freeze finetuning\n",
        "#     freeze_trainable_layers=2,        # train only the last 5 layers; freeze the rest\n",
        "#     freeze_trainable_modules= \"all\",\n",
        "#     # dataset settings\n",
        "#     # dataset=\"wiki_2percent\",\n",
        "#     dataset=\"wiki_1percent\",\n",
        "#     # dataset_config_name=\"20231101.en\",\n",
        "#     # split=\"train\",\n",
        "#     template=\"llama3\",\n",
        "#     # max_samples=1000,\n",
        "#     max_samples=1000,\n",
        "#     # max_samples=500,\n",
        "#     cutoff_len=2048,\n",
        "#     overwrite_cache=True,\n",
        "#     preprocessing_num_workers=16,\n",
        "#     dataloader_num_workers=4,\n",
        "\n",
        "#     # output and checkpointing\n",
        "#     output_dir=\"llama3-3b_freeze_1per\",\n",
        "#     save_strategy=\"steps\",\n",
        "#     save_steps=500, ## change from 1000 to 120\n",
        "#     save_total_limit=10,\n",
        "#     logging_steps=10,\n",
        "#     overwrite_output_dir=True,\n",
        "#     plot_loss=True,\n",
        "#     report_to=\"none\",\n",
        "\n",
        "#     # optimizer and schedule\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=8,\n",
        "#     learning_rate=1e-4,\n",
        "#     num_train_epochs=3.0,\n",
        "#     lr_scheduler_type=\"cosine\",\n",
        "#     warmup_ratio=0.1,\n",
        "\n",
        "#     # precision and device\n",
        "#     # bf16=True,               # Use bfloat16 precision\n",
        "#     # fp16=False,              # Disable float16\n",
        "\n",
        "#     ## use to enable 4090\n",
        "#     bf16=True,\n",
        "#     fp16=False,\n",
        "\n",
        "# # // let Accelerate pick up device placement:\n",
        "#     device_map=\"auto\",\n",
        "#     low_cpu_mem_usage=True,\n",
        "#     optim_target_device=\"cuda\",\n",
        "# # +  // remove any no_cuda or cpu_only flags\n",
        "#     optim=\"adamw_torch\",     # Use standard AdamW\n",
        "#     # optim_target_device=\"cuda\",\n",
        "#     # no_cuda=True,\n",
        "#     # eval\n",
        "#     ddp_timeout=180000000,\n",
        "#     gradient_checkpointing=True,\n",
        "#     eval_dataset=None,\n",
        "#     eval_steps=None,\n",
        "#     # launcher=\"python\",\n",
        "#     # nproc_per_node=1\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l79go5vmyfd"
      },
      "outputs": [],
      "source": [
        "# changed\n",
        "import json\n",
        "args = dict(\n",
        "    # model settings\n",
        "    model_name_or_path=\"meta-llama/LLaMA-3.2-3B-Instruct\",\n",
        "    trust_remote_code=True,\n",
        "\n",
        "    # training stage\n",
        "    stage=\"pt\",\n",
        "    do_train=True,\n",
        "    finetuning_type=\"freeze\",\n",
        "    freeze_trainable_layers=4,\n",
        "    freeze_trainable_modules=\"all\",\n",
        "\n",
        "    # dataset settings\n",
        "    dataset=\"wiki_1percent,markdown_docs\",\n",
        "    # dataset=\"wiki_2percent\",\n",
        "    # max_samples=1000,\n",
        "    cutoff_len=2048,\n",
        "    overwrite_cache=True,\n",
        "    preprocessing_num_workers=16,\n",
        "    dataloader_num_workers=4,\n",
        "\n",
        "    # output and checkpointing\n",
        "    output_dir=\"llama3-3b_freeze_1per\",\n",
        "    # output_dir=\"llama3-3b_freeze_1per_8layers\",\n",
        "    logging_steps=10,\n",
        "    overwrite_output_dir=True,\n",
        "    plot_loss=True,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # optimizer and schedule\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=0.0001,\n",
        "    num_train_epochs=1.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # precision and device\n",
        "    bf16=True,\n",
        "\n",
        "    # distributed training timeout\n",
        "    ddp_timeout=18000,\n",
        "    # deepspeed=\"ds_config.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rCLsOCOWi_T"
      },
      "source": [
        "```\n",
        "\"wiki_1percent\": {\n",
        "  \"file_name\": \"wiki_1percent.json\",\n",
        "  \"columns\": {\n",
        "    \"prompt\": \"text\"\n",
        "  }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGp5Ct6NaFer"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM\n",
        "# def verify_frozen_layers(model, freeze_trainable_layers):\n",
        "#     total_layers = len(model.model.layers)\n",
        "#     frozen_layers = total_layers - freeze_trainable_layers\n",
        "\n",
        "#     for i, layer in enumerate(model.model.layers):\n",
        "#         if i < frozen_layers:\n",
        "#             for param in layer.parameters():\n",
        "#                 assert not param.requires_grad, f\"Layer {i} should be frozen\"\n",
        "#         else:\n",
        "#             for param in layer.parameters():\n",
        "#                 assert param.requires_grad, f\"Layer {i} should be trainable\"\n",
        "\n",
        "#     print(f\"Verification complete: {frozen_layers} layers frozen, \"\n",
        "#           f\"{freeze_trainable_layers} layers trainable\")\n",
        "\n",
        "# # Example usage:\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "#                                              use_auth_token=True)\n",
        "# freeze_trainable_layers = 4\n",
        "# verify_frozen_layers(model, freeze_trainable_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vGAAWlvVbF7"
      },
      "outputs": [],
      "source": [
        "json.dump(args, open(\"CPT_LayerFreezing_1per.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "# json.dump(args, open(\"CPT_LayerFreezing_1per_8layers.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "# json.dump(args, open(\"CPT_LayerFreezing_2per.json\", \"w\", encoding=\"utf-8\"), indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bgyyx6bV1fG",
        "outputId": "f95d7d10-62f0-4899-e55f-78aef9dce759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|2025-06-19 09:39:52] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:47347\n",
            "W0619 09:39:53.216000 7610 site-packages/torch/distributed/run.py:766] \n",
            "W0619 09:39:53.216000 7610 site-packages/torch/distributed/run.py:766] *****************************************\n",
            "W0619 09:39:53.216000 7610 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0619 09:39:53.216000 7610 site-packages/torch/distributed/run.py:766] *****************************************\n",
            "[INFO|2025-06-19 09:39:55] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
            "[INFO|2025-06-19 09:39:55] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:39:57,024 >> loading file tokenizer.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:39:57,025 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:39:57,025 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:39:57,025 >> loading file special_tokens_map.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:39:57,025 >> loading file tokenizer_config.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:39:57,025 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 09:39:57,233 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:698] 2025-06-19 09:40:00,303 >> loading configuration file config.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 09:40:00,305 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:40:01,315 >> loading file tokenizer.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:40:01,315 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:40:01,315 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:40:01,315 >> loading file special_tokens_map.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:40:01,315 >> loading file tokenizer_config.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-06-19 09:40:01,315 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 09:40:01,518 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|2025-06-19 09:40:01] llamafactory.data.template:148 >> `template` was not specified, try parsing the chat template from the tokenizer.\n",
            "[INFO|2025-06-19 09:40:01] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
            "[INFO|2025-06-19 09:40:01] llamafactory.data.loader:143 >> Loading dataset wiki_1percent.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 0 examples [00:00, ? examples/s]/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank1]:[W619 09:40:03.151582821 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
            "Generating train split: 64078 examples [00:01, 35340.18 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100%|| 64078/64078 [00:00<00:00, 18\n",
            "[INFO|2025-06-19 09:40:04] llamafactory.data.loader:143 >> Loading dataset markdown_docs.jsonl...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 37 examples [00:00, 13508.81 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100%|| 37/37 [00:00<00:00, 364.65 e\n",
            "/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W619 09:40:05.376496501 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
            "Running tokenizer on dataset (num_proc=16): 100%|| 64115/64115 [00:16<00:00, 38\n",
            "training example:\n",
            "input_ids:\n",
            "[2127, 1132, 2191, 374, 264, 5054, 19675, 323, 7351, 430, 374, 44929, 315, 682, 1120, 7174, 369, 11447, 323, 26737, 311, 90376, 279, 14673, 433, 8349, 10519, 26225, 78242, 323, 30022, 11, 11383, 2737, 7140, 90160, 11, 323, 32682, 13, 1556, 1132, 2191, 28424, 369, 279, 14039, 315, 279, 1614, 449, 1614, 1752, 34775, 323, 37079, 1949, 30257, 13, 1666, 264, 35901, 2163, 29480, 7351, 11, 420, 5403, 315, 44565, 2191, 374, 9277, 389, 279, 3117, 61943, 2163, 315, 279, 5054, 20326, 11, 6118, 7633, 439, 279, 57125, 20611, 315, 279, 41289, 7351, 320, 2808, 531, 8997, 51618, 3677, 95668, 617, 12439, 304, 34775, 2085, 16287, 12694, 1132, 552, 1317, 1603, 279, 21967, 315, 5415, 11, 77563, 11, 477, 991, 19505, 13, 3161, 279, 10205, 315, 39433, 70994, 13162, 11, 67451, 42914, 9017, 11447, 1101, 16392, 13, 10541, 35483, 315, 78431, 6848, 527, 1766, 682, 6957, 3925, 11, 6617, 44565, 2191, 22763, 505, 279, 92931, 13, 12220, 279, 15629, 4376, 315, 279, 220, 777, 339, 323, 279, 1176, 11026, 315, 279, 220, 508, 339, 9478, 11, 279, 78431, 7351, 20415, 3384, 304, 1455, 5596, 315, 279, 1917, 323, 1047, 264, 5199, 3560, 304, 7487, 6, 28970, 369, 91225, 49686, 13, 40741, 78431, 8853, 315, 3463, 14454, 2391, 420, 4261, 13, 1556, 1132, 1705, 617, 4529, 961, 304, 3892, 93574, 11, 1455, 35146, 304, 279, 12366, 6947, 2957, 11, 279, 8690, 16803, 5111, 323, 279, 15506, 16803, 5111, 11, 6832, 842, 13160, 279, 842, 315, 279, 29924, 11639, 315, 44565, 2191, 13, 763, 279, 1566, 11026, 315, 279, 220, 508, 339, 323, 1139, 279, 220, 1691, 267, 9478, 11, 279, 78431, 7351, 706, 1027, 594, 86153, 3131, 810, 11, 7982, 304, 23354, 323, 10383, 2949, 7294, 98231, 380, 11, 7294, 48260, 323, 7294, 74419, 8082, 19567, 382, 2127, 1132, 1705, 3539, 17226, 20414, 11, 902, 1253, 387, 8965, 18255, 1139, 30191, 323, 41993, 15174, 26, 1070, 374, 5199, 28347, 1990, 279, 1403, 13, 38321, 661, 5528, 1456, 311, 38553, 1148, 459, 78431, 8396, 2643, 387, 1093, 11, 719, 30191, 26411, 11, 902, 617, 35901, 4529, 264, 16806, 2543, 11, 9395, 311, 63331, 11447, 323, 279, 1614, 13, 9176, 62814, 315, 3823, 36017, 617, 1027, 28160, 555, 78431, 10334, 11, 43665, 11, 323, 550, 7332, 382, 32960, 99174, 11, 57726, 11, 323, 7419, 4815, 791, 1880, 1631, 5848, 6371, 315, 44565, 2191, 374, 505, 279, 38050, 18341, 459, 847, 71, 689, 11, 7438, 330, 30096, 264, 49080, 498, 24306, 315, 279, 9436, 459, 12, 3573, 30096, 909, 323, 279, 3492, 802, 31764, 437, 3573, 38491, 1, 477, 330, 81, 8646, 1865, 578, 21166, 482, 2191, 72214, 279, 42933, 1510, 430, 9428, 2530, 459, 15630, 13, 1556, 1132, 2191, 8111, 304, 6498, 505, 220, 10513, 17, 439, 44565, 44618, 323, 459, 15630, 505, 220, 9800, 24, 26, 4216, 6498, 603, 1154, 20654, 4147, 264, 5647, 315, 19823, 13, 40741, 48752, 2949, 279, 8753, 22910, 61336, 872, 19949, 439, 93134, 11, 8051, 2478, 1778, 13487, 6222, 1690, 6325, 449, 3010, 93134, 13, 9176, 14110, 5548, 315, 279, 220, 777, 339, 9478, 1778, 439, 12656, 4359, 7678, 320, 10005, 21, 4235, 10750, 21, 8, 323, 93537, 1226, 275, 2785, 320, 5245, 23, 4235, 9674, 16, 8, 1053, 17210, 311, 279, 78431, 83258, 315, 279, 1828, 9659, 719, 1550, 539, 1005, 78431, 477, 44565, 2191, 304, 23524, 5694, 477, 872, 21463, 382, 791, 1176, 5054, 55475, 311, 1650, 5678, 459, 78431, 1754, 574, 38077, 12278, 974, 764, 393, 583, 31721, 263, 320, 5245, 24, 4235, 9714, 20, 705, 36024, 279, 16287, 7342, 315, 44565, 2191, 304, 279, 5209, 12, 777, 339, 9478, 13, 8876, 279, 220, 9378, 15, 82, 323, 7314, 304, 9822, 11, 57125, 2191, 706, 3629, 1027, 1511, 439, 264, 74450, 369, 44565, 2191, 323, 1202, 1005, 439, 264, 74450, 374, 2103, 4279, 4994, 279, 3723, 4273, 13, 4427, 603, 1154, 315, 57125, 2191, 8464, 311, 3927, 4633, 1949, 48831, 19675, 1193, 11, 323, 1949, 48831, 44565, 2191, 304, 4040, 374, 61937, 57125, 44565, 2191, 382, 8142, 279, 4751, 57125, 706, 1027, 14090, 69593, 449, 44565, 2191, 11, 1202, 7438, 706, 810, 6051, 1027, 80703, 555, 22622, 25375, 505, 2679, 30450, 85129, 5315, 11, 2737, 2225, 279, 1561, 14043, 323, 57125, 28187, 1705, 11, 889, 656, 539, 22712, 5694, 449, 59021, 3674, 1705, 477, 264, 348, 53290, 4717, 11, 323, 14560, 13042, 45750, 11, 889, 527, 15871, 11920, 449, 8431, 58455, 13, 23212, 11, 1063, 93134, 1005, 57125, 41289, 311, 5766, 44565, 2191, 596, 8389, 390, 14632, 323, 20654, 1082, 1202, 13537, 449, 51618, 13, 1556, 1132, 2191, 374, 44029, 1511, 311, 7664, 279, 7294, 43802, 20631, 20611, 315, 279, 41289, 7351, 13, 1556, 1132, 2191, 374, 13168, 291, 311, 41289, 7739, 902, 527, 1614, 36185, 477, 505, 3485, 13, 99394, 315, 44565, 2191, 8965, 11415, 44565, 2191, 596, 41289, 16792, 323, 9940, 1082, 13865, 520, 6968, 29953, 354, 316, 552, 1990, 279, 1403, 13, 4427, 31839, 7664, 44565, 2191, 439, 3515, 1690, 34453, 505, 84581, 11, 323, 1694, 2225, 18250, 323, 41289, 719, 810, 779, 13, 9176, 31839, 8007, 459, 277, 971, 98231, 2191, 439, 264, 70847, 315, 78431, 16565, 382, 8142, 14076, 311, 279, 1614, 374, 8792, 311, 78431, 3463, 11, 27409, 44565, 2191, 374, 539, 459, 4228, 3465, 369, 31839, 11, 439, 1070, 374, 264, 2763, 315, 10430, 4315, 31839, 323, 93134, 389, 279, 5030, 11, 323, 5370, 60701, 45493, 44565, 2191, 10284, 22009, 13, 17559, 36222, 3079, 5540, 2997, 279, 690, 369, 264, 2536, 23283, 3035, 535, 8396, 11, 279, 38001, 315, 279, 1614, 41705, 11, 279, 16801, 430, 3823, 7138, 6276, 12966, 311, 3073, 304, 477, 5208, 9017, 1778, 264, 2536, 23283, 3035, 535, 8396, 11, 323, 264, 24710, 389, 1268, 311, 1180, 311, 23564, 279, 10728, 315, 459, 15630, 382, 13730, 271, 4808, 17515, 944, 11639, 4815, 10438, 279, 9886, 315, 25861, 323, 9919, 11, 9749, 11447, 1550, 539, 3073, 13, 1102, 574, 1306, 279, 15244, 315, 11447, 430, 44565, 4633, 6848, 1051, 16948, 37588, 439, 264, 13010, 13, 578, 1455, 28289, 5956, 34291, 311, 44565, 2191, 304, 279, 14154, 1917, 1051, 304, 5734, 323, 25431, 13, 763, 5734, 11, 41903, 44565, 2191, 320, 1820, 10430, 389, 279, 57008, 315, 279, 1614, 8, 574, 91784, 660, 555, 60608, 380, 61787, 68844, 526, 67927, 323, 445, 3524, 8510, 13, 32944, 3002, 71883, 42914, 11, 60608, 2191, 706, 1027, 1071, 311, 617, 1047, 330, 91645, 16961, 811, 1, 315, 44565, 2191, 382, 2127, 1132, 292, 33726, 1051, 1101, 83280, 555, 28375, 5493, 323, 61787, 304, 25431, 13, 362, 60478, 4010, 355, 323, 34940, 511, 645, 1511, 279, 21849, 315, 6898, 343, 606, 311, 41468, 279, 12324, 1990, 7016, 27070, 555, 279, 1614, 323, 4443, 51360, 13, 328, 78046, 29440, 59652, 1122, 11527, 15320, 323, 29676, 389, 279, 1314, 315, 3927, 11542, 315, 42563, 13, 356, 1910, 1233, 27292, 3823, 2383, 320, 17101, 437, 8, 323, 5938, 11527, 1418, 4560, 311, 3974, 4184, 311, 7138, 320, 764, 4548, 570, 71883, 1233, 1051, 33445, 315, 264, 8396, 3196, 389, 57751, 323, 11919, 4398, 4315, 1202, 10495, 2085, 279, 9546, 315, 264, 1614, 382, 644, 42108, 4606, 11, 1070, 574, 912, 44565, 4633, 5820, 3734, 1063, 14943, 5411, 10597, 19567, 13, 4314, 11, 323, 1023, 10451, 19567, 11, 3010, 6688, 7342, 311, 10597, 44565, 2191, 13, 763, 279, 328, 46488, 1122, 21080, 11, 40091, 67, 587, 2663, 369, 459, 77271, 20631, 8396, 323, 279, 76445, 315, 87149, 11, 1193, 311, 387, 5246, 16070, 555, 35414, 735, 38155, 358, 382, 644, 15004, 969, 11, 10597, 31237, 82, 89194, 2403, 279, 1614, 13, 763, 4606, 11, 5370, 31237, 82, 8040, 7294, 21395, 323, 57125, 61555, 13, 50086, 291, 2802, 304, 61386, 488, 2391, 279, 55383, 323, 304, 879, 19971, 2391, 279, 1050, 1659, 28101, 5540, 315, 7294, 43802, 20631, 37019, 2191, 11, 8104, 304, 9822, 13, 92931, 11774, 311, 20207, 11447, 320, 5132, 1299, 323, 10597, 8, 323, 279, 93574, 315, 279, 220, 11128, 15, 82, 323, 220, 10336, 23, 682, 85747, 279, 42933, 4500, 315, 1148, 6244, 279, 11639, 315, 29924, 44565, 2191, 382, 49552, 11639, 720, 16397, 279, 8753, 22910, 11, 49638, 5315, 1778, 439, 279, 2998, 4193, 5512, 323, 279, 220, 5602, 264, 13353, 1486, 304, 279, 74454, 315, 7294, 21395, 323, 6918, 380, 58214, 13, 578, 1176, 78431, 60701, 8040, 6957, 279, 220, 972, 339, 9478, 439, 12656, 4359, 7678, 16948, 37588, 41903, 44565, 2191, 304, 9635, 11, 57323, 20445, 275, 318, 3876, 279, 1614, 11, 7639, 65292, 1215, 596, 7422, 63675, 279, 1648, 311, 3927, 2191, 323, 38077, 12278, 974, 764, 393, 583, 31721, 263, 596, 10334, 315, 27848, 2191, 1766, 70225, 17614, 304, 9822, 13, 3296, 279, 3389, 220, 9674, 15, 82, 11, 5370, 78431, 8853, 315, 3463, 1047, 3719, 1664, 39817, 323, 264, 12330, 315, 1243, 31069, 3728, 8082, 10222, 505, 220, 9367, 15, 311, 220, 7529, 19, 13, 1115, 11639, 315, 29924, 44565, 2191, 36513, 3156, 279, 842, 315, 279, 15506, 16803, 5111, 323, 374, 6646, 279, 21411, 4325, 315, 44565, 2191, 382, 38537, 505, 27848, 2191, 11, 92551, 36769, 359, 258, 18538, 6667, 80244, 44565, 2191, 323, 10862, 279, 7327, 22938, 5794, 596, 10229, 11, 264, 538, 12128, 11552, 3010, 3967, 439, 279, 5629, 7327, 430, 14454, 304, 220, 9714, 19, 311, 52696, 17226, 30191, 60701, 13, 578, 7327, 6244, 264, 5199, 5054, 5457, 11, 449, 35131, 28187, 1694, 264, 6522, 7216, 323, 264, 4562, 315, 1202, 3331, 9251, 13, 36769, 359, 258, 596, 37480, 320, 1820, 622, 5808, 28331, 8, 323, 393, 583, 31721, 263, 596, 20723, 320, 1820, 27848, 1705, 8, 16475, 1614, 51618, 11, 59416, 5054, 63944, 3012, 2191, 323, 2678, 3424, 58348, 13, 4740, 26242, 42254, 11, 279, 36769, 359, 258, 1705, 1051, 67331, 505, 279, 7327, 555, 279, 28187, 1705, 520, 279, 220, 9674, 17, 86026, 8151, 13, 1556, 1132, 1705, 1051, 12020, 30293, 304, 279, 10657, 7327, 11, 1694, 13967, 67331, 304, 220, 9378, 21, 13, 36769, 359, 258, 51287, 19698, 430, 422, 14110, 5548, 18661, 2410, 555, 28187, 596, 3878, 11, 814, 1053, 842, 709, 279, 502, 43049, 1821, 315, 7487, 13, 763, 2077, 311, 872, 95989, 505, 279, 5629, 7327, 11, 93134, 14454, 279, 800, 13, 2417, 1291, 7327, 13, 9636, 279, 10383, 315, 11291, 735, 897, 354, 8148, 11, 264, 8690, 55475, 323, 28568, 11, 459, 277, 971, 88389, 359, 2191, 29204, 5795, 449, 6667, 74050, 13, 1556, 277, 971, 88389, 359, 1705, 11, 889, 24465, 20343, 505, 279, 220, 9674, 16, 12366, 6947, 2957, 11, 64854, 369, 1949, 80375, 323, 369, 279, 8141, 315, 11822, 4184, 311, 832, 596, 3966, 382, 1383, 279, 2543, 315, 279, 220, 508, 339, 9478, 11, 44565, 2191, 1047, 9041, 682, 927, 279, 1917, 13, 1102, 574, 264, 28289, 4668, 315, 279, 6625, 22013, 950, 380, 7351, 13, 763, 5734, 11, 2678, 5315, 315, 4236, 25973, 279, 3823, 4633, 463, 31419, 1873, 2373, 315, 459, 277, 971, 88389, 359, 2191, 13, 27286, 574, 264, 80310, 369, 42301, 1245, 12822, 505, 6460, 14875, 5961, 11, 889, 7882, 311, 279, 11002, 6864, 311, 4007, 13, 763, 20023, 5270, 11, 32164, 574, 264, 86568, 369, 459, 277, 971, 1355, 88, 303, 950, 2191, 11, 1405, 433, 6244, 279, 1455, 21102, 2163, 29480, 34649, 13, 12220, 420, 892, 11, 264, 23413, 315, 93134, 18306, 26411, 315, 30191, 5054, 9349, 11, 3967, 439, 30617, 315, 279, 56408, 13, 578, 834, 9792, 479, 315, 279, 8753, 41289, 7351, 1139, 1690, 5315, 323, 279, 11572, 323, 61087, 315, 1690, 57298, 2402, 311, 47426, 49028, 2768, 279, 46735, 315, 279, 12366, 6947, 2957, 92867, 3927, 380, 5054, 7645, 323, 14385, 13, 7570, 3582, 1690, 93134, 1612, 4979, 5694, 505, 1521, 20320, 14385, 11, 4225, 27322, 3782, 5304, 279, 7351, 323, 13865, 1051, 1903, 311, 5471, 93134, 15644, 1113, 311, 279, 2326, 11, 2737, 279, 40782, 3298, 315, 220, 7028, 18, 11, 1101, 2663, 279, 1556, 1132, 380, 1398, 9134, 3298, 13, 15388, 2191, 574, 2500, 8446, 902, 1063, 93134, 18306, 2391, 420, 4261, 382, 20397, 10742, 11, 93134, 99325, 31408, 304, 279, 8690, 22910, 304, 14076, 311, 279, 5929, 7351, 11, 5423, 304, 279, 386, 22506, 39142, 939, 81236, 26, 4869, 11, 814, 2322, 25984, 46735, 1306, 279, 92501, 3109, 1047, 27276, 4147, 11, 2737, 2391, 279, 97660, 45378, 53848, 13, 26778, 93134, 505, 62579, 6902, 323, 23223, 30010, 311, 19278, 11, 1603, 279, 92501, 82, 33745, 279, 78431, 7351, 1070, 2288, 13, 3161, 279, 93134, 1694]\n",
            "inputs:\n",
            "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. As a historically left-wing movement, this reading of anarchism is placed on the farthest left of the political spectrum, usually described as the libertarian wing of the socialist movement (libertarian socialism).\n",
            "\n",
            "Humans have lived in societies without formal hierarchies long before the establishment of states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more, growing in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements.\n",
            "\n",
            "Anarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. Evolutionary methods try to simulate what an anarchist society might be like, but revolutionary tactics, which have historically taken a violent turn, aim to overthrow authority and the state. Many facets of human civilization have been influenced by anarchist theory, critique, and praxis.\n",
            "\n",
            "Etymology, terminology, and definition \n",
            "\n",
            "The etymological origin of anarchism is from the Ancient Greek anarkhia, meaning \"without a ruler\", composed of the prefix an- (\"without\") and the word arkhos (\"leader\" or \"ruler\"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (17561836) and Wilhelm Weitling (18081871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.\n",
            "\n",
            "The first political philosopher to call himself an anarchist () was Pierre-Joseph Proudhon (18091865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.\n",
            "\n",
            "While the term libertarian has been largely synonymous with anarchism, its meaning has more recently been diluted by wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism's negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism's socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberal and socialist but more so. Many scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.\n",
            "\n",
            "While opposition to the state is central to anarchist thought, defining anarchism is not an easy task for scholars, as there is a lot of discussion among scholars and anarchists on the matter, and various currents perceive anarchism slightly differently. Major definitional elements include the will for a non-coercive society, the rejection of the state apparatus, the belief that human nature allows humans to exist in or progress toward such a non-coercive society, and a suggestion on how to act to pursue the ideal of anarchy.\n",
            "\n",
            "History\n",
            "\n",
            "Pre-modern era \n",
            "\n",
            "Before the creation of towns and cities, established authority did not exist. It was after the institution of authority that anarchistic ideas were espoused as a reaction. The most notable precursors to anarchism in the ancient world were in China and Greece. In China, philosophical anarchism (the discussion on the legitimacy of the state) was delineated by Taoist philosophers Zhuang Zhou and Laozi. Alongside Stoicism, Taoism has been said to have had \"significant anticipations\" of anarchism.\n",
            "\n",
            "Anarchic attitudes were also articulated by tragedians and philosophers in Greece. Aeschylus and Sophocles used the myth of Antigone to illustrate the conflict between laws imposed by the state and personal autonomy. Socrates questioned Athenian authorities constantly and insisted on the right of individual freedom of conscience. Cynics dismissed human law (nomos) and associated authorities while trying to live according to nature (physis). Stoics were supportive of a society based on unofficial and friendly relations among its citizens without the presence of a state.\n",
            "\n",
            "In medieval Europe, there was no anarchistic activity except some ascetic religious movements. These, and other Muslim movements, later gave birth to religious anarchism. In the Sasanian Empire, Mazdak called for an egalitarian society and the abolition of monarchy, only to be soon executed by Emperor Kavad I.\n",
            "\n",
            "In Basra, religious sects preached against the state. In Europe, various sects developed anti-state and libertarian tendencies. Renewed interest in antiquity during the Renaissance and in private judgment during the Reformation restored elements of anti-authoritarian secularism, particularly in France. Enlightenment challenges to intellectual authority (secular and religious) and the revolutions of the 1790s and 1848 all spurred the ideological development of what became the era of classical anarchism.\n",
            "\n",
            "Modern era \n",
            "During the French Revolution, partisan groups such as the Enrags and the  saw a turning point in the fermentation of anti-state and federalist sentiments. The first anarchist currents developed throughout the 18th century as William Godwin espoused philosophical anarchism in England, morally delegitimising the state, Max Stirner's thinking paved the way to individualism and Pierre-Joseph Proudhon's theory of mutualism found fertile soil in France. By the late 1870s, various anarchist schools of thought had become well-defined and a wave of then unprecedented globalisation occurred from 1880 to 1914. This era of classical anarchism lasted until the end of the Spanish Civil War and is considered the golden age of anarchism.\n",
            "\n",
            "Drawing from mutualism, Mikhail Bakunin founded collectivist anarchism and entered the International Workingmen's Association, a class worker union later known as the First International that formed in 1864 to unite diverse revolutionary currents. The International became a significant political force, with Karl Marx being a leading figure and a member of its General Council. Bakunin's faction (the Jura Federation) and Proudhon's followers (the mutualists) opposed state socialism, advocating political abstentionism and small property holdings. After bitter disputes, the Bakuninists were expelled from the International by the Marxists at the 1872 Hague Congress. Anarchists were treated similarly in the Second International, being ultimately expelled in 1896. Bakunin famously predicted that if revolutionaries gained power by Marx's terms, they would end up the new tyrants of workers. In response to their expulsion from the First International, anarchists formed the St. Imier International. Under the influence of Peter Kropotkin, a Russian philosopher and scientist, anarcho-communism overlapped with collectivism. Anarcho-communists, who drew inspiration from the 1871 Paris Commune, advocated for free federation and for the distribution of goods according to one's needs.\n",
            "\n",
            "By the turn of the 20th century, anarchism had spread all over the world. It was a notable feature of the international syndicalist movement. In China, small groups of students imported the humanistic pro-science version of anarcho-communism. Tokyo was a hotspot for rebellious youth from East Asian countries, who moved to the Japanese capital to study. In Latin America, Argentina was a stronghold for anarcho-syndicalism, where it became the most prominent left-wing ideology. During this time, a minority of anarchists adopted tactics of revolutionary political violence, known as propaganda of the deed. The dismemberment of the French socialist movement into many groups and the execution and exile of many Communards to penal colonies following the suppression of the Paris Commune favoured individualist political expression and acts. Even though many anarchists distanced themselves from these terrorist acts, infamy came upon the movement and attempts were made to prevent anarchists immigrating to the US, including the Immigration Act of 1903, also called the Anarchist Exclusion Act. Illegalism was another strategy which some anarchists adopted during this period.\n",
            "\n",
            "Despite concerns, anarchists enthusiastically participated in the Russian Revolution in opposition to the White movement, especially in the Makhnovshchina; however, they met harsh suppression after the Bolshevik government had stabilised, including during the Kronstadt rebellion. Several anarchists from Petrograd and Moscow fled to Ukraine, before the Bolsheviks crushed the anarchist movement there too. With the anarchists being\n",
            "[INFO|configuration_utils.py:698] 2025-06-19 09:40:23,004 >> loading configuration file config.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 09:40:23,005 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 09:40:23] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1151] 2025-06-19 09:40:23,134 >> loading weights file model.safetensors from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-19 09:40:23,135 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 09:40:23,135 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 2/2 [00:05<00:00,  2.60s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-19 09:40:28,352 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-19 09:40:28,352 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/LLaMA-3.2-3B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "Loading checkpoint shards: 100%|| 2/2 [00:05<00:00,  2.58s/it]\n",
            "[INFO|configuration_utils.py:1090] 2025-06-19 09:40:28,865 >> loading configuration file generation_config.json from cache at /home/ai-research-lab/.cache/huggingface/hub/models--meta-llama--LLaMA-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 09:40:28,865 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 09:40:29] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-06-19 09:40:29] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-19 09:40:29] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-06-19 09:40:29] llamafactory.model.adapter:143 >> Fine-tuning method: Freeze\n",
            "[INFO|2025-06-19 09:40:29] llamafactory.model.adapter:143 >> Set trainable layers: .24.,.25.,.26.,.27.\n",
            "[INFO|2025-06-19 09:40:29] llamafactory.model.loader:143 >> trainable params: 402,677,760 || all params: 3,212,749,824 || trainable%: 12.5337\n",
            "[INFO|trainer.py:756] 2025-06-19 09:40:29,409 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2409] 2025-06-19 09:40:30,095 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-06-19 09:40:30,095 >>   Num examples = 36,115\n",
            "[INFO|trainer.py:2411] 2025-06-19 09:40:30,095 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2412] 2025-06-19 09:40:30,095 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2415] 2025-06-19 09:40:30,095 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2416] 2025-06-19 09:40:30,095 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2417] 2025-06-19 09:40:30,095 >>   Total optimization steps = 2,258\n",
            "[INFO|trainer.py:2418] 2025-06-19 09:40:30,096 >>   Number of trainable parameters = 402,677,760\n",
            "{'loss': 2.3486, 'grad_norm': 0.4706062376499176, 'learning_rate': 3.982300884955752e-06, 'epoch': 0.0}\n",
            "{'loss': 2.3441, 'grad_norm': 0.44924476742744446, 'learning_rate': 8.407079646017701e-06, 'epoch': 0.01}\n",
            "{'loss': 2.2424, 'grad_norm': 0.4252140522003174, 'learning_rate': 1.2831858407079647e-05, 'epoch': 0.01}\n",
            "{'loss': 2.2992, 'grad_norm': 0.41716423630714417, 'learning_rate': 1.7256637168141594e-05, 'epoch': 0.02}\n",
            "{'loss': 2.2522, 'grad_norm': 0.4427279829978943, 'learning_rate': 2.1681415929203542e-05, 'epoch': 0.02}\n",
            "{'loss': 2.2791, 'grad_norm': 0.4480474293231964, 'learning_rate': 2.610619469026549e-05, 'epoch': 0.03}\n",
            "{'loss': 2.2367, 'grad_norm': 0.40530925989151, 'learning_rate': 3.0530973451327434e-05, 'epoch': 0.03}\n",
            "{'loss': 2.2835, 'grad_norm': 0.44593098759651184, 'learning_rate': 3.495575221238938e-05, 'epoch': 0.04}\n",
            "{'loss': 2.302, 'grad_norm': 0.4346984326839447, 'learning_rate': 3.938053097345133e-05, 'epoch': 0.04}\n",
            "{'loss': 2.2779, 'grad_norm': 0.4234446585178375, 'learning_rate': 4.380530973451328e-05, 'epoch': 0.04}\n",
            "{'loss': 2.2408, 'grad_norm': 0.4585871696472168, 'learning_rate': 4.823008849557522e-05, 'epoch': 0.05}\n",
            "{'loss': 2.2545, 'grad_norm': 0.46434667706489563, 'learning_rate': 5.265486725663717e-05, 'epoch': 0.05}\n",
            "{'loss': 2.2007, 'grad_norm': 0.46079570055007935, 'learning_rate': 5.707964601769912e-05, 'epoch': 0.06}\n",
            "{'loss': 2.2333, 'grad_norm': 0.3945293128490448, 'learning_rate': 6.150442477876106e-05, 'epoch': 0.06}\n",
            "{'loss': 2.3149, 'grad_norm': 0.4322696030139923, 'learning_rate': 6.592920353982302e-05, 'epoch': 0.07}\n",
            "{'loss': 2.2933, 'grad_norm': 0.47283586859703064, 'learning_rate': 7.035398230088496e-05, 'epoch': 0.07}\n",
            "{'loss': 2.2336, 'grad_norm': 0.41888228058815, 'learning_rate': 7.477876106194691e-05, 'epoch': 0.08}\n",
            "{'loss': 2.2878, 'grad_norm': 0.4321129620075226, 'learning_rate': 7.920353982300885e-05, 'epoch': 0.08}\n",
            "{'loss': 2.2502, 'grad_norm': 0.43974703550338745, 'learning_rate': 8.362831858407079e-05, 'epoch': 0.08}\n",
            "{'loss': 2.2669, 'grad_norm': 0.47438845038414, 'learning_rate': 8.805309734513275e-05, 'epoch': 0.09}\n",
            "{'loss': 2.2985, 'grad_norm': 0.46205607056617737, 'learning_rate': 9.247787610619469e-05, 'epoch': 0.09}\n",
            "{'loss': 2.2514, 'grad_norm': 0.40476304292678833, 'learning_rate': 9.690265486725664e-05, 'epoch': 0.1}\n",
            "{'loss': 2.26, 'grad_norm': 0.4431220591068268, 'learning_rate': 9.999946218355495e-05, 'epoch': 0.1}\n",
            "{'loss': 2.243, 'grad_norm': 0.42879384756088257, 'learning_rate': 9.998990132416733e-05, 'epoch': 0.11}\n",
            "{'loss': 2.2617, 'grad_norm': 0.42171502113342285, 'learning_rate': 9.99683916186972e-05, 'epoch': 0.11}\n",
            "{'loss': 2.2505, 'grad_norm': 0.46991461515426636, 'learning_rate': 9.99349382085062e-05, 'epoch': 0.12}\n",
            "{'loss': 2.2563, 'grad_norm': 0.43805646896362305, 'learning_rate': 9.988954908980232e-05, 'epoch': 0.12}\n",
            "{'loss': 2.2572, 'grad_norm': 0.4173353314399719, 'learning_rate': 9.983223511172866e-05, 'epoch': 0.12}\n",
            "{'loss': 2.2818, 'grad_norm': 0.43167442083358765, 'learning_rate': 9.976300997377022e-05, 'epoch': 0.13}\n",
            "{'loss': 2.2765, 'grad_norm': 0.4380871057510376, 'learning_rate': 9.968189022247931e-05, 'epoch': 0.13}\n",
            "{'loss': 2.2453, 'grad_norm': 0.45474162697792053, 'learning_rate': 9.95888952475206e-05, 'epoch': 0.14}\n",
            "{'loss': 2.2336, 'grad_norm': 0.43555429577827454, 'learning_rate': 9.948404727703642e-05, 'epoch': 0.14}\n",
            "{'loss': 2.2451, 'grad_norm': 0.40018734335899353, 'learning_rate': 9.936737137233372e-05, 'epoch': 0.15}\n",
            "{'loss': 2.2383, 'grad_norm': 0.42877182364463806, 'learning_rate': 9.923889542189376e-05, 'epoch': 0.15}\n",
            "{'loss': 2.2977, 'grad_norm': 0.40533968806266785, 'learning_rate': 9.90986501347061e-05, 'epoch': 0.16}\n",
            "{'loss': 2.2946, 'grad_norm': 0.39139118790626526, 'learning_rate': 9.894666903292835e-05, 'epoch': 0.16}\n",
            "{'loss': 2.2519, 'grad_norm': 0.4317380487918854, 'learning_rate': 9.878298844387348e-05, 'epoch': 0.16}\n",
            "{'loss': 2.2496, 'grad_norm': 0.43273070454597473, 'learning_rate': 9.860764749132678e-05, 'epoch': 0.17}\n",
            "{'loss': 2.256, 'grad_norm': 0.4785121977329254, 'learning_rate': 9.842068808619425e-05, 'epoch': 0.17}\n",
            "{'loss': 2.2925, 'grad_norm': 0.4316929280757904, 'learning_rate': 9.822215491648475e-05, 'epoch': 0.18}\n",
            "{'loss': 2.2364, 'grad_norm': 0.403774231672287, 'learning_rate': 9.80120954366286e-05, 'epoch': 0.18}\n",
            "{'loss': 2.2401, 'grad_norm': 0.40092068910598755, 'learning_rate': 9.779055985613461e-05, 'epoch': 0.19}\n",
            "{'loss': 2.197, 'grad_norm': 0.37031158804893494, 'learning_rate': 9.75576011275889e-05, 'epoch': 0.19}\n",
            "{'loss': 2.223, 'grad_norm': 0.4016660451889038, 'learning_rate': 9.731327493399775e-05, 'epoch': 0.19}\n",
            "{'loss': 2.2534, 'grad_norm': 0.3984716534614563, 'learning_rate': 9.705763967547809e-05, 'epoch': 0.2}\n",
            "{'loss': 2.2553, 'grad_norm': 0.4207526445388794, 'learning_rate': 9.679075645529832e-05, 'epoch': 0.2}\n",
            "{'loss': 2.2463, 'grad_norm': 0.4682912826538086, 'learning_rate': 9.651268906527305e-05, 'epoch': 0.21}\n",
            "{'loss': 2.2395, 'grad_norm': 0.4092140197753906, 'learning_rate': 9.622350397051536e-05, 'epoch': 0.21}\n",
            "{'loss': 2.2474, 'grad_norm': 0.42167890071868896, 'learning_rate': 9.592327029354992e-05, 'epoch': 0.22}\n",
            "{'loss': 2.248, 'grad_norm': 0.6052901148796082, 'learning_rate': 9.561205979779095e-05, 'epoch': 0.22}\n",
            " 22%|                             | 500/2258 [26:11<1:32:22,  3.15s/it][INFO|trainer.py:3993] 2025-06-19 10:06:45,251 >> Saving model checkpoint to llama3-3b_freeze_1per/checkpoint-500\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 10:06:45,252 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 10:06:45,253 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 10:06:49,213 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/checkpoint-500/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 10:06:49,213 >> chat template saved in llama3-3b_freeze_1per/checkpoint-500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 10:06:49,214 >> tokenizer config file saved in llama3-3b_freeze_1per/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 10:06:49,214 >> Special tokens file saved in llama3-3b_freeze_1per/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.2849, 'grad_norm': 0.402281254529953, 'learning_rate': 9.528994687038891e-05, 'epoch': 0.23}\n",
            "{'loss': 2.2242, 'grad_norm': 0.43558940291404724, 'learning_rate': 9.49570085044502e-05, 'epoch': 0.23}\n",
            "{'loss': 2.2889, 'grad_norm': 0.4125150144100189, 'learning_rate': 9.461332428063381e-05, 'epoch': 0.23}\n",
            "{'loss': 2.2089, 'grad_norm': 0.418983519077301, 'learning_rate': 9.42589763481295e-05, 'epoch': 0.24}\n",
            "{'loss': 2.2236, 'grad_norm': 0.399522602558136, 'learning_rate': 9.389404940502217e-05, 'epoch': 0.24}\n",
            "{'loss': 2.2308, 'grad_norm': 0.4034527540206909, 'learning_rate': 9.351863067804678e-05, 'epoch': 0.25}\n",
            "{'loss': 2.2203, 'grad_norm': 0.3854360580444336, 'learning_rate': 9.313280990173905e-05, 'epoch': 0.25}\n",
            "{'loss': 2.2369, 'grad_norm': 0.3760169446468353, 'learning_rate': 9.273667929698657e-05, 'epoch': 0.26}\n",
            "{'loss': 2.3004, 'grad_norm': 0.3769288957118988, 'learning_rate': 9.233033354898575e-05, 'epoch': 0.26}\n",
            "{'loss': 2.2336, 'grad_norm': 0.3940066993236542, 'learning_rate': 9.191386978460962e-05, 'epoch': 0.27}\n",
            "{'loss': 2.2932, 'grad_norm': 0.37079018354415894, 'learning_rate': 9.14873875491921e-05, 'epoch': 0.27}\n",
            "{'loss': 2.2331, 'grad_norm': 0.40292859077453613, 'learning_rate': 9.105098878273412e-05, 'epoch': 0.27}\n",
            "{'loss': 2.2605, 'grad_norm': 0.40894195437431335, 'learning_rate': 9.060477779553742e-05, 'epoch': 0.28}\n",
            "{'loss': 2.2331, 'grad_norm': 0.409405916929245, 'learning_rate': 9.014886124327176e-05, 'epoch': 0.28}\n",
            "{'loss': 2.1769, 'grad_norm': 0.384040504693985, 'learning_rate': 8.968334810148149e-05, 'epoch': 0.29}\n",
            "{'loss': 2.2708, 'grad_norm': 0.3700837790966034, 'learning_rate': 8.920834963953769e-05, 'epoch': 0.29}\n",
            "{'loss': 2.2116, 'grad_norm': 0.37670984864234924, 'learning_rate': 8.872397939404198e-05, 'epoch': 0.3}\n",
            "{'loss': 2.2693, 'grad_norm': 0.3680243194103241, 'learning_rate': 8.823035314168839e-05, 'epoch': 0.3}\n",
            "{'loss': 2.2224, 'grad_norm': 0.3811365067958832, 'learning_rate': 8.77275888715898e-05, 'epoch': 0.31}\n",
            "{'loss': 2.2008, 'grad_norm': 0.38877785205841064, 'learning_rate': 8.721580675707568e-05, 'epoch': 0.31}\n",
            "{'loss': 2.266, 'grad_norm': 0.3985789716243744, 'learning_rate': 8.669512912696746e-05, 'epoch': 0.31}\n",
            "{'loss': 2.2357, 'grad_norm': 0.4052039086818695, 'learning_rate': 8.616568043633897e-05, 'epoch': 0.32}\n",
            "{'loss': 2.2162, 'grad_norm': 0.37013453245162964, 'learning_rate': 8.562758723676856e-05, 'epoch': 0.32}\n",
            "{'loss': 2.2523, 'grad_norm': 0.3894713222980499, 'learning_rate': 8.508097814609002e-05, 'epoch': 0.33}\n",
            "{'loss': 2.2028, 'grad_norm': 0.3716041147708893, 'learning_rate': 8.452598381764971e-05, 'epoch': 0.33}\n",
            "{'loss': 2.2307, 'grad_norm': 0.38706547021865845, 'learning_rate': 8.396273690907706e-05, 'epoch': 0.34}\n",
            "{'loss': 2.2021, 'grad_norm': 0.3896755874156952, 'learning_rate': 8.33913720505762e-05, 'epoch': 0.34}\n",
            "{'loss': 2.2359, 'grad_norm': 0.391355037689209, 'learning_rate': 8.281202581274575e-05, 'epoch': 0.35}\n",
            "{'loss': 2.2472, 'grad_norm': 0.3567502200603485, 'learning_rate': 8.222483667393518e-05, 'epoch': 0.35}\n",
            "{'loss': 2.1984, 'grad_norm': 0.33896350860595703, 'learning_rate': 8.162994498714489e-05, 'epoch': 0.35}\n",
            "{'loss': 2.2458, 'grad_norm': 0.36153122782707214, 'learning_rate': 8.102749294647838e-05, 'epoch': 0.36}\n",
            "{'loss': 2.2292, 'grad_norm': 0.3542573153972626, 'learning_rate': 8.041762455315416e-05, 'epoch': 0.36}\n",
            "{'loss': 2.219, 'grad_norm': 0.36645859479904175, 'learning_rate': 7.980048558108595e-05, 'epoch': 0.37}\n",
            "{'loss': 2.2373, 'grad_norm': 0.36383262276649475, 'learning_rate': 7.917622354203901e-05, 'epoch': 0.37}\n",
            "{'loss': 2.2312, 'grad_norm': 0.3688139319419861, 'learning_rate': 7.854498765037109e-05, 'epoch': 0.38}\n",
            "{'loss': 2.2014, 'grad_norm': 0.3766467869281769, 'learning_rate': 7.790692878736643e-05, 'epoch': 0.38}\n",
            "{'loss': 2.2432, 'grad_norm': 0.3873195946216583, 'learning_rate': 7.726219946517136e-05, 'epoch': 0.39}\n",
            "{'loss': 2.1892, 'grad_norm': 0.35528138279914856, 'learning_rate': 7.661095379034004e-05, 'epoch': 0.39}\n",
            "{'loss': 2.2401, 'grad_norm': 0.3657779395580292, 'learning_rate': 7.595334742699909e-05, 'epoch': 0.39}\n",
            "{'loss': 2.1478, 'grad_norm': 0.3681999742984772, 'learning_rate': 7.528953755964001e-05, 'epoch': 0.4}\n",
            "{'loss': 2.2722, 'grad_norm': 0.35902339220046997, 'learning_rate': 7.461968285554805e-05, 'epoch': 0.4}\n",
            "{'loss': 2.1826, 'grad_norm': 0.35894662141799927, 'learning_rate': 7.394394342687673e-05, 'epoch': 0.41}\n",
            "{'loss': 2.2129, 'grad_norm': 0.35097163915634155, 'learning_rate': 7.326248079237711e-05, 'epoch': 0.41}\n",
            "{'loss': 2.2112, 'grad_norm': 0.35511741042137146, 'learning_rate': 7.257545783879058e-05, 'epoch': 0.42}\n",
            "{'loss': 2.1502, 'grad_norm': 0.35396939516067505, 'learning_rate': 7.188303878191492e-05, 'epoch': 0.42}\n",
            "{'loss': 2.1763, 'grad_norm': 0.3851473033428192, 'learning_rate': 7.11853891273526e-05, 'epoch': 0.43}\n",
            "{'loss': 2.2085, 'grad_norm': 0.3696553409099579, 'learning_rate': 7.048267563095074e-05, 'epoch': 0.43}\n",
            "{'loss': 2.2334, 'grad_norm': 0.37758776545524597, 'learning_rate': 6.977506625894224e-05, 'epoch': 0.43}\n",
            "{'loss': 2.2257, 'grad_norm': 0.3503907024860382, 'learning_rate': 6.90627301477976e-05, 'epoch': 0.44}\n",
            "{'loss': 2.2152, 'grad_norm': 0.3461887538433075, 'learning_rate': 6.834583756379715e-05, 'epoch': 0.44}\n",
            " 44%|                    | 1000/2258 [52:35<1:06:15,  3.16s/it][INFO|trainer.py:3993] 2025-06-19 10:33:09,168 >> Saving model checkpoint to llama3-3b_freeze_1per/checkpoint-1000\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 10:33:09,169 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 10:33:09,169 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 10:33:13,158 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/checkpoint-1000/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 10:33:13,159 >> chat template saved in llama3-3b_freeze_1per/checkpoint-1000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 10:33:13,160 >> tokenizer config file saved in llama3-3b_freeze_1per/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 10:33:13,160 >> Special tokens file saved in llama3-3b_freeze_1per/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.1916, 'grad_norm': 0.36101827025413513, 'learning_rate': 6.762455986233301e-05, 'epoch': 0.45}\n",
            "{'loss': 2.2019, 'grad_norm': 0.34269556403160095, 'learning_rate': 6.689906944695091e-05, 'epoch': 0.45}\n",
            "{'loss': 2.2063, 'grad_norm': 0.37633007764816284, 'learning_rate': 6.61695397281413e-05, 'epoch': 0.46}\n",
            "{'loss': 2.225, 'grad_norm': 0.38360926508903503, 'learning_rate': 6.543614508188997e-05, 'epoch': 0.46}\n",
            "{'loss': 2.2524, 'grad_norm': 0.39212217926979065, 'learning_rate': 6.469906080799775e-05, 'epoch': 0.47}\n",
            "{'loss': 2.1997, 'grad_norm': 0.3683655261993408, 'learning_rate': 6.395846308817947e-05, 'epoch': 0.47}\n",
            "{'loss': 2.2371, 'grad_norm': 0.3625892996788025, 'learning_rate': 6.321452894395204e-05, 'epoch': 0.47}\n",
            "{'loss': 2.1813, 'grad_norm': 0.3543241322040558, 'learning_rate': 6.246743619432188e-05, 'epoch': 0.48}\n",
            "{'loss': 2.1922, 'grad_norm': 0.3812260329723358, 'learning_rate': 6.171736341328172e-05, 'epoch': 0.48}\n",
            "{'loss': 2.2642, 'grad_norm': 0.3539370000362396, 'learning_rate': 6.096448988712685e-05, 'epoch': 0.49}\n",
            "{'loss': 2.2206, 'grad_norm': 0.39097118377685547, 'learning_rate': 6.0208995571601243e-05, 'epoch': 0.49}\n",
            "{'loss': 2.1822, 'grad_norm': 0.3561065196990967, 'learning_rate': 5.945106104888358e-05, 'epoch': 0.5}\n",
            "{'loss': 2.2215, 'grad_norm': 0.36300748586654663, 'learning_rate': 5.869086748442352e-05, 'epoch': 0.5}\n",
            "{'loss': 2.1942, 'grad_norm': 0.3543805778026581, 'learning_rate': 5.792859658363864e-05, 'epoch': 0.51}\n",
            "{'loss': 2.2141, 'grad_norm': 0.3367845416069031, 'learning_rate': 5.7164430548482226e-05, 'epoch': 0.51}\n",
            "{'loss': 2.1498, 'grad_norm': 0.36816301941871643, 'learning_rate': 5.639855203389249e-05, 'epoch': 0.51}\n",
            "{'loss': 2.1757, 'grad_norm': 0.3657177984714508, 'learning_rate': 5.563114410413331e-05, 'epoch': 0.52}\n",
            "{'loss': 2.1721, 'grad_norm': 0.3456834554672241, 'learning_rate': 5.486239018903737e-05, 'epoch': 0.52}\n",
            "{'loss': 2.2167, 'grad_norm': 0.37416407465934753, 'learning_rate': 5.409247404016174e-05, 'epoch': 0.53}\n",
            "{'loss': 2.2028, 'grad_norm': 0.3393229842185974, 'learning_rate': 5.332157968686667e-05, 'epoch': 0.53}\n",
            "{'loss': 2.2216, 'grad_norm': 0.3721288740634918, 'learning_rate': 5.254989139232793e-05, 'epoch': 0.54}\n",
            "{'loss': 2.2245, 'grad_norm': 0.34475579857826233, 'learning_rate': 5.1777593609493194e-05, 'epoch': 0.54}\n",
            "{'loss': 2.2352, 'grad_norm': 0.3499159812927246, 'learning_rate': 5.100487093699327e-05, 'epoch': 0.54}\n",
            "{'loss': 2.1652, 'grad_norm': 0.3408925235271454, 'learning_rate': 5.023190807501825e-05, 'epoch': 0.55}\n",
            "{'loss': 2.2045, 'grad_norm': 0.36668187379837036, 'learning_rate': 4.945888978116958e-05, 'epoch': 0.55}\n",
            "{'loss': 2.2065, 'grad_norm': 0.370256632566452, 'learning_rate': 4.868600082629834e-05, 'epoch': 0.56}\n",
            "{'loss': 2.2031, 'grad_norm': 0.3768027126789093, 'learning_rate': 4.7913425950340293e-05, 'epoch': 0.56}\n",
            "{'loss': 2.1922, 'grad_norm': 0.3543565571308136, 'learning_rate': 4.714134981815848e-05, 'epoch': 0.57}\n",
            "{'loss': 2.2079, 'grad_norm': 0.3744192123413086, 'learning_rate': 4.6369956975403546e-05, 'epoch': 0.57}\n",
            "{'loss': 2.1694, 'grad_norm': 0.3487931787967682, 'learning_rate': 4.5599431804402816e-05, 'epoch': 0.58}\n",
            "{'loss': 2.1502, 'grad_norm': 0.3459630012512207, 'learning_rate': 4.482995848008814e-05, 'epoch': 0.58}\n",
            "{'loss': 2.2082, 'grad_norm': 0.352194219827652, 'learning_rate': 4.406172092597355e-05, 'epoch': 0.58}\n",
            "{'loss': 2.2173, 'grad_norm': 0.3791966736316681, 'learning_rate': 4.329490277019286e-05, 'epoch': 0.59}\n",
            "{'loss': 2.1882, 'grad_norm': 0.3490530252456665, 'learning_rate': 4.2529687301607865e-05, 'epoch': 0.59}\n",
            "{'loss': 2.2305, 'grad_norm': 0.37042781710624695, 'learning_rate': 4.176625742599781e-05, 'epoch': 0.6}\n",
            "{'loss': 2.1424, 'grad_norm': 0.3474940359592438, 'learning_rate': 4.100479562234014e-05, 'epoch': 0.6}\n",
            "{'loss': 2.2005, 'grad_norm': 0.3655359148979187, 'learning_rate': 4.0245483899193595e-05, 'epoch': 0.61}\n",
            "{'loss': 2.2464, 'grad_norm': 0.35997211933135986, 'learning_rate': 3.9488503751193434e-05, 'epoch': 0.61}\n",
            "{'loss': 2.1843, 'grad_norm': 0.3561861515045166, 'learning_rate': 3.873403611566972e-05, 'epoch': 0.62}\n",
            "{'loss': 2.1967, 'grad_norm': 0.3538995385169983, 'learning_rate': 3.798226132939875e-05, 'epoch': 0.62}\n",
            "{'loss': 2.1519, 'grad_norm': 0.36660507321357727, 'learning_rate': 3.723335908549794e-05, 'epoch': 0.62}\n",
            "{'loss': 2.1648, 'grad_norm': 0.3644775152206421, 'learning_rate': 3.648750839047477e-05, 'epoch': 0.63}\n",
            "{'loss': 2.2149, 'grad_norm': 0.3585454821586609, 'learning_rate': 3.574488752143956e-05, 'epoch': 0.63}\n",
            "{'loss': 2.1996, 'grad_norm': 0.3701086640357971, 'learning_rate': 3.500567398349289e-05, 'epoch': 0.64}\n",
            "{'loss': 2.1891, 'grad_norm': 0.3405938744544983, 'learning_rate': 3.427004446729736e-05, 'epoch': 0.64}\n",
            "{'loss': 2.1816, 'grad_norm': 0.3454572260379791, 'learning_rate': 3.3538174806844004e-05, 'epoch': 0.65}\n",
            "{'loss': 2.188, 'grad_norm': 0.35238224267959595, 'learning_rate': 3.2810239937423634e-05, 'epoch': 0.65}\n",
            "{'loss': 2.1848, 'grad_norm': 0.34085872769355774, 'learning_rate': 3.208641385381284e-05, 'epoch': 0.66}\n",
            "{'loss': 2.1812, 'grad_norm': 0.3758131265640259, 'learning_rate': 3.136686956868488e-05, 'epoch': 0.66}\n",
            "{'loss': 2.1594, 'grad_norm': 0.47873425483703613, 'learning_rate': 3.065177907125524e-05, 'epoch': 0.66}\n",
            " 66%|            | 1500/2258 [1:19:03<39:58,  3.16s/it][INFO|trainer.py:3993] 2025-06-19 10:59:37,890 >> Saving model checkpoint to llama3-3b_freeze_1per/checkpoint-1500\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 10:59:37,891 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 10:59:37,891 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 10:59:41,831 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/checkpoint-1500/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 10:59:41,831 >> chat template saved in llama3-3b_freeze_1per/checkpoint-1500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 10:59:41,832 >> tokenizer config file saved in llama3-3b_freeze_1per/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 10:59:41,832 >> Special tokens file saved in llama3-3b_freeze_1per/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 2.2119, 'grad_norm': 0.3634222745895386, 'learning_rate': 2.9941313286171995e-05, 'epoch': 0.67}\n",
            "{'loss': 2.207, 'grad_norm': 0.34536200761795044, 'learning_rate': 2.923564203266036e-05, 'epoch': 0.67}\n",
            "{'loss': 2.1803, 'grad_norm': 0.34702423214912415, 'learning_rate': 2.853493398393172e-05, 'epoch': 0.68}\n",
            "{'loss': 2.2016, 'grad_norm': 0.3452838361263275, 'learning_rate': 2.7839356626866465e-05, 'epoch': 0.68}\n",
            "{'loss': 2.1679, 'grad_norm': 0.356002539396286, 'learning_rate': 2.714907622198035e-05, 'epoch': 0.69}\n",
            "{'loss': 2.1564, 'grad_norm': 0.35816919803619385, 'learning_rate': 2.6464257763684118e-05, 'epoch': 0.69}\n",
            "{'loss': 2.1386, 'grad_norm': 0.3667888641357422, 'learning_rate': 2.578506494084565e-05, 'epoch': 0.7}\n",
            "{'loss': 2.1706, 'grad_norm': 0.3775452673435211, 'learning_rate': 2.5111660097664192e-05, 'epoch': 0.7}\n",
            "{'loss': 2.1677, 'grad_norm': 0.34288525581359863, 'learning_rate': 2.4444204194865987e-05, 'epoch': 0.7}\n",
            "{'loss': 2.2026, 'grad_norm': 0.35151228308677673, 'learning_rate': 2.3782856771230626e-05, 'epoch': 0.71}\n",
            "{'loss': 2.2132, 'grad_norm': 0.34756702184677124, 'learning_rate': 2.312777590545725e-05, 'epoch': 0.71}\n",
            "{'loss': 2.1408, 'grad_norm': 0.35673651099205017, 'learning_rate': 2.2479118178379805e-05, 'epoch': 0.72}\n",
            "{'loss': 2.1955, 'grad_norm': 0.35224229097366333, 'learning_rate': 2.1837038635540186e-05, 'epoch': 0.72}\n",
            "{'loss': 2.2076, 'grad_norm': 0.36772045493125916, 'learning_rate': 2.1201690750128516e-05, 'epoch': 0.73}\n",
            "{'loss': 2.2029, 'grad_norm': 0.35460230708122253, 'learning_rate': 2.057322638629917e-05, 'epoch': 0.73}\n",
            "{'loss': 2.2216, 'grad_norm': 0.3783073425292969, 'learning_rate': 1.995179576287139e-05, 'epoch': 0.74}\n",
            "{'loss': 2.1669, 'grad_norm': 0.3516208827495575, 'learning_rate': 1.9337547417423262e-05, 'epoch': 0.74}\n",
            "{'loss': 2.162, 'grad_norm': 0.35624513030052185, 'learning_rate': 1.873062817078743e-05, 'epoch': 0.74}\n",
            "{'loss': 2.1924, 'grad_norm': 0.36382031440734863, 'learning_rate': 1.8131183091957204e-05, 'epoch': 0.75}\n",
            "{'loss': 2.1356, 'grad_norm': 0.3692913055419922, 'learning_rate': 1.7539355463411526e-05, 'epoch': 0.75}\n",
            "{'loss': 2.1819, 'grad_norm': 0.35561367869377136, 'learning_rate': 1.6955286746866716e-05, 'epoch': 0.76}\n",
            "{'loss': 2.1564, 'grad_norm': 0.3414313495159149, 'learning_rate': 1.637911654946364e-05, 'epoch': 0.76}\n",
            "{'loss': 2.1839, 'grad_norm': 0.35941556096076965, 'learning_rate': 1.5810982590398053e-05, 'epoch': 0.77}\n",
            "{'loss': 2.1214, 'grad_norm': 0.3600020408630371, 'learning_rate': 1.5251020668002236e-05, 'epoch': 0.77}\n",
            "{'loss': 2.2204, 'grad_norm': 0.3544289469718933, 'learning_rate': 1.4699364627285777e-05, 'epoch': 0.78}\n",
            "{'loss': 2.1622, 'grad_norm': 0.36242958903312683, 'learning_rate': 1.4156146327943292e-05, 'epoch': 0.78}\n",
            "{'loss': 2.1991, 'grad_norm': 0.3407411575317383, 'learning_rate': 1.362149561283655e-05, 'epoch': 0.78}\n",
            "{'loss': 2.1695, 'grad_norm': 0.3571203649044037, 'learning_rate': 1.3095540276958806e-05, 'epoch': 0.79}\n",
            "{'loss': 2.1458, 'grad_norm': 0.34414228796958923, 'learning_rate': 1.2578406036888569e-05, 'epoch': 0.79}\n",
            "{'loss': 2.1884, 'grad_norm': 0.3537386357784271, 'learning_rate': 1.2070216500740173e-05, 'epoch': 0.8}\n",
            "{'loss': 2.1796, 'grad_norm': 0.35408079624176025, 'learning_rate': 1.1571093138618344e-05, 'epoch': 0.8}\n",
            "{'loss': 2.1691, 'grad_norm': 0.35392555594444275, 'learning_rate': 1.108115525358372e-05, 'epoch': 0.81}\n",
            "{'loss': 2.2044, 'grad_norm': 0.3448476791381836, 'learning_rate': 1.0600519953136506e-05, 'epoch': 0.81}\n",
            "{'loss': 2.1763, 'grad_norm': 0.3416488766670227, 'learning_rate': 1.0129302121224743e-05, 'epoch': 0.82}\n",
            "{'loss': 2.1767, 'grad_norm': 0.3589450418949127, 'learning_rate': 9.667614390784185e-06, 'epoch': 0.82}\n",
            "{'loss': 2.217, 'grad_norm': 0.3590821623802185, 'learning_rate': 9.215567116816226e-06, 'epoch': 0.82}\n",
            "{'loss': 2.1749, 'grad_norm': 0.36777743697166443, 'learning_rate': 8.773268350010211e-06, 'epoch': 0.83}\n",
            "{'loss': 2.1796, 'grad_norm': 0.35169538855552673, 'learning_rate': 8.34082381091662e-06, 'epoch': 0.83}\n",
            "{'loss': 2.1642, 'grad_norm': 0.3547387421131134, 'learning_rate': 7.918336864677216e-06, 'epoch': 0.84}\n",
            "{'loss': 2.187, 'grad_norm': 0.35706689953804016, 'learning_rate': 7.505908496318159e-06, 'epoch': 0.84}\n",
            "{'loss': 2.1981, 'grad_norm': 0.35227179527282715, 'learning_rate': 7.1036372866120245e-06, 'epoch': 0.85}\n",
            "{'loss': 2.1829, 'grad_norm': 0.34889984130859375, 'learning_rate': 6.711619388514517e-06, 'epoch': 0.85}\n",
            "{'loss': 2.1285, 'grad_norm': 0.34530189633369446, 'learning_rate': 6.329948504181521e-06, 'epoch': 0.86}\n",
            "{'loss': 2.1839, 'grad_norm': 0.3646378219127655, 'learning_rate': 5.958715862571868e-06, 'epoch': 0.86}\n",
            "{'loss': 2.2288, 'grad_norm': 0.3698970079421997, 'learning_rate': 5.5980101976413954e-06, 'epoch': 0.86}\n",
            "{'loss': 2.1994, 'grad_norm': 0.34709569811820984, 'learning_rate': 5.247917727133239e-06, 'epoch': 0.87}\n",
            "{'loss': 2.2052, 'grad_norm': 0.36334776878356934, 'learning_rate': 4.908522131969606e-06, 'epoch': 0.87}\n",
            "{'loss': 2.1612, 'grad_norm': 0.343031644821167, 'learning_rate': 4.579904536249985e-06, 'epoch': 0.88}\n",
            "{'loss': 2.1266, 'grad_norm': 0.3686233162879944, 'learning_rate': 4.2621434878603705e-06, 'epoch': 0.88}\n",
            "{'loss': 2.1733, 'grad_norm': 0.3742804527282715, 'learning_rate': 3.955314939698424e-06, 'epoch': 0.89}\n",
            " 89%|    | 2000/2258 [1:45:30<13:35,  3.16s/it][INFO|trainer.py:3993] 2025-06-19 11:26:04,791 >> Saving model checkpoint to llama3-3b_freeze_1per/checkpoint-2000\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 11:26:04,801 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 11:26:04,803 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 11:26:08,963 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/checkpoint-2000/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 11:26:08,966 >> chat template saved in llama3-3b_freeze_1per/checkpoint-2000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 11:26:08,968 >> tokenizer config file saved in llama3-3b_freeze_1per/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 11:26:08,968 >> Special tokens file saved in llama3-3b_freeze_1per/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 2.1669, 'grad_norm': 0.34640124440193176, 'learning_rate': 3.659492231518741e-06, 'epoch': 0.89}\n",
            "{'loss': 2.1661, 'grad_norm': 0.34861162304878235, 'learning_rate': 3.3747460724028403e-06, 'epoch': 0.89}\n",
            "{'loss': 2.2015, 'grad_norm': 0.3505542576313019, 'learning_rate': 3.1011445238578852e-06, 'epoch': 0.9}\n",
            "{'loss': 2.2233, 'grad_norm': 0.3511661887168884, 'learning_rate': 2.8387529835483205e-06, 'epoch': 0.9}\n",
            "{'loss': 2.1977, 'grad_norm': 0.37963852286338806, 'learning_rate': 2.5876341696641214e-06, 'epoch': 0.91}\n",
            "{'loss': 2.1481, 'grad_norm': 0.35771533846855164, 'learning_rate': 2.3478481059296065e-06, 'epoch': 0.91}\n",
            "{'loss': 2.162, 'grad_norm': 0.35926875472068787, 'learning_rate': 2.1194521072562256e-06, 'epoch': 0.92}\n",
            "{'loss': 2.1613, 'grad_norm': 0.35177379846572876, 'learning_rate': 1.9025007660428861e-06, 'epoch': 0.92}\n",
            "{'loss': 2.0747, 'grad_norm': 0.37042638659477234, 'learning_rate': 1.6970459391269678e-06, 'epoch': 0.93}\n",
            "{'loss': 2.2154, 'grad_norm': 0.3549666404724121, 'learning_rate': 1.503136735389238e-06, 'epoch': 0.93}\n",
            "{'loss': 2.2147, 'grad_norm': 0.3504517376422882, 'learning_rate': 1.3208195040155858e-06, 'epoch': 0.93}\n",
            "{'loss': 2.1635, 'grad_norm': 0.344946026802063, 'learning_rate': 1.1501378234184234e-06, 'epoch': 0.94}\n",
            "{'loss': 2.1843, 'grad_norm': 0.34963223338127136, 'learning_rate': 9.911324908203123e-07, 'epoch': 0.94}\n",
            "{'loss': 2.1974, 'grad_norm': 0.3592987358570099, 'learning_rate': 8.438415125024357e-07, 'epoch': 0.95}\n",
            "{'loss': 2.1659, 'grad_norm': 0.3468186557292938, 'learning_rate': 7.083000947200991e-07, 'epoch': 0.95}\n",
            "{'loss': 2.2009, 'grad_norm': 0.35208258032798767, 'learning_rate': 5.845406352875893e-07, 'epoch': 0.96}\n",
            "{'loss': 2.1622, 'grad_norm': 0.354877769947052, 'learning_rate': 4.7259271583424135e-07, 'epoch': 0.96}\n",
            "{'loss': 2.1621, 'grad_norm': 0.3440960645675659, 'learning_rate': 3.724830947337277e-07, 'epoch': 0.97}\n",
            "{'loss': 2.156, 'grad_norm': 0.39885881543159485, 'learning_rate': 2.8423570070808537e-07, 'epoch': 0.97}\n",
            "{'loss': 2.1886, 'grad_norm': 0.3622981011867523, 'learning_rate': 2.0787162710818042e-07, 'epoch': 0.97}\n",
            "{'loss': 2.1725, 'grad_norm': 0.3352329134941101, 'learning_rate': 1.4340912687185736e-07, 'epoch': 0.98}\n",
            "{'loss': 2.1959, 'grad_norm': 0.35079699754714966, 'learning_rate': 9.086360816102923e-08, 'epoch': 0.98}\n",
            "{'loss': 2.1329, 'grad_norm': 0.35652104020118713, 'learning_rate': 5.024763067873472e-08, 'epoch': 0.99}\n",
            "{'loss': 2.1621, 'grad_norm': 0.34873226284980774, 'learning_rate': 2.157090266703965e-08, 'epoch': 0.99}\n",
            "{'loss': 2.2008, 'grad_norm': 0.3425908088684082, 'learning_rate': 4.840278586548674e-09, 'epoch': 1.0}\n",
            "100%|| 2258/2258 [1:59:11<00:00,  2.82s/it][INFO|trainer.py:3993] 2025-06-19 11:39:45,406 >> Saving model checkpoint to llama3-3b_freeze_1per/checkpoint-2258\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 11:39:45,407 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-2258/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 11:39:45,407 >> Configuration saved in llama3-3b_freeze_1per/checkpoint-2258/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 11:39:49,404 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/checkpoint-2258/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 11:39:49,405 >> chat template saved in llama3-3b_freeze_1per/checkpoint-2258/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 11:39:49,406 >> tokenizer config file saved in llama3-3b_freeze_1per/checkpoint-2258/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 11:39:49,406 >> Special tokens file saved in llama3-3b_freeze_1per/checkpoint-2258/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-06-19 11:39:51,200 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7161.1044, 'train_samples_per_second': 5.043, 'train_steps_per_second': 0.315, 'train_loss': 2.2117355995414743, 'epoch': 1.0}\n",
            "100%|| 2258/2258 [1:59:17<00:00,  3.17s/it]\n",
            "[INFO|trainer.py:3993] 2025-06-19 11:39:51,212 >> Saving model checkpoint to llama3-3b_freeze_1per\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 11:39:51,213 >> Configuration saved in llama3-3b_freeze_1per/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 11:39:51,213 >> Configuration saved in llama3-3b_freeze_1per/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 11:39:55,177 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 11:39:55,177 >> chat template saved in llama3-3b_freeze_1per/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 11:39:55,178 >> tokenizer config file saved in llama3-3b_freeze_1per/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 11:39:55,178 >> Special tokens file saved in llama3-3b_freeze_1per/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =          1.0\n",
            "  total_flos               = 1165030041GF\n",
            "  train_loss               =       2.2117\n",
            "  train_runtime            =   1:59:21.10\n",
            "  train_samples_per_second =        5.043\n",
            "  train_steps_per_second   =        0.315\n",
            "Figure saved at: llama3-3b_freeze_1per/training_loss.png\n",
            "[WARNING|2025-06-19 11:39:55] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[INFO|modelcard.py:450] 2025-06-19 11:39:55,695 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "# !llamafactory-cli train CPT_LayerFreezing_1per.json\n",
        "!llamafactory-cli train CPT_LayerFreezing_1per.json\n",
        "# !llamafactory-cli train CPT_LayerFreezing_1per_8layers.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHE3I_8uZQgN"
      },
      "source": [
        "## Progress Report - June 14th, 2024\n",
        "\n",
        "**Today's Progress:**\n",
        "\n",
        "*   Performed Pre-training (CPT) with Layer Freezing on a 1% subset of the Wikipedia dataset using LLaMA-Factory.\n",
        "*   Completed the training process and saved checkpoints.\n",
        "*   Conducted inference using the pre-trained model checkpoints.\n",
        "\n",
        "**Challenges Encountered:**\n",
        "\n",
        "*   Experienced issues with system resources, specifically running out of RAM and GPU space, which required optimization and memory management steps.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "*   Further inference and model merging.\n",
        "*   Integrate and work with Ramzan's dataset.\n",
        "*   Perform instruction tuning on saved checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKOBX6CCWAV2"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "MeDp6XQEZc36",
        "outputId": "655eb81a-1cc7-4fe6-8b73-f552edf35ed4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llamafactory'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllamafactory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llamafactory'"
          ]
        }
      ],
      "source": [
        "import torch, gc\n",
        "from llamafactory.chat import ChatModel\n",
        "\n",
        "# Cleanup\n",
        "try:\n",
        "    del model\n",
        "    del trainer\n",
        "    del inputs\n",
        "except NameError:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Config\n",
        "args = {\n",
        "    \"model_name_or_path\": \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3-3b_freeze_1per\",\n",
        "    \"template\": \"llama3\",\n",
        "    \"quantization_bit\": 8,\n",
        "    # \"flash_attn\": True\n",
        "}\n",
        "\n",
        "# Load\n",
        "print(\"Initializing model...\")\n",
        "try:\n",
        "    chat_model = ChatModel(args)\n",
        "    print(\" Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\" Critical error: {e}\")\n",
        "    raise RuntimeError(\"Model loading failed after all attempts\")\n",
        "\n",
        "messages = []\n",
        "print(\"\\nModel ready for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67FGOK2mZjVM"
      },
      "outputs": [],
      "source": [
        "#  [Block 2: Chat Interface Setup]\n",
        "def process_query(query):\n",
        "    \"\"\"Process user query and generate response\"\"\"\n",
        "    global messages\n",
        "\n",
        "    if query.strip().lower() == \"exit\":\n",
        "        return False, \"Exiting...\"\n",
        "\n",
        "    if query.strip().lower() == \"clear\":\n",
        "        messages = []\n",
        "        torch_gc()\n",
        "        return True, \" Conversation history cleared\"\n",
        "\n",
        "    # Add user message to history\n",
        "    messages.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "    # Generate response\n",
        "    print(\"Assistant: \", end=\"\", flush=True)\n",
        "    response = \"\"\n",
        "    try:\n",
        "        for new_text in chat_model.stream_chat(messages):\n",
        "            print(new_text, end=\"\", flush=True)\n",
        "            response += new_text\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        return True, f\" Error generating response: {e}\"\n",
        "\n",
        "    # Add assistant response to history\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    return True, \"\"\n",
        "\n",
        "# Print welcome message\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Welcome to LLaMA Chat Interface!\")\n",
        "print(\"Commands:\")\n",
        "print(\"- Type 'clear' to reset conversation history\")\n",
        "print(\"- Type 'exit' to end the session\")\n",
        "print(\"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAk-mE1EZpN9"
      },
      "outputs": [],
      "source": [
        "#  [Block 3: Interactive Chat Session]\n",
        "while True:\n",
        "    try:\n",
        "        # Get user input\n",
        "        query = input(\"User: \")\n",
        "\n",
        "        # Process query\n",
        "        continue_session, output = process_query(query)\n",
        "\n",
        "        # Handle special commands\n",
        "        if output:\n",
        "            print(output)\n",
        "\n",
        "        # Exit condition\n",
        "        if not continue_session:\n",
        "            break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n Session interrupted by user\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Unexpected error: {e}\")\n",
        "        print(\"Resetting conversation...\")\n",
        "        messages = []\n",
        "        torch_gc()\n",
        "\n",
        "# Cleanup\n",
        "# torch_gc()\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Session ended. GPU resources freed.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIOeZWAHS-LR"
      },
      "source": [
        "## Merging with  Freez Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ubUAdMFmjO",
        "outputId": "cca6ebda-c2cd-4afd-9597-c99cc8c7c90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/LLaMA-Factory/'\n",
            "/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,750 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,750 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,750 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,750 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,750 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,750 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 12:10:55,974 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-19 12:10:55,975 >> loading configuration file llama3-3b_freeze_1per/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 12:10:55,976 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,977 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,977 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,977 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,977 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,977 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:10:55,977 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 12:10:56,214 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-06-19 12:10:56] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|configuration_utils.py:696] 2025-06-19 12:10:56,230 >> loading configuration file llama3-3b_freeze_1per/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 12:10:56,230 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 12:10:56] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-19 12:10:56,353 >> loading weights file llama3-3b_freeze_1per/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-19 12:10:56,353 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 12:10:56,353 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 2/2 [00:00<00:00, 23.36it/s]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-19 12:10:56,454 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-19 12:10:56,454 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at llama3-3b_freeze_1per.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-19 12:10:56,455 >> loading configuration file llama3-3b_freeze_1per/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 12:10:56,455 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 12:10:56] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-19 12:10:56] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
            "[INFO|2025-06-19 12:10:56] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 12:10:56,461 >> Configuration saved in llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 12:10:56,461 >> Configuration saved in llama3_freeze_merged/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 12:10:58,841 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_freeze_merged/model.safetensors.index.json.\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 12:11:00,253 >> Configuration saved in llama3-3b_freeze_1per/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 12:11:00,254 >> Configuration saved in llama3-3b_freeze_1per/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 12:11:02,578 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at llama3-3b_freeze_1per/model.safetensors.index.json.\n",
            "[INFO|hub.py:827] 2025-06-19 12:11:05,080 >> Uploading the following files to wbasharat/llama3-3b_freeze_1per: model.safetensors.index.json,model-00002-of-00004.safetensors,generation_config.json,model-00001-of-00004.safetensors,model-00004-of-00004.safetensors,README.md,model-00003-of-00004.safetensors,config.json\n",
            "Uploading...: 100%|| 6.43G/6.43G [05:22<00:00, 19.9MB/s]\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 12:16:31,687 >> chat template saved in llama3_freeze_merged/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 12:16:31,688 >> tokenizer config file saved in llama3_freeze_merged/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 12:16:31,689 >> Special tokens file saved in llama3_freeze_merged/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 12:16:32,317 >> chat template saved in llama3-3b_freeze_1per/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 12:16:32,318 >> tokenizer config file saved in llama3-3b_freeze_1per/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 12:16:32,318 >> Special tokens file saved in llama3-3b_freeze_1per/special_tokens_map.json\n",
            "[INFO|hub.py:827] 2025-06-19 12:16:32,443 >> Uploading the following files to wbasharat/llama3-3b_freeze_1per: special_tokens_map.json,tokenizer_config.json,chat_template.jinja,tokenizer.json,README.md\n",
            "Uploading...: 100%|| 17.2M/17.2M [00:02<00:00, 7.16MB/s]\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "[INFO|2025-06-19 12:16:36] llamafactory.train.tuner:143 >> Ollama modelfile saved in llama3_freeze_merged/Modelfile\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"llama3-3b_freeze_1per\",                # use official non-quantized Llama-3-8B-Instruct model                      # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                        # same to the one in training\n",
        "  finetuning_type=\"freeze\",                                   # same to the one in training\n",
        "  export_dir=\"llama3_freeze_merged\",                          # the path to save the merged model\n",
        "  export_size=2,                                            # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                                      # the device used in export, can be chosen from `cpu` and `auto`\n",
        "  export_hub_model_id=\"wbasharat/llama3-3b_freeze_1per\",               # the Hugging Face hub ID to upload model\n",
        ")\n",
        "json.dump(args, open(\"merged_freeze_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merged_freeze_llama3.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB-vnZrmmyfz",
        "outputId": "376ec820-c126-4490-83c3-7243252a7278"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llamafactory'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllamafactory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Clean up\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llamafactory'"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "from llamafactory.chat import ChatModel\n",
        "\n",
        "# 1. Clean up\n",
        "for name in (\"chat_model\", \"messages\"):\n",
        "    if name in globals():\n",
        "        del globals()[name]\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Inference config\n",
        "inference_args = {\n",
        "    \"model_name_or_path\": \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged\",\n",
        "    \"template\": \"llama3\",\n",
        "    \"quantization_bit\": 8,  # optional: remove if not using 8bit quant\n",
        "}\n",
        "\n",
        "# 3. Load the model\n",
        "print(\"Initializing ChatModel\")\n",
        "chat_model = ChatModel(inference_args)\n",
        "\n",
        "# 4. Report device via torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Model loaded on {device}\\n\")\n",
        "\n",
        "# 5. Chat loop\n",
        "messages = []\n",
        "print(\"=\"*50)\n",
        "print(\"Type 'exit' to quit, 'clear' to reset history\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "while True:\n",
        "    query = input(\"User: \").strip()\n",
        "    if not query:\n",
        "        continue\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\" Goodbye!\")\n",
        "        break\n",
        "    if query.lower() == \"clear\":\n",
        "        messages = []\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\" Conversation history cleared.\\n\")\n",
        "        continue\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": query})\n",
        "    print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in chat_model.stream_chat(messages):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "        response += chunk\n",
        "    print(\"\\n\")\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axsv1TwITsKh"
      },
      "source": [
        "# Instruction Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcbnTjfAlluR"
      },
      "source": [
        "### Dataset & Model Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZmfyNtGmyf0"
      },
      "source": [
        "## Data Loading & pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AIwHmm6myf0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "###changed the dataset for testing purposes\n",
        "\n",
        "# alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-korean\", split=\"train\")\n",
        "alpaca_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "# alpaca_dataset = load_dataset(\"vicgalle/alpaca-gpt4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wvqZYPFmyf1",
        "outputId": "1d179b10-8c41-40ca-ffa8-aacd32dfe3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading Alpaca dataset (English version) from 'tatsu-lab/alpaca'...\n",
            " Loaded 52002 samples from Alpaca dataset.\n",
            " Formatting Alpaca dataset into LLaMA-Factory SFT format...\n",
            " Successfully formatted 52002 samples.\n",
            " Saving formatted dataset to /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/alpaca_instruction_tuning.json...\n",
            " Saved 52002 samples to /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/alpaca_instruction_tuning.json\n",
            " Checking and updating /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/dataset_info.json...\n",
            " Loaded existing dataset_info.json.\n",
            " Updated dataset info file at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data/dataset_info.json with 'alpaca_en' entry.\n",
            "\n",
            "Script execution complete. Your data is ready for LLaMA-Factory training!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Configuration ---\n",
        "# Directory where the processed data and info files will be saved\n",
        "output_dir = \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/data\"\n",
        "# Name for the formatted Alpaca instruction tuning dataset\n",
        "output_file_name = \"alpaca_instruction_tuning.json\"\n",
        "# Full path for the output dataset file\n",
        "output_file_path = os.path.join(output_dir, output_file_name)\n",
        "# Full path for the dataset information file\n",
        "dataset_info_file_path = os.path.join(output_dir, \"dataset_info.json\")\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# --- Data Loading ---\n",
        "print(\" Loading Alpaca dataset (English version) from 'tatsu-lab/alpaca'...\")\n",
        "alpaca_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "print(f\" Loaded {len(alpaca_dataset)} samples from Alpaca dataset.\")\n",
        "\n",
        "# --- Data Formatting Function ---\n",
        "def format_alpaca_for_llama_factory_sft(example):\n",
        "    \"\"\"\n",
        "    Formats a single Alpaca example into the LLaMA-Factory SFT (Supervised Fine-Tuning) format.\n",
        "    The instruction and input fields from the original Alpaca dataset are combined into\n",
        "    the 'instruction' field for LLaMA-Factory, which serves as the user's prompt.\n",
        "    The 'input' field in the LLaMA-Factory format is left empty as per the desired structure.\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are a helpful assistant. Respond to the user's request.\"\n",
        "\n",
        "    # Combine instruction and input into a single 'instruction' field for LLaMA-Factory\n",
        "    # If there's an input, append it after a newline for clarity\n",
        "    formatted_instruction = example[\"instruction\"]\n",
        "    if example[\"input\"]:\n",
        "        formatted_instruction = f\"{formatted_instruction}\\n\\nInput: {example['input']}\"\n",
        "\n",
        "    return {\n",
        "        \"instruction\": formatted_instruction,  # This will be mapped to LLaMA-Factory's 'prompt' and 'query'\n",
        "        \"input\": \"\",                           # Explicitly empty as per desired format\n",
        "        \"output\": example[\"output\"],           # The model's expected response\n",
        "        \"system\": system_prompt,               # A fixed system prompt\n",
        "        \"history\": []                          # Empty history for single-turn conversations\n",
        "    }\n",
        "\n",
        "# --- Apply Formatting ---\n",
        "print(\" Formatting Alpaca dataset into LLaMA-Factory SFT format...\")\n",
        "formatted_data = [format_alpaca_for_llama_factory_sft(ex) for ex in alpaca_dataset]\n",
        "print(f\" Successfully formatted {len(formatted_data)} samples.\")\n",
        "\n",
        "# --- Save Formatted Dataset ---\n",
        "print(f\" Saving formatted dataset to {output_file_path}...\")\n",
        "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(formatted_data, f, indent=2, ensure_ascii=False)\n",
        "print(f\" Saved {len(formatted_data)} samples to {output_file_path}\")\n",
        "\n",
        "# --- Update dataset_info.json ---\n",
        "# Define the entry for the Alpaca English dataset\n",
        "alpaca_en_info = {\n",
        "    \"alpaca_en\": {\n",
        "        \"file_name\": output_file_name,\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",  # This is the full instruction/query presented to the user\n",
        "            \"query\": \"instruction\",   # LLaMA-Factory expects the user's query here.\n",
        "                                      # Since we merged original 'instruction' and 'input' into\n",
        "                                      # the new 'instruction' field, we map 'query' to it.\n",
        "            \"response\": \"output\",     # The model's desired output\n",
        "            \"system\": \"system\",       # The system message\n",
        "            \"history\": \"history\"      # Conversation history (empty for single-turn)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\" Checking and updating {dataset_info_file_path}...\")\n",
        "\n",
        "# Load existing dataset_info.json if it exists, otherwise start with an empty dictionary\n",
        "current_dataset_info = {}\n",
        "if os.path.exists(dataset_info_file_path):\n",
        "    try:\n",
        "        with open(dataset_info_file_path, 'r', encoding='utf-8') as f:\n",
        "            current_dataset_info = json.load(f)\n",
        "        print(f\" Loaded existing dataset_info.json.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\" Warning: Could not decode existing {dataset_info_file_path}. Starting fresh.\")\n",
        "        current_dataset_info = {}\n",
        "\n",
        "# Update the existing data with the new Alpaca entry\n",
        "# This will add \"alpaca_en\" if it's new, or update it if it already exists\n",
        "current_dataset_info.update(alpaca_en_info)\n",
        "\n",
        "# Save the updated dataset_info.json\n",
        "with open(dataset_info_file_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(current_dataset_info, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\" Updated dataset info file at {dataset_info_file_path} with 'alpaca_en' entry.\")\n",
        "print(\"\\nScript execution complete. Your data is ready for LLaMA-Factory training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yty5Cpi0myf1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    model_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged\",  # Pre-training checkpoint\n",
        "    template=\"llama3\",\n",
        "    trust_remote_code=True,\n",
        "    # Training stage\n",
        "    stage=\"sft\",\n",
        "    do_train=True,\n",
        "    finetuning_type=\"freeze\",\n",
        "    freeze_trainable_layers=4,\n",
        "    freeze_trainable_modules=\"all\",\n",
        "\n",
        "    # Dataset settings\n",
        "    dataset=\"alpaca_en\",\n",
        "    cutoff_len=2048,\n",
        "    # max_samples=1000,\n",
        "    overwrite_cache=True,\n",
        "    preprocessing_num_workers=16,\n",
        "    dataloader_num_workers=4,\n",
        "\n",
        "    # Output and checkpointing\n",
        "    output_dir=\"llama3_3b_freeze_instructionTuning\",\n",
        "    overwrite_output_dir=True,\n",
        "    logging_steps=10,\n",
        "    # save_steps=500,\n",
        "    # save_strategy=\"steps\",\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # eval_steps=500,\n",
        "    # load_best_model_at_end=True,\n",
        "    # metric_for_best_model=\"eval_loss\",\n",
        "    # plot_loss=True,\n",
        "\n",
        "    # Optimizer and schedule\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1.0e-5,\n",
        "    num_train_epochs=1.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # Precision and device\n",
        "    bf16=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP_Ma0hamyf2"
      },
      "source": [
        "```\n",
        "\n",
        "  \"alpaca_instruction_tuning\": {\n",
        "    \"file_name\": \"alpaca_instruction_tuning.json\",\n",
        "    \"columns\": {\n",
        "      \"instruction\": \"instruction\",\n",
        "      \"input\": \"input\",\n",
        "      \"output\": \"output\"\n",
        "    }\n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IAMHAX2r2NF"
      },
      "outputs": [],
      "source": [
        "json.dump(args, open(\"InstructionTuning_LayerFreezing_Alpaca.json\", \"w\", encoding=\"utf-8\"), indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftb_Ymz4sJzw",
        "outputId": "e4fa91f0-6cb9-4311-e9b7-063f81c2fe41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|2025-06-19 12:18:52] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:48031\n",
            "W0619 12:18:53.192000 10766 site-packages/torch/distributed/run.py:766] \n",
            "W0619 12:18:53.192000 10766 site-packages/torch/distributed/run.py:766] *****************************************\n",
            "W0619 12:18:53.192000 10766 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0619 12:18:53.192000 10766 site-packages/torch/distributed/run.py:766] *****************************************\n",
            "[INFO|2025-06-19 12:18:55] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,599 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,599 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,599 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,599 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,599 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,599 >> loading file chat_template.jinja\n",
            "[INFO|2025-06-19 12:18:55] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 12:18:55,855 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-19 12:18:55,855 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 12:18:55,857 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,857 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,857 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,857 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,857 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,857 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 12:18:55,857 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 12:18:56,128 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-06-19 12:18:56] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-06-19 12:18:56] llamafactory.data.loader:143 >> Loading dataset alpaca_instruction_tuning.json...\n",
            "/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank1]:[W619 12:18:56.267497246 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 52002 examples [00:00, 147764.58 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100%|| 52002/52002 [00:00<00:00, 28\n",
            "/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W619 12:18:57.701382329 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
            "Running tokenizer on dataset (num_proc=16): 100%|| 52002/52002 [00:02<00:00, 18\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 40633, 311, 279, 1217, 596, 1715, 13, 128009, 128006, 882, 128007, 271, 36227, 2380, 10631, 369, 19994, 9498, 627, 36227, 2380, 10631, 369, 19994, 9498, 13, 128009, 128006, 78191, 128007, 271, 16, 5253, 266, 264, 24770, 10173, 323, 1304, 2771, 311, 2997, 11510, 315, 26390, 323, 24822, 13, 720, 17, 13, 33918, 15870, 311, 2567, 701, 2547, 4642, 323, 3831, 13, 720, 18, 13, 2175, 3403, 6212, 323, 10519, 264, 13263, 6212, 9899, 13, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant. Respond to the user's request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Give three tips for staying healthy.\n",
            "Give three tips for staying healthy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 5253, 266, 264, 24770, 10173, 323, 1304, 2771, 311, 2997, 11510, 315, 26390, 323, 24822, 13, 720, 17, 13, 33918, 15870, 311, 2567, 701, 2547, 4642, 323, 3831, 13, 720, 18, 13, 2175, 3403, 6212, 323, 10519, 264, 13263, 6212, 9899, 13, 128009]\n",
            "labels:\n",
            "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.<|eot_id|>\n",
            "[INFO|configuration_utils.py:696] 2025-06-19 12:19:00,747 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 12:19:00,747 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 12:19:00] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-19 12:19:00,841 >> loading weights file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-19 12:19:00,841 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 12:19:00,842 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:00<00:00,  7.23it/s]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-19 12:19:01,406 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-19 12:19:01,406 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-19 12:19:01,407 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 12:19:01,407 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 12:19:01] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-06-19 12:19:01] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-19 12:19:01] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-06-19 12:19:01] llamafactory.model.adapter:143 >> Fine-tuning method: Freeze\n",
            "[INFO|2025-06-19 12:19:01] llamafactory.model.adapter:143 >> Set trainable layers: .24.,.25.,.26.,.27.\n",
            "[INFO|2025-06-19 12:19:01] llamafactory.model.loader:143 >> trainable params: 402,677,760 || all params: 3,212,749,824 || trainable%: 12.5337\n",
            "[INFO|trainer.py:756] 2025-06-19 12:19:01,427 >> Using auto half precision backend\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.03s/it]\n",
            "[INFO|trainer.py:2409] 2025-06-19 12:19:05,734 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-06-19 12:19:05,734 >>   Num examples = 52,002\n",
            "[INFO|trainer.py:2411] 2025-06-19 12:19:05,734 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2412] 2025-06-19 12:19:05,734 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2415] 2025-06-19 12:19:05,734 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2416] 2025-06-19 12:19:05,734 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2417] 2025-06-19 12:19:05,734 >>   Total optimization steps = 3,251\n",
            "[INFO|trainer.py:2418] 2025-06-19 12:19:05,734 >>   Number of trainable parameters = 402,677,760\n",
            "{'loss': 1.5937, 'grad_norm': 2.188612937927246, 'learning_rate': 2.760736196319019e-07, 'epoch': 0.0}\n",
            "{'loss': 1.7112, 'grad_norm': 2.125214099884033, 'learning_rate': 5.828220858895705e-07, 'epoch': 0.01}\n",
            "{'loss': 1.5615, 'grad_norm': 2.523192882537842, 'learning_rate': 8.895705521472393e-07, 'epoch': 0.01}\n",
            "{'loss': 1.6321, 'grad_norm': 1.9093602895736694, 'learning_rate': 1.196319018404908e-06, 'epoch': 0.01}\n",
            "{'loss': 1.6749, 'grad_norm': 1.9947975873947144, 'learning_rate': 1.5030674846625768e-06, 'epoch': 0.02}\n",
            "{'loss': 1.6495, 'grad_norm': 2.474825620651245, 'learning_rate': 1.8098159509202457e-06, 'epoch': 0.02}\n",
            "{'loss': 1.5864, 'grad_norm': 2.274228096008301, 'learning_rate': 2.1165644171779144e-06, 'epoch': 0.02}\n",
            "{'loss': 1.5427, 'grad_norm': 2.0482068061828613, 'learning_rate': 2.423312883435583e-06, 'epoch': 0.02}\n",
            "{'loss': 1.6266, 'grad_norm': 2.1983842849731445, 'learning_rate': 2.7300613496932514e-06, 'epoch': 0.03}\n",
            "{'loss': 1.6293, 'grad_norm': 1.9698982238769531, 'learning_rate': 3.0368098159509207e-06, 'epoch': 0.03}\n",
            "{'loss': 1.522, 'grad_norm': 1.610905647277832, 'learning_rate': 3.343558282208589e-06, 'epoch': 0.03}\n",
            "{'loss': 1.5245, 'grad_norm': 1.994406819343567, 'learning_rate': 3.650306748466258e-06, 'epoch': 0.04}\n",
            "{'loss': 1.5532, 'grad_norm': 3.490478038787842, 'learning_rate': 3.957055214723927e-06, 'epoch': 0.04}\n",
            "{'loss': 1.543, 'grad_norm': 1.896549105644226, 'learning_rate': 4.2638036809815955e-06, 'epoch': 0.04}\n",
            "{'loss': 1.4727, 'grad_norm': 1.6426506042480469, 'learning_rate': 4.570552147239265e-06, 'epoch': 0.05}\n",
            "{'loss': 1.4876, 'grad_norm': 1.8614815473556519, 'learning_rate': 4.877300613496933e-06, 'epoch': 0.05}\n",
            "{'loss': 1.5396, 'grad_norm': 1.5803849697113037, 'learning_rate': 5.184049079754602e-06, 'epoch': 0.05}\n",
            "{'loss': 1.5297, 'grad_norm': 1.9359421730041504, 'learning_rate': 5.490797546012271e-06, 'epoch': 0.06}\n",
            "{'loss': 1.5209, 'grad_norm': 1.6397360563278198, 'learning_rate': 5.797546012269939e-06, 'epoch': 0.06}\n",
            "{'loss': 1.5387, 'grad_norm': 2.3023014068603516, 'learning_rate': 6.104294478527608e-06, 'epoch': 0.06}\n",
            "{'loss': 1.4715, 'grad_norm': 1.4689282178878784, 'learning_rate': 6.411042944785276e-06, 'epoch': 0.06}\n",
            "{'loss': 1.5548, 'grad_norm': 1.7797211408615112, 'learning_rate': 6.717791411042945e-06, 'epoch': 0.07}\n",
            "{'loss': 1.4991, 'grad_norm': 1.6161227226257324, 'learning_rate': 7.0245398773006145e-06, 'epoch': 0.07}\n",
            "{'loss': 1.4547, 'grad_norm': 2.026249408721924, 'learning_rate': 7.331288343558283e-06, 'epoch': 0.07}\n",
            "{'loss': 1.4975, 'grad_norm': 1.7401193380355835, 'learning_rate': 7.638036809815951e-06, 'epoch': 0.08}\n",
            "{'loss': 1.5027, 'grad_norm': 1.7252579927444458, 'learning_rate': 7.944785276073619e-06, 'epoch': 0.08}\n",
            "{'loss': 1.4344, 'grad_norm': 1.8093215227127075, 'learning_rate': 8.251533742331288e-06, 'epoch': 0.08}\n",
            "{'loss': 1.4256, 'grad_norm': 1.5291951894760132, 'learning_rate': 8.558282208588958e-06, 'epoch': 0.09}\n",
            "{'loss': 1.5048, 'grad_norm': 1.6975511312484741, 'learning_rate': 8.865030674846627e-06, 'epoch': 0.09}\n",
            "{'loss': 1.4868, 'grad_norm': 1.7274008989334106, 'learning_rate': 9.171779141104295e-06, 'epoch': 0.09}\n",
            "{'loss': 1.5118, 'grad_norm': 1.9936004877090454, 'learning_rate': 9.478527607361964e-06, 'epoch': 0.1}\n",
            "{'loss': 1.4176, 'grad_norm': 1.6821800470352173, 'learning_rate': 9.785276073619633e-06, 'epoch': 0.1}\n",
            "{'loss': 1.547, 'grad_norm': 2.1153244972229004, 'learning_rate': 9.999974044455328e-06, 'epoch': 0.1}\n",
            "{'loss': 1.4143, 'grad_norm': 1.5774905681610107, 'learning_rate': 9.999512620046523e-06, 'epoch': 0.1}\n",
            "{'loss': 1.4506, 'grad_norm': 1.5233283042907715, 'learning_rate': 9.998474467024492e-06, 'epoch': 0.11}\n",
            "{'loss': 1.3972, 'grad_norm': 1.5956811904907227, 'learning_rate': 9.996859705147424e-06, 'epoch': 0.11}\n",
            "{'loss': 1.429, 'grad_norm': 1.6387439966201782, 'learning_rate': 9.994668520689349e-06, 'epoch': 0.11}\n",
            "{'loss': 1.4741, 'grad_norm': 1.637653112411499, 'learning_rate': 9.99190116641866e-06, 'epoch': 0.12}\n",
            "{'loss': 1.4454, 'grad_norm': 2.1729509830474854, 'learning_rate': 9.988557961568956e-06, 'epoch': 0.12}\n",
            "{'loss': 1.422, 'grad_norm': 1.8631823062896729, 'learning_rate': 9.984639291802206e-06, 'epoch': 0.12}\n",
            "{'loss': 1.3755, 'grad_norm': 1.4277979135513306, 'learning_rate': 9.980145609164269e-06, 'epoch': 0.13}\n",
            "{'loss': 1.4443, 'grad_norm': 1.774747610092163, 'learning_rate': 9.975077432032749e-06, 'epoch': 0.13}\n",
            "{'loss': 1.5161, 'grad_norm': 2.9450862407684326, 'learning_rate': 9.969435345057186e-06, 'epoch': 0.13}\n",
            "{'loss': 1.3846, 'grad_norm': 2.249408483505249, 'learning_rate': 9.963219999091624e-06, 'epoch': 0.14}\n",
            "{'loss': 1.3818, 'grad_norm': 1.6821365356445312, 'learning_rate': 9.956432111119522e-06, 'epoch': 0.14}\n",
            "{'loss': 1.3793, 'grad_norm': 1.3960992097854614, 'learning_rate': 9.949072464171053e-06, 'epoch': 0.14}\n",
            "{'loss': 1.3938, 'grad_norm': 2.068983316421509, 'learning_rate': 9.941141907232766e-06, 'epoch': 0.14}\n",
            "{'loss': 1.5531, 'grad_norm': 1.7519304752349854, 'learning_rate': 9.932641355149655e-06, 'epoch': 0.15}\n",
            "{'loss': 1.4123, 'grad_norm': 1.5993551015853882, 'learning_rate': 9.923571788519632e-06, 'epoch': 0.15}\n",
            "{'loss': 1.4033, 'grad_norm': 1.5582642555236816, 'learning_rate': 9.913934253580397e-06, 'epoch': 0.15}\n",
            " 15%|                                | 500/3251 [15:42<1:26:02,  1.88s/it][INFO|trainer.py:3993] 2025-06-19 12:34:51,823 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-500\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 12:34:51,837 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 12:34:51,841 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 12:34:56,162 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-500/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 12:34:56,166 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 12:34:56,170 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 12:34:56,170 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.4043, 'grad_norm': 2.0095372200012207, 'learning_rate': 9.903729862088749e-06, 'epoch': 0.16}\n",
            "{'loss': 1.4969, 'grad_norm': 1.5438120365142822, 'learning_rate': 9.892959791192342e-06, 'epoch': 0.16}\n",
            "{'loss': 1.4685, 'grad_norm': 2.087374210357666, 'learning_rate': 9.881625283293895e-06, 'epoch': 0.16}\n",
            "{'loss': 1.4364, 'grad_norm': 1.5517823696136475, 'learning_rate': 9.86972764590786e-06, 'epoch': 0.17}\n",
            "{'loss': 1.4723, 'grad_norm': 1.7580238580703735, 'learning_rate': 9.857268251509602e-06, 'epoch': 0.17}\n",
            "{'loss': 1.3768, 'grad_norm': 1.8415729999542236, 'learning_rate': 9.844248537377074e-06, 'epoch': 0.17}\n",
            "{'loss': 1.5035, 'grad_norm': 1.6531018018722534, 'learning_rate': 9.830670005425012e-06, 'epoch': 0.18}\n",
            "{'loss': 1.4301, 'grad_norm': 1.4717234373092651, 'learning_rate': 9.81653422203168e-06, 'epoch': 0.18}\n",
            "{'loss': 1.3342, 'grad_norm': 1.9278172254562378, 'learning_rate': 9.801842817858178e-06, 'epoch': 0.18}\n",
            "{'loss': 1.4657, 'grad_norm': 1.7053804397583008, 'learning_rate': 9.786597487660336e-06, 'epoch': 0.18}\n",
            "{'loss': 1.4247, 'grad_norm': 1.8427103757858276, 'learning_rate': 9.77079999009321e-06, 'epoch': 0.19}\n",
            "{'loss': 1.4101, 'grad_norm': 2.0634958744049072, 'learning_rate': 9.754452147508208e-06, 'epoch': 0.19}\n",
            "{'loss': 1.4387, 'grad_norm': 1.5164633989334106, 'learning_rate': 9.737555845742869e-06, 'epoch': 0.19}\n",
            "{'loss': 1.4727, 'grad_norm': 1.4632337093353271, 'learning_rate': 9.720113033903323e-06, 'epoch': 0.2}\n",
            "{'loss': 1.4951, 'grad_norm': 1.9438015222549438, 'learning_rate': 9.702125724139439e-06, 'epoch': 0.2}\n",
            "{'loss': 1.4199, 'grad_norm': 1.6583399772644043, 'learning_rate': 9.683595991412725e-06, 'epoch': 0.2}\n",
            "{'loss': 1.4897, 'grad_norm': 1.9592790603637695, 'learning_rate': 9.664525973256949e-06, 'epoch': 0.21}\n",
            "{'loss': 1.3749, 'grad_norm': 2.0686733722686768, 'learning_rate': 9.644917869531567e-06, 'epoch': 0.21}\n",
            "{'loss': 1.4326, 'grad_norm': 1.5898768901824951, 'learning_rate': 9.624773942167958e-06, 'epoch': 0.21}\n",
            "{'loss': 1.4528, 'grad_norm': 1.9041224718093872, 'learning_rate': 9.604096514908492e-06, 'epoch': 0.22}\n",
            "{'loss': 1.4811, 'grad_norm': 1.5599472522735596, 'learning_rate': 9.582887973038463e-06, 'epoch': 0.22}\n",
            "{'loss': 1.4674, 'grad_norm': 1.634364128112793, 'learning_rate': 9.561150763110945e-06, 'epoch': 0.22}\n",
            "{'loss': 1.4873, 'grad_norm': 1.9966652393341064, 'learning_rate': 9.538887392664544e-06, 'epoch': 0.22}\n",
            "{'loss': 1.3356, 'grad_norm': 1.9898709058761597, 'learning_rate': 9.516100429934158e-06, 'epoch': 0.23}\n",
            "{'loss': 1.4812, 'grad_norm': 1.6077020168304443, 'learning_rate': 9.492792503554695e-06, 'epoch': 0.23}\n",
            "{'loss': 1.4843, 'grad_norm': 1.735878825187683, 'learning_rate': 9.468966302257858e-06, 'epoch': 0.23}\n",
            "{'loss': 1.3604, 'grad_norm': 1.58035147190094, 'learning_rate': 9.444624574561965e-06, 'epoch': 0.24}\n",
            "{'loss': 1.4034, 'grad_norm': 1.6289101839065552, 'learning_rate': 9.4197701284549e-06, 'epoch': 0.24}\n",
            "{'loss': 1.4941, 'grad_norm': 1.4807171821594238, 'learning_rate': 9.394405831070186e-06, 'epoch': 0.24}\n",
            "{'loss': 1.4321, 'grad_norm': 1.7850133180618286, 'learning_rate': 9.368534608356244e-06, 'epoch': 0.25}\n",
            "{'loss': 1.4377, 'grad_norm': 2.0035603046417236, 'learning_rate': 9.342159444738865e-06, 'epoch': 0.25}\n",
            "{'loss': 1.4167, 'grad_norm': 1.8204973936080933, 'learning_rate': 9.315283382776932e-06, 'epoch': 0.25}\n",
            "{'loss': 1.4293, 'grad_norm': 1.9443880319595337, 'learning_rate': 9.287909522811442e-06, 'epoch': 0.26}\n",
            "{'loss': 1.3608, 'grad_norm': 1.5358123779296875, 'learning_rate': 9.26004102260786e-06, 'epoch': 0.26}\n",
            "{'loss': 1.4321, 'grad_norm': 2.1366469860076904, 'learning_rate': 9.231681096991852e-06, 'epoch': 0.26}\n",
            "{'loss': 1.3899, 'grad_norm': 1.4529597759246826, 'learning_rate': 9.202833017478421e-06, 'epoch': 0.26}\n",
            "{'loss': 1.3503, 'grad_norm': 2.1444785594940186, 'learning_rate': 9.173500111894536e-06, 'epoch': 0.27}\n",
            "{'loss': 1.3797, 'grad_norm': 1.954200029373169, 'learning_rate': 9.14368576399522e-06, 'epoch': 0.27}\n",
            "{'loss': 1.4567, 'grad_norm': 1.826448678970337, 'learning_rate': 9.113393413073224e-06, 'epoch': 0.27}\n",
            "{'loss': 1.4274, 'grad_norm': 2.2636632919311523, 'learning_rate': 9.08262655356228e-06, 'epoch': 0.28}\n",
            "{'loss': 1.3493, 'grad_norm': 1.8828189373016357, 'learning_rate': 9.051388734633993e-06, 'epoch': 0.28}\n",
            "{'loss': 1.4623, 'grad_norm': 1.4084105491638184, 'learning_rate': 9.019683559788413e-06, 'epoch': 0.28}\n",
            "{'loss': 1.3335, 'grad_norm': 1.9003851413726807, 'learning_rate': 8.987514686438353e-06, 'epoch': 0.29}\n",
            "{'loss': 1.3275, 'grad_norm': 1.5289127826690674, 'learning_rate': 8.954885825487485e-06, 'epoch': 0.29}\n",
            "{'loss': 1.3193, 'grad_norm': 1.8435882329940796, 'learning_rate': 8.921800740902246e-06, 'epoch': 0.29}\n",
            "{'loss': 1.3944, 'grad_norm': 1.644405484199524, 'learning_rate': 8.888263249277656e-06, 'epoch': 0.3}\n",
            "{'loss': 1.4767, 'grad_norm': 1.2626346349716187, 'learning_rate': 8.854277219397034e-06, 'epoch': 0.3}\n",
            "{'loss': 1.4497, 'grad_norm': 1.4008811712265015, 'learning_rate': 8.819846571785716e-06, 'epoch': 0.3}\n",
            "{'loss': 1.4073, 'grad_norm': 1.7734520435333252, 'learning_rate': 8.784975278258783e-06, 'epoch': 0.3}\n",
            "{'loss': 1.3593, 'grad_norm': 1.7544602155685425, 'learning_rate': 8.749667361462903e-06, 'epoch': 0.31}\n",
            " 31%|                         | 1000/3251 [31:30<1:10:13,  1.87s/it][INFO|trainer.py:3993] 2025-06-19 12:50:39,971 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-1000\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 12:50:39,976 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 12:50:39,978 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 12:50:44,182 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-1000/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 12:50:44,184 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-1000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 12:50:44,187 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 12:50:44,187 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.4108, 'grad_norm': 1.7498501539230347, 'learning_rate': 8.713926894412273e-06, 'epoch': 0.31}\n",
            "{'loss': 1.415, 'grad_norm': 1.826395034790039, 'learning_rate': 8.677758000018777e-06, 'epoch': 0.31}\n",
            "{'loss': 1.45, 'grad_norm': 1.8749536275863647, 'learning_rate': 8.641164850616379e-06, 'epoch': 0.32}\n",
            "{'loss': 1.3432, 'grad_norm': 1.7560936212539673, 'learning_rate': 8.604151667479814e-06, 'epoch': 0.32}\n",
            "{'loss': 1.445, 'grad_norm': 1.6857489347457886, 'learning_rate': 8.566722720337634e-06, 'epoch': 0.32}\n",
            "{'loss': 1.3317, 'grad_norm': 1.563329815864563, 'learning_rate': 8.528882326879669e-06, 'epoch': 0.33}\n",
            "{'loss': 1.3745, 'grad_norm': 2.1401021480560303, 'learning_rate': 8.490634852258938e-06, 'epoch': 0.33}\n",
            "{'loss': 1.3833, 'grad_norm': 1.4356790781021118, 'learning_rate': 8.451984708588122e-06, 'epoch': 0.33}\n",
            "{'loss': 1.3961, 'grad_norm': 1.5474573373794556, 'learning_rate': 8.412936354430575e-06, 'epoch': 0.34}\n",
            "{'loss': 1.3821, 'grad_norm': 1.7777544260025024, 'learning_rate': 8.373494294286003e-06, 'epoch': 0.34}\n",
            "{'loss': 1.4486, 'grad_norm': 1.6604543924331665, 'learning_rate': 8.333663078070845e-06, 'epoch': 0.34}\n",
            "{'loss': 1.3656, 'grad_norm': 1.700741171836853, 'learning_rate': 8.293447300593402e-06, 'epoch': 0.34}\n",
            "{'loss': 1.3548, 'grad_norm': 2.0677695274353027, 'learning_rate': 8.2528516010238e-06, 'epoch': 0.35}\n",
            "{'loss': 1.4188, 'grad_norm': 1.5665311813354492, 'learning_rate': 8.211880662358818e-06, 'epoch': 0.35}\n",
            "{'loss': 1.4656, 'grad_norm': 2.347297430038452, 'learning_rate': 8.170539210881686e-06, 'epoch': 0.35}\n",
            "{'loss': 1.3876, 'grad_norm': 1.5792713165283203, 'learning_rate': 8.128832015616862e-06, 'epoch': 0.36}\n",
            "{'loss': 1.369, 'grad_norm': 1.6267733573913574, 'learning_rate': 8.0867638877799e-06, 'epoch': 0.36}\n",
            "{'loss': 1.378, 'grad_norm': 1.7657021284103394, 'learning_rate': 8.044339680222448e-06, 'epoch': 0.36}\n",
            "{'loss': 1.4269, 'grad_norm': 1.6877226829528809, 'learning_rate': 8.001564286872424e-06, 'epoch': 0.37}\n",
            "{'loss': 1.3764, 'grad_norm': 1.5845848321914673, 'learning_rate': 7.958442642169469e-06, 'epoch': 0.37}\n",
            "{'loss': 1.321, 'grad_norm': 1.8538445234298706, 'learning_rate': 7.91497972049574e-06, 'epoch': 0.37}\n",
            "{'loss': 1.4578, 'grad_norm': 1.7848995923995972, 'learning_rate': 7.871180535602064e-06, 'epoch': 0.38}\n",
            "{'loss': 1.3848, 'grad_norm': 1.4076851606369019, 'learning_rate': 7.827050140029578e-06, 'epoch': 0.38}\n",
            "{'loss': 1.3783, 'grad_norm': 1.6770434379577637, 'learning_rate': 7.782593624526874e-06, 'epoch': 0.38}\n",
            "{'loss': 1.3996, 'grad_norm': 1.7696045637130737, 'learning_rate': 7.737816117462752e-06, 'epoch': 0.38}\n",
            "{'loss': 1.3353, 'grad_norm': 2.188258647918701, 'learning_rate': 7.692722784234624e-06, 'epoch': 0.39}\n",
            "{'loss': 1.3631, 'grad_norm': 1.7823063135147095, 'learning_rate': 7.647318826672653e-06, 'epoch': 0.39}\n",
            "{'loss': 1.4112, 'grad_norm': 1.8350634574890137, 'learning_rate': 7.60160948243968e-06, 'epoch': 0.39}\n",
            "{'loss': 1.3146, 'grad_norm': 2.168588399887085, 'learning_rate': 7.555600024427028e-06, 'epoch': 0.4}\n",
            "{'loss': 1.4247, 'grad_norm': 1.934554934501648, 'learning_rate': 7.509295760146234e-06, 'epoch': 0.4}\n",
            "{'loss': 1.3168, 'grad_norm': 1.846503496170044, 'learning_rate': 7.462702031116793e-06, 'epoch': 0.4}\n",
            "{'loss': 1.4409, 'grad_norm': 2.0117781162261963, 'learning_rate': 7.415824212249977e-06, 'epoch': 0.41}\n",
            "{'loss': 1.4706, 'grad_norm': 2.120562791824341, 'learning_rate': 7.368667711228796e-06, 'epoch': 0.41}\n",
            "{'loss': 1.4615, 'grad_norm': 1.613035798072815, 'learning_rate': 7.321237967884194e-06, 'epoch': 0.41}\n",
            "{'loss': 1.3806, 'grad_norm': 2.175032138824463, 'learning_rate': 7.273540453567512e-06, 'epoch': 0.42}\n",
            "{'loss': 1.3137, 'grad_norm': 1.9493680000305176, 'learning_rate': 7.22558067051935e-06, 'epoch': 0.42}\n",
            "{'loss': 1.4164, 'grad_norm': 1.5248697996139526, 'learning_rate': 7.177364151234822e-06, 'epoch': 0.42}\n",
            "{'loss': 1.3461, 'grad_norm': 1.8208898305892944, 'learning_rate': 7.128896457825364e-06, 'epoch': 0.42}\n",
            "{'loss': 1.3812, 'grad_norm': 2.097526788711548, 'learning_rate': 7.080183181377092e-06, 'epoch': 0.43}\n",
            "{'loss': 1.3455, 'grad_norm': 1.4147419929504395, 'learning_rate': 7.031229941305838e-06, 'epoch': 0.43}\n",
            "{'loss': 1.4141, 'grad_norm': 1.7449054718017578, 'learning_rate': 6.982042384708911e-06, 'epoch': 0.43}\n",
            "{'loss': 1.4198, 'grad_norm': 1.6649380922317505, 'learning_rate': 6.932626185713664e-06, 'epoch': 0.44}\n",
            "{'loss': 1.4422, 'grad_norm': 1.627325415611267, 'learning_rate': 6.882987044822942e-06, 'epoch': 0.44}\n",
            "{'loss': 1.4177, 'grad_norm': 1.3033195734024048, 'learning_rate': 6.833130688257489e-06, 'epoch': 0.44}\n",
            "{'loss': 1.4341, 'grad_norm': 1.6808857917785645, 'learning_rate': 6.783062867295397e-06, 'epoch': 0.45}\n",
            "{'loss': 1.4298, 'grad_norm': 1.8386844396591187, 'learning_rate': 6.732789357608644e-06, 'epoch': 0.45}\n",
            "{'loss': 1.4126, 'grad_norm': 1.9442981481552124, 'learning_rate': 6.6823159585968355e-06, 'epoch': 0.45}\n",
            "{'loss': 1.3944, 'grad_norm': 1.70588219165802, 'learning_rate': 6.631648492718207e-06, 'epoch': 0.46}\n",
            "{'loss': 1.4262, 'grad_norm': 1.5630933046340942, 'learning_rate': 6.580792804817954e-06, 'epoch': 0.46}\n",
            "{'loss': 1.4906, 'grad_norm': 1.8695162534713745, 'learning_rate': 6.529754761453999e-06, 'epoch': 0.46}\n",
            " 46%|                     | 1500/3251 [47:18<54:43,  1.88s/it][INFO|trainer.py:3993] 2025-06-19 13:06:28,467 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-1500\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 13:06:28,468 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 13:06:28,468 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 13:06:32,482 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-1500/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 13:06:32,483 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-1500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 13:06:32,484 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 13:06:32,484 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.4235, 'grad_norm': 1.7638208866119385, 'learning_rate': 6.4785402502202345e-06, 'epoch': 0.46}\n",
            "{'loss': 1.3871, 'grad_norm': 1.936367154121399, 'learning_rate': 6.427155179067351e-06, 'epoch': 0.47}\n",
            "{'loss': 1.3619, 'grad_norm': 1.4785962104797363, 'learning_rate': 6.375605475621318e-06, 'epoch': 0.47}\n",
            "{'loss': 1.3868, 'grad_norm': 1.7084400653839111, 'learning_rate': 6.32389708649958e-06, 'epoch': 0.47}\n",
            "{'loss': 1.4065, 'grad_norm': 1.3061679601669312, 'learning_rate': 6.272035976625089e-06, 'epoch': 0.48}\n",
            "{'loss': 1.3519, 'grad_norm': 2.1007907390594482, 'learning_rate': 6.220028128538188e-06, 'epoch': 0.48}\n",
            "{'loss': 1.3297, 'grad_norm': 1.4507414102554321, 'learning_rate': 6.167879541706508e-06, 'epoch': 0.48}\n",
            "{'loss': 1.3345, 'grad_norm': 1.791157603263855, 'learning_rate': 6.115596231832863e-06, 'epoch': 0.49}\n",
            "{'loss': 1.3859, 'grad_norm': 1.8119010925292969, 'learning_rate': 6.063184230161319e-06, 'epoch': 0.49}\n",
            "{'loss': 1.4053, 'grad_norm': 1.7475061416625977, 'learning_rate': 6.0106495827814296e-06, 'epoch': 0.49}\n",
            "{'loss': 1.3873, 'grad_norm': 1.653916835784912, 'learning_rate': 5.95799834993079e-06, 'epoch': 0.5}\n",
            "{'loss': 1.4245, 'grad_norm': 1.9006754159927368, 'learning_rate': 5.905236605295941e-06, 'epoch': 0.5}\n",
            "{'loss': 1.4119, 'grad_norm': 2.6180498600006104, 'learning_rate': 5.852370435311723e-06, 'epoch': 0.5}\n",
            "{'loss': 1.3964, 'grad_norm': 2.0757126808166504, 'learning_rate': 5.799405938459175e-06, 'epoch': 0.5}\n",
            "{'loss': 1.382, 'grad_norm': 1.8625751733779907, 'learning_rate': 5.746349224562021e-06, 'epoch': 0.51}\n",
            "{'loss': 1.4295, 'grad_norm': 1.579948902130127, 'learning_rate': 5.6932064140818655e-06, 'epoch': 0.51}\n",
            "{'loss': 1.3179, 'grad_norm': 1.9733411073684692, 'learning_rate': 5.639983637412151e-06, 'epoch': 0.51}\n",
            "{'loss': 1.3996, 'grad_norm': 1.8700569868087769, 'learning_rate': 5.586687034170981e-06, 'epoch': 0.52}\n",
            "{'loss': 1.4181, 'grad_norm': 1.7021383047103882, 'learning_rate': 5.533322752492867e-06, 'epoch': 0.52}\n",
            "{'loss': 1.3465, 'grad_norm': 1.86091148853302, 'learning_rate': 5.479896948319497e-06, 'epoch': 0.52}\n",
            "{'loss': 1.4429, 'grad_norm': 1.609135627746582, 'learning_rate': 5.426415784689608e-06, 'epoch': 0.53}\n",
            "{'loss': 1.3661, 'grad_norm': 1.590213418006897, 'learning_rate': 5.372885431028041e-06, 'epoch': 0.53}\n",
            "{'loss': 1.4148, 'grad_norm': 1.8340033292770386, 'learning_rate': 5.319312062434046e-06, 'epoch': 0.53}\n",
            "{'loss': 1.3726, 'grad_norm': 1.9402637481689453, 'learning_rate': 5.265701858968944e-06, 'epoch': 0.54}\n",
            "{'loss': 1.3632, 'grad_norm': 1.7568137645721436, 'learning_rate': 5.212061004943218e-06, 'epoch': 0.54}\n",
            "{'loss': 1.406, 'grad_norm': 1.6150096654891968, 'learning_rate': 5.158395688203105e-06, 'epoch': 0.54}\n",
            "{'loss': 1.4313, 'grad_norm': 1.7428228855133057, 'learning_rate': 5.1047120994167855e-06, 'epoch': 0.54}\n",
            "{'loss': 1.4096, 'grad_norm': 1.6148452758789062, 'learning_rate': 5.051016431360252e-06, 'epoch': 0.55}\n",
            "{'loss': 1.3847, 'grad_norm': 2.475104331970215, 'learning_rate': 4.997314878202917e-06, 'epoch': 0.55}\n",
            "{'loss': 1.3788, 'grad_norm': 1.417831540107727, 'learning_rate': 4.943613634793092e-06, 'epoch': 0.55}\n",
            "{'loss': 1.3882, 'grad_norm': 1.5008305311203003, 'learning_rate': 4.8899188959433465e-06, 'epoch': 0.56}\n",
            "{'loss': 1.4576, 'grad_norm': 1.7968528270721436, 'learning_rate': 4.8362368557159105e-06, 'epoch': 0.56}\n",
            "{'loss': 1.3568, 'grad_norm': 1.6415594816207886, 'learning_rate': 4.782573706708133e-06, 'epoch': 0.56}\n",
            "{'loss': 1.3618, 'grad_norm': 1.64385187625885, 'learning_rate': 4.728935639338136e-06, 'epoch': 0.57}\n",
            "{'loss': 1.3834, 'grad_norm': 1.8637425899505615, 'learning_rate': 4.675328841130695e-06, 'epoch': 0.57}\n",
            "{'loss': 1.4458, 'grad_norm': 1.7505671977996826, 'learning_rate': 4.621759496003472e-06, 'epoch': 0.57}\n",
            "{'loss': 1.341, 'grad_norm': 1.8147209882736206, 'learning_rate': 4.568233783553653e-06, 'epoch': 0.58}\n",
            "{'loss': 1.442, 'grad_norm': 2.0813283920288086, 'learning_rate': 4.514757878345093e-06, 'epoch': 0.58}\n",
            "{'loss': 1.3692, 'grad_norm': 1.9719730615615845, 'learning_rate': 4.461337949196036e-06, 'epoch': 0.58}\n",
            "{'loss': 1.3993, 'grad_norm': 1.6842180490493774, 'learning_rate': 4.4079801584674955e-06, 'epoch': 0.58}\n",
            "{'loss': 1.4264, 'grad_norm': 1.6981372833251953, 'learning_rate': 4.354690661352384e-06, 'epoch': 0.59}\n",
            "{'loss': 1.3596, 'grad_norm': 1.6803631782531738, 'learning_rate': 4.301475605165471e-06, 'epoch': 0.59}\n",
            "{'loss': 1.3886, 'grad_norm': 2.023716449737549, 'learning_rate': 4.248341128634247e-06, 'epoch': 0.59}\n",
            "{'loss': 1.4154, 'grad_norm': 1.7116389274597168, 'learning_rate': 4.195293361190777e-06, 'epoch': 0.6}\n",
            "{'loss': 1.4042, 'grad_norm': 1.981019139289856, 'learning_rate': 4.142338422264629e-06, 'epoch': 0.6}\n",
            "{'loss': 1.2694, 'grad_norm': 1.7961031198501587, 'learning_rate': 4.089482420576955e-06, 'epoch': 0.6}\n",
            "{'loss': 1.3554, 'grad_norm': 1.9459381103515625, 'learning_rate': 4.036731453435805e-06, 'epoch': 0.61}\n",
            "{'loss': 1.4627, 'grad_norm': 2.4830310344696045, 'learning_rate': 3.984091606032768e-06, 'epoch': 0.61}\n",
            "{'loss': 1.4589, 'grad_norm': 1.5707837343215942, 'learning_rate': 3.931568950740991e-06, 'epoch': 0.61}\n",
            "{'loss': 1.4251, 'grad_norm': 1.4237366914749146, 'learning_rate': 3.8791695464146975e-06, 'epoch': 0.62}\n",
            " 62%|              | 2000/3251 [1:03:05<39:05,  1.87s/it][INFO|trainer.py:3993] 2025-06-19 13:22:15,631 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-2000\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 13:22:15,632 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 13:22:15,632 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 13:22:19,528 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-2000/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 13:22:19,528 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-2000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 13:22:19,529 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 13:22:19,529 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.3664, 'grad_norm': 1.7126110792160034, 'learning_rate': 3.82689943769025e-06, 'epoch': 0.62}\n",
            "{'loss': 1.3981, 'grad_norm': 1.9220479726791382, 'learning_rate': 3.7747646542888685e-06, 'epoch': 0.62}\n",
            "{'loss': 1.33, 'grad_norm': 1.9410724639892578, 'learning_rate': 3.7227712103210485e-06, 'epoch': 0.62}\n",
            "{'loss': 1.4353, 'grad_norm': 1.9398798942565918, 'learning_rate': 3.6709251035928002e-06, 'epoch': 0.63}\n",
            "{'loss': 1.3224, 'grad_norm': 1.5501084327697754, 'learning_rate': 3.6192323149137556e-06, 'epoch': 0.63}\n",
            "{'loss': 1.3836, 'grad_norm': 1.8590247631072998, 'learning_rate': 3.5676988074072405e-06, 'epoch': 0.63}\n",
            "{'loss': 1.356, 'grad_norm': 1.877701997756958, 'learning_rate': 3.5163305258223913e-06, 'epoch': 0.64}\n",
            "{'loss': 1.3329, 'grad_norm': 2.058706283569336, 'learning_rate': 3.4651333958483804e-06, 'epoch': 0.64}\n",
            "{'loss': 1.4029, 'grad_norm': 2.0486042499542236, 'learning_rate': 3.4141133234308486e-06, 'epoch': 0.64}\n",
            "{'loss': 1.36, 'grad_norm': 1.9313846826553345, 'learning_rate': 3.3632761940906167e-06, 'epoch': 0.65}\n",
            "{'loss': 1.3332, 'grad_norm': 1.7312344312667847, 'learning_rate': 3.312627872244745e-06, 'epoch': 0.65}\n",
            "{'loss': 1.4566, 'grad_norm': 1.8840608596801758, 'learning_rate': 3.2621742005300333e-06, 'epoch': 0.65}\n",
            "{'loss': 1.4384, 'grad_norm': 1.9687373638153076, 'learning_rate': 3.2119209991290346e-06, 'epoch': 0.66}\n",
            "{'loss': 1.4422, 'grad_norm': 1.3374855518341064, 'learning_rate': 3.161874065098654e-06, 'epoch': 0.66}\n",
            "{'loss': 1.4064, 'grad_norm': 1.6537058353424072, 'learning_rate': 3.1120391717014165e-06, 'epoch': 0.66}\n",
            "{'loss': 1.3318, 'grad_norm': 1.7769643068313599, 'learning_rate': 3.0624220677394854e-06, 'epoch': 0.66}\n",
            "{'loss': 1.3975, 'grad_norm': 1.6621564626693726, 'learning_rate': 3.0130284768914934e-06, 'epoch': 0.67}\n",
            "{'loss': 1.3835, 'grad_norm': 1.8803027868270874, 'learning_rate': 2.963864097052279e-06, 'epoch': 0.67}\n",
            "{'loss': 1.4392, 'grad_norm': 1.7693639993667603, 'learning_rate': 2.914934599675594e-06, 'epoch': 0.67}\n",
            "{'loss': 1.3321, 'grad_norm': 1.8410146236419678, 'learning_rate': 2.8662456291198615e-06, 'epoch': 0.68}\n",
            "{'loss': 1.3722, 'grad_norm': 2.384826183319092, 'learning_rate': 2.817802801997059e-06, 'epoch': 0.68}\n",
            "{'loss': 1.4287, 'grad_norm': 2.347892999649048, 'learning_rate': 2.769611706524805e-06, 'epoch': 0.68}\n",
            "{'loss': 1.3843, 'grad_norm': 1.9703872203826904, 'learning_rate': 2.7216779018817193e-06, 'epoch': 0.69}\n",
            "{'loss': 1.3737, 'grad_norm': 1.8144779205322266, 'learning_rate': 2.67400691756613e-06, 'epoch': 0.69}\n",
            "{'loss': 1.4626, 'grad_norm': 1.6386412382125854, 'learning_rate': 2.6266042527582103e-06, 'epoch': 0.69}\n",
            "{'loss': 1.4377, 'grad_norm': 1.6607774496078491, 'learning_rate': 2.5794753756856085e-06, 'epoch': 0.7}\n",
            "{'loss': 1.4153, 'grad_norm': 1.6950820684432983, 'learning_rate': 2.532625722992651e-06, 'epoch': 0.7}\n",
            "{'loss': 1.3753, 'grad_norm': 1.7399295568466187, 'learning_rate': 2.4860606991131857e-06, 'epoch': 0.7}\n",
            "{'loss': 1.3777, 'grad_norm': 1.4393662214279175, 'learning_rate': 2.4397856756471435e-06, 'epoch': 0.7}\n",
            "{'loss': 1.3334, 'grad_norm': 1.8434476852416992, 'learning_rate': 2.3938059907408876e-06, 'epoch': 0.71}\n",
            "{'loss': 1.3388, 'grad_norm': 1.8587676286697388, 'learning_rate': 2.3481269484714208e-06, 'epoch': 0.71}\n",
            "{'loss': 1.3848, 'grad_norm': 1.4031702280044556, 'learning_rate': 2.3027538182345237e-06, 'epoch': 0.71}\n",
            "{'loss': 1.3324, 'grad_norm': 1.6170867681503296, 'learning_rate': 2.257691834136889e-06, 'epoch': 0.72}\n",
            "{'loss': 1.4185, 'grad_norm': 1.8258475065231323, 'learning_rate': 2.2129461943923407e-06, 'epoch': 0.72}\n",
            "{'loss': 1.3671, 'grad_norm': 1.6360119581222534, 'learning_rate': 2.1685220607221736e-06, 'epoch': 0.72}\n",
            "{'loss': 1.4182, 'grad_norm': 1.4681671857833862, 'learning_rate': 2.1244245577597174e-06, 'epoch': 0.73}\n",
            "{'loss': 1.3619, 'grad_norm': 1.795547604560852, 'learning_rate': 2.0806587724591725e-06, 'epoch': 0.73}\n",
            "{'loss': 1.3779, 'grad_norm': 1.7101846933364868, 'learning_rate': 2.0372297535087987e-06, 'epoch': 0.73}\n",
            "{'loss': 1.3191, 'grad_norm': 1.1781045198440552, 'learning_rate': 1.99414251074851e-06, 'epoch': 0.74}\n",
            "{'loss': 1.3448, 'grad_norm': 2.1019012928009033, 'learning_rate': 1.9514020145919536e-06, 'epoch': 0.74}\n",
            "{'loss': 1.3509, 'grad_norm': 1.74428129196167, 'learning_rate': 1.9090131954531456e-06, 'epoch': 0.74}\n",
            "{'loss': 1.2644, 'grad_norm': 1.7526655197143555, 'learning_rate': 1.8669809431776991e-06, 'epoch': 0.74}\n",
            "{'loss': 1.389, 'grad_norm': 1.561090350151062, 'learning_rate': 1.825310106478762e-06, 'epoch': 0.75}\n",
            "{'loss': 1.4007, 'grad_norm': 1.8476390838623047, 'learning_rate': 1.7840054923776774e-06, 'epoch': 0.75}\n",
            "{'loss': 1.3751, 'grad_norm': 1.810234785079956, 'learning_rate': 1.7430718656494583e-06, 'epoch': 0.75}\n",
            "{'loss': 1.324, 'grad_norm': 1.7119070291519165, 'learning_rate': 1.7025139482731385e-06, 'epoch': 0.76}\n",
            "{'loss': 1.4546, 'grad_norm': 1.6860429048538208, 'learning_rate': 1.6623364188870622e-06, 'epoch': 0.76}\n",
            "{'loss': 1.3165, 'grad_norm': 2.2773847579956055, 'learning_rate': 1.6225439122491683e-06, 'epoch': 0.76}\n",
            "{'loss': 1.3646, 'grad_norm': 1.6327574253082275, 'learning_rate': 1.5831410187023388e-06, 'epoch': 0.77}\n",
            "{'loss': 1.3865, 'grad_norm': 1.6478521823883057, 'learning_rate': 1.5441322836448712e-06, 'epoch': 0.77}\n",
            " 77%|        | 2500/3251 [1:18:52<23:31,  1.88s/it][INFO|trainer.py:3993] 2025-06-19 13:38:02,481 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-2500\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 13:38:02,481 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 13:38:02,482 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 13:38:06,407 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-2500/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 13:38:06,407 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-2500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 13:38:06,408 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 13:38:06,408 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.423, 'grad_norm': 1.617278814315796, 'learning_rate': 1.5055222070061353e-06, 'epoch': 0.77}\n",
            "{'loss': 1.3986, 'grad_norm': 2.08894944190979, 'learning_rate': 1.467315242727474e-06, 'epoch': 0.78}\n",
            "{'loss': 1.34, 'grad_norm': 1.8528721332550049, 'learning_rate': 1.4295157982484131e-06, 'epoch': 0.78}\n",
            "{'loss': 1.3809, 'grad_norm': 1.8781933784484863, 'learning_rate': 1.3921282339982283e-06, 'epoch': 0.78}\n",
            "{'loss': 1.3348, 'grad_norm': 1.6492068767547607, 'learning_rate': 1.3551568628929434e-06, 'epoch': 0.78}\n",
            "{'loss': 1.3741, 'grad_norm': 1.7782962322235107, 'learning_rate': 1.3186059498377978e-06, 'epoch': 0.79}\n",
            "{'loss': 1.3636, 'grad_norm': 1.9359198808670044, 'learning_rate': 1.2824797112352717e-06, 'epoch': 0.79}\n",
            "{'loss': 1.3706, 'grad_norm': 1.7959542274475098, 'learning_rate': 1.2467823144986846e-06, 'epoch': 0.79}\n",
            "{'loss': 1.4703, 'grad_norm': 2.0707485675811768, 'learning_rate': 1.2115178775714608e-06, 'epoch': 0.8}\n",
            "{'loss': 1.3108, 'grad_norm': 1.9550713300704956, 'learning_rate': 1.1766904684520941e-06, 'epoch': 0.8}\n",
            "{'loss': 1.3713, 'grad_norm': 1.7504757642745972, 'learning_rate': 1.142304104724873e-06, 'epoch': 0.8}\n",
            "{'loss': 1.4214, 'grad_norm': 1.6616389751434326, 'learning_rate': 1.1083627530964325e-06, 'epoch': 0.81}\n",
            "{'loss': 1.327, 'grad_norm': 1.690402626991272, 'learning_rate': 1.0748703289381574e-06, 'epoch': 0.81}\n",
            "{'loss': 1.4386, 'grad_norm': 1.6621170043945312, 'learning_rate': 1.0418306958345214e-06, 'epoch': 0.81}\n",
            "{'loss': 1.2915, 'grad_norm': 1.8907595872879028, 'learning_rate': 1.0092476651373966e-06, 'epoch': 0.82}\n",
            "{'loss': 1.3712, 'grad_norm': 1.7873717546463013, 'learning_rate': 9.77124995526385e-07, 'epoch': 0.82}\n",
            "{'loss': 1.4122, 'grad_norm': 1.4856430292129517, 'learning_rate': 9.454663925752317e-07, 'epoch': 0.82}\n",
            "{'loss': 1.291, 'grad_norm': 1.856831669807434, 'learning_rate': 9.142755083243577e-07, 'epoch': 0.82}\n",
            "{'loss': 1.4105, 'grad_norm': 1.77671480178833, 'learning_rate': 8.83555940859574e-07, 'epoch': 0.83}\n",
            "{'loss': 1.3628, 'grad_norm': 1.5837700366973877, 'learning_rate': 8.533112338970157e-07, 'epoch': 0.83}\n",
            "{'loss': 1.3784, 'grad_norm': 1.4943342208862305, 'learning_rate': 8.23544876374352e-07, 'epoch': 0.83}\n",
            "{'loss': 1.3855, 'grad_norm': 1.778272271156311, 'learning_rate': 7.942603020483108e-07, 'epoch': 0.84}\n",
            "{'loss': 1.3912, 'grad_norm': 1.5445842742919922, 'learning_rate': 7.654608890985709e-07, 'epoch': 0.84}\n",
            "{'loss': 1.3815, 'grad_norm': 2.265162706375122, 'learning_rate': 7.371499597380671e-07, 'epoch': 0.84}\n",
            "{'loss': 1.3735, 'grad_norm': 1.3799372911453247, 'learning_rate': 7.093307798297483e-07, 'epoch': 0.85}\n",
            "{'loss': 1.2988, 'grad_norm': 2.9888968467712402, 'learning_rate': 6.820065585098379e-07, 'epoch': 0.85}\n",
            "{'loss': 1.3852, 'grad_norm': 1.4751216173171997, 'learning_rate': 6.551804478176383e-07, 'epoch': 0.85}\n",
            "{'loss': 1.4352, 'grad_norm': 1.4602136611938477, 'learning_rate': 6.288555423319204e-07, 'epoch': 0.86}\n",
            "{'loss': 1.4127, 'grad_norm': 1.527107834815979, 'learning_rate': 6.030348788139406e-07, 'epoch': 0.86}\n",
            "{'loss': 1.4454, 'grad_norm': 2.2337000370025635, 'learning_rate': 5.777214358571353e-07, 'epoch': 0.86}\n",
            "{'loss': 1.3539, 'grad_norm': 1.7598868608474731, 'learning_rate': 5.529181335435124e-07, 'epoch': 0.86}\n",
            "{'loss': 1.4114, 'grad_norm': 2.1397862434387207, 'learning_rate': 5.286278331068018e-07, 'epoch': 0.87}\n",
            "{'loss': 1.4186, 'grad_norm': 1.8661941289901733, 'learning_rate': 5.048533366023939e-07, 'epoch': 0.87}\n",
            "{'loss': 1.3948, 'grad_norm': 1.9002039432525635, 'learning_rate': 4.815973865841028e-07, 'epoch': 0.87}\n",
            "{'loss': 1.4736, 'grad_norm': 1.3656959533691406, 'learning_rate': 4.588626657877898e-07, 'epoch': 0.88}\n",
            "{'loss': 1.3474, 'grad_norm': 1.769657850265503, 'learning_rate': 4.366517968218981e-07, 'epoch': 0.88}\n",
            "{'loss': 1.3589, 'grad_norm': 1.7248674631118774, 'learning_rate': 4.1496734186490694e-07, 'epoch': 0.88}\n",
            "{'loss': 1.3709, 'grad_norm': 2.625051498413086, 'learning_rate': 3.9381180236977623e-07, 'epoch': 0.89}\n",
            "{'loss': 1.3571, 'grad_norm': 1.7688307762145996, 'learning_rate': 3.7318761877538244e-07, 'epoch': 0.89}\n",
            "{'loss': 1.3776, 'grad_norm': 1.86923348903656, 'learning_rate': 3.5309717022499414e-07, 'epoch': 0.89}\n",
            "{'loss': 1.336, 'grad_norm': 2.7671499252319336, 'learning_rate': 3.3354277429182626e-07, 'epoch': 0.9}\n",
            "{'loss': 1.4324, 'grad_norm': 1.9307332038879395, 'learning_rate': 3.145266867116881e-07, 'epoch': 0.9}\n",
            "{'loss': 1.3488, 'grad_norm': 1.9089264869689941, 'learning_rate': 2.9605110112277126e-07, 'epoch': 0.9}\n",
            "{'loss': 1.4343, 'grad_norm': 2.0394561290740967, 'learning_rate': 2.7811814881259503e-07, 'epoch': 0.9}\n",
            "{'loss': 1.4171, 'grad_norm': 1.615846872329712, 'learning_rate': 2.6072989847215123e-07, 'epoch': 0.91}\n",
            "{'loss': 1.4062, 'grad_norm': 1.8691933155059814, 'learning_rate': 2.4388835595726355e-07, 'epoch': 0.91}\n",
            "{'loss': 1.3902, 'grad_norm': 1.6013176441192627, 'learning_rate': 2.2759546405719813e-07, 'epoch': 0.91}\n",
            "{'loss': 1.3459, 'grad_norm': 2.048679828643799, 'learning_rate': 2.1185310227055335e-07, 'epoch': 0.92}\n",
            "{'loss': 1.3591, 'grad_norm': 1.673806071281433, 'learning_rate': 1.9666308658844002e-07, 'epoch': 0.92}\n",
            "{'loss': 1.3296, 'grad_norm': 2.3706135749816895, 'learning_rate': 1.8202716928499842e-07, 'epoch': 0.92}\n",
            " 92%|  | 3000/3251 [1:34:39<07:52,  1.88s/it][INFO|trainer.py:3993] 2025-06-19 13:53:49,428 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-3000\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 13:53:49,428 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 13:53:49,429 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 13:53:53,391 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-3000/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 13:53:53,392 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-3000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 13:53:53,393 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 13:53:53,393 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 1.3137, 'grad_norm': 2.363891363143921, 'learning_rate': 1.6794703871526042e-07, 'epoch': 0.93}\n",
            "{'loss': 1.3026, 'grad_norm': 1.9394612312316895, 'learning_rate': 1.544243191203859e-07, 'epoch': 0.93}\n",
            "{'loss': 1.3215, 'grad_norm': 1.8024474382400513, 'learning_rate': 1.414605704402966e-07, 'epoch': 0.93}\n",
            "{'loss': 1.4311, 'grad_norm': 1.4872071743011475, 'learning_rate': 1.290572881337221e-07, 'epoch': 0.94}\n",
            "{'loss': 1.3986, 'grad_norm': 1.5986506938934326, 'learning_rate': 1.1721590300569296e-07, 'epoch': 0.94}\n",
            "{'loss': 1.3721, 'grad_norm': 1.5549079179763794, 'learning_rate': 1.0593778104248442e-07, 'epoch': 0.94}\n",
            "{'loss': 1.3304, 'grad_norm': 1.9179457426071167, 'learning_rate': 9.522422325404234e-08, 'epoch': 0.94}\n",
            "{'loss': 1.4375, 'grad_norm': 1.8749114274978638, 'learning_rate': 8.507646552390004e-08, 'epoch': 0.95}\n",
            "{'loss': 1.3549, 'grad_norm': 1.5815984010696411, 'learning_rate': 7.549567846661388e-08, 'epoch': 0.95}\n",
            "{'loss': 1.4132, 'grad_norm': 1.8495337963104248, 'learning_rate': 6.648296729272142e-08, 'epoch': 0.95}\n",
            "{'loss': 1.3226, 'grad_norm': 1.7406744956970215, 'learning_rate': 5.8039371681249955e-08, 'epoch': 0.96}\n",
            "{'loss': 1.3896, 'grad_norm': 1.4466358423233032, 'learning_rate': 5.0165865659780876e-08, 'epoch': 0.96}\n",
            "{'loss': 1.324, 'grad_norm': 2.07432222366333, 'learning_rate': 4.286335749209003e-08, 'epoch': 0.96}\n",
            "{'loss': 1.4704, 'grad_norm': 2.058565855026245, 'learning_rate': 3.613268957337324e-08, 'epoch': 0.97}\n",
            "{'loss': 1.3945, 'grad_norm': 1.6334311962127686, 'learning_rate': 2.997463833306735e-08, 'epoch': 0.97}\n",
            "{'loss': 1.4089, 'grad_norm': 1.9587987661361694, 'learning_rate': 2.4389914145288575e-08, 'epoch': 0.97}\n",
            "{'loss': 1.4233, 'grad_norm': 1.7320564985275269, 'learning_rate': 1.937916124688022e-08, 'epoch': 0.98}\n",
            "{'loss': 1.402, 'grad_norm': 1.3163948059082031, 'learning_rate': 1.494295766310161e-08, 'epoch': 0.98}\n",
            "{'loss': 1.3816, 'grad_norm': 2.0800702571868896, 'learning_rate': 1.108181514094253e-08, 'epoch': 0.98}\n",
            "{'loss': 1.3408, 'grad_norm': 1.9902880191802979, 'learning_rate': 7.796179090094891e-09, 'epoch': 0.98}\n",
            "{'loss': 1.4232, 'grad_norm': 1.472288966178894, 'learning_rate': 5.086428531568821e-09, 'epoch': 0.99}\n",
            "{'loss': 1.3277, 'grad_norm': 2.08453106880188, 'learning_rate': 2.952876053970433e-09, 'epoch': 0.99}\n",
            "{'loss': 1.4058, 'grad_norm': 1.5494060516357422, 'learning_rate': 1.3957677774428802e-09, 'epoch': 0.99}\n",
            "{'loss': 1.4056, 'grad_norm': 1.7394042015075684, 'learning_rate': 4.15283325274074e-10, 'epoch': 1.0}\n",
            "{'loss': 1.4634, 'grad_norm': 1.323145866394043, 'learning_rate': 1.1535803177142868e-11, 'epoch': 1.0}\n",
            "100%|| 3251/3251 [1:42:37<00:00,  1.83s/it][INFO|trainer.py:3993] 2025-06-19 14:01:47,549 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning/checkpoint-3251\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 14:01:47,550 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-3251/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 14:01:47,550 >> Configuration saved in llama3_3b_freeze_instructionTuning/checkpoint-3251/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 14:01:51,693 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/checkpoint-3251/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 14:01:51,693 >> chat template saved in llama3_3b_freeze_instructionTuning/checkpoint-3251/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 14:01:51,694 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/checkpoint-3251/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 14:01:51,694 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/checkpoint-3251/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-06-19 14:01:53,529 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6167.7954, 'train_samples_per_second': 8.431, 'train_steps_per_second': 0.527, 'train_loss': 1.4094693632868025, 'epoch': 1.0}\n",
            "100%|| 3251/3251 [1:42:43<00:00,  1.90s/it]\n",
            "[INFO|trainer.py:3993] 2025-06-19 14:01:53,573 >> Saving model checkpoint to llama3_3b_freeze_instructionTuning\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 14:01:53,574 >> Configuration saved in llama3_3b_freeze_instructionTuning/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 14:01:53,574 >> Configuration saved in llama3_3b_freeze_instructionTuning/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 14:01:57,561 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 14:01:57,562 >> chat template saved in llama3_3b_freeze_instructionTuning/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 14:01:57,563 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 14:01:57,563 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =         1.0\n",
            "  total_flos               = 102053866GF\n",
            "  train_loss               =      1.4095\n",
            "  train_runtime            =  1:42:47.79\n",
            "  train_samples_per_second =       8.431\n",
            "  train_steps_per_second   =       0.527\n",
            "[INFO|modelcard.py:450] 2025-06-19 14:01:57,659 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli train InstructionTuning_LayerFreezing_Alpaca.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDk8snDQsUJP"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VBRKm_IsVzk"
      },
      "outputs": [],
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "import torch\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "    # Load BASE model (original Llama-3-8B-Instruct)\n",
        "    model_name_or_path=\"meta-llama/LLaMA-3.2-3B-Instruct\",\n",
        "\n",
        "    # Point to YOUR instruction-tuned checkpoint\n",
        "    adapter_name_or_path=\"llama3_3b_freeze_instructionTuning\",\n",
        "\n",
        "    # MUST match training configuration\n",
        "    finetuning_type=\"freeze\",  # Not \"lora\"!\n",
        "    template=\"llama3\",\n",
        "\n",
        "    # Add quantization for efficiency\n",
        "    quantization_bit=8,          # 8-bit quantization\n",
        "    flash_attn=True,             # Faster attention\n",
        "    torch_dtype=torch.bfloat16,  # Precision\n",
        "    device_map=\"auto\"            # Automatic device placement\n",
        ")\n",
        "\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application! Commands: 'clear' to reset history, 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"\\nUser: \")\n",
        "        if query.strip().lower() == \"exit\":\n",
        "            break\n",
        "        if query.strip().lower() == \"clear\":\n",
        "            messages = []\n",
        "            torch_gc()\n",
        "            print(\"History cleared.\")\n",
        "            continue\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": query})\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "        response = \"\"\n",
        "        for new_text in chat_model.stream_chat(messages):\n",
        "            print(new_text, end=\"\", flush=True)\n",
        "            response += new_text\n",
        "\n",
        "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "        print()  # New line after response\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExiting...\")\n",
        "        break\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGE55Vjzt9Nf"
      },
      "source": [
        "## Merge the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bCWVfIjt9qm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    # Load BASE model (original model architecture)\n",
        "    model_name_or_path=\"llama3_3b_freeze_instructionTuning\",\n",
        "\n",
        "    # Point to YOUR instruction-tuned checkpoint\n",
        "    # adapter_name_or_path=\"llama3_3b_freeze_instructionTuning\",\n",
        "\n",
        "    # MUST match training configuration\n",
        "    template=\"llama3\",\n",
        "    finetuning_type=\"freeze\",  # Changed from \"lora\" to \"freeze\"\n",
        "\n",
        "    # Export settings\n",
        "    export_dir=\"llama3_freeze_merged_fintuned\",  # Path to save merged model\n",
        "    export_size=2,                     # File shard size in GB\n",
        "    export_device=\"cpu\",               # Use CPU for safer export\n",
        "    export_hub_model_id=\"wbasharat/llama3_3b_freeze_instructionTuning\",  # Uncomment to upload to HF Hub\n",
        ")\n",
        "\n",
        "# Save config to JSON\n",
        "with open(\"export_freeze_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(args, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saT8BET6Xb3d",
        "outputId": "8e7658b3-890b-43d0-d1df-7e00c38fd562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/LLaMA-Factory/'\n",
            "/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,716 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,716 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,716 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,716 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,716 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,716 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 14:02:20,951 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-19 14:02:20,951 >> loading configuration file llama3_3b_freeze_instructionTuning/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 14:02:20,953 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,954 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,954 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,954 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,954 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,954 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-19 14:02:20,954 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-19 14:02:21,193 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-06-19 14:02:21] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|configuration_utils.py:696] 2025-06-19 14:02:21,210 >> loading configuration file llama3_3b_freeze_instructionTuning/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-19 14:02:21,211 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 14:02:21] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-19 14:02:21,342 >> loading weights file llama3_3b_freeze_instructionTuning/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-19 14:02:21,342 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 14:02:21,343 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 2/2 [00:00<00:00, 23.44it/s]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-19 14:02:21,442 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-19 14:02:21,442 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at llama3_3b_freeze_instructionTuning.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-19 14:02:21,443 >> loading configuration file llama3_3b_freeze_instructionTuning/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-19 14:02:21,443 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-19 14:02:21] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-19 14:02:21] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
            "[INFO|2025-06-19 14:02:21] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 14:02:21,449 >> Configuration saved in llama3_freeze_merged_fintuned/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 14:02:21,449 >> Configuration saved in llama3_freeze_merged_fintuned/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 14:02:23,883 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_freeze_merged_fintuned/model.safetensors.index.json.\n",
            "[INFO|configuration_utils.py:424] 2025-06-19 14:02:25,433 >> Configuration saved in llama3_3b_freeze_instructionTuning/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-06-19 14:02:25,434 >> Configuration saved in llama3_3b_freeze_instructionTuning/generation_config.json\n",
            "[INFO|modeling_utils.py:3733] 2025-06-19 14:02:27,785 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at llama3_3b_freeze_instructionTuning/model.safetensors.index.json.\n",
            "[INFO|hub.py:827] 2025-06-19 14:02:30,371 >> Uploading the following files to wbasharat/llama3_3b_freeze_instructionTuning: model.safetensors.index.json,model-00002-of-00004.safetensors,generation_config.json,model-00001-of-00004.safetensors,model-00004-of-00004.safetensors,README.md,model-00003-of-00004.safetensors,config.json\n",
            "Uploading...: 100%|| 6.43G/6.43G [05:22<00:00, 19.9MB/s]\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 14:07:55,671 >> chat template saved in llama3_freeze_merged_fintuned/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 14:07:55,673 >> tokenizer config file saved in llama3_freeze_merged_fintuned/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 14:07:55,673 >> Special tokens file saved in llama3_freeze_merged_fintuned/special_tokens_map.json\n",
            "README.md: 100%|| 5.19k/5.19k [00:00<00:00, 45.1MB/s]\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-19 14:07:56,555 >> chat template saved in llama3_3b_freeze_instructionTuning/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-19 14:07:56,556 >> tokenizer config file saved in llama3_3b_freeze_instructionTuning/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-19 14:07:56,557 >> Special tokens file saved in llama3_3b_freeze_instructionTuning/special_tokens_map.json\n",
            "[INFO|hub.py:827] 2025-06-19 14:07:56,683 >> Uploading the following files to wbasharat/llama3_3b_freeze_instructionTuning: special_tokens_map.json,tokenizer_config.json,chat_template.jinja,tokenizer.json,README.md\n",
            "Uploading...: 100%|| 17.2M/17.2M [00:02<00:00, 7.16MB/s]\n",
            "[INFO|2025-06-19 14:08:01] llamafactory.train.tuner:143 >> Ollama modelfile saved in llama3_freeze_merged_fintuned/Modelfile\n"
          ]
        }
      ],
      "source": [
        "# Run export command\n",
        "%cd /content/LLaMA-Factory/\n",
        "!llamafactory-cli export export_freeze_model.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-enN0DTGXoyM"
      },
      "source": [
        "## Merge Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JURrxbhXtq_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"llama3_freeze_merged\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"llama3_freeze_merged\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf4tJc8VX0BI"
      },
      "outputs": [],
      "source": [
        "export_hub_model_id=\"CPT_IT_frozen_layers_llama_3b/llama3-3b-freeze-instruction-tuned\",\n",
        "export_hub_token=\"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\",  # Add this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1z7u9XAmyf4"
      },
      "source": [
        "# Instruction Tuning Using Lora Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQiDf1NLmyf5",
        "outputId": "ebc08135-1811-4166-9963-8262c9b2649d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set the environment variable to use GPU 1\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "# llama3_3b_freeze_instructionTuning\n",
        "# %cd LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BU8vwjHmyf5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    model_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged\",\n",
        "    trust_remote_code=True,\n",
        "    template=\"llama3\", # Added from your 'dataset' section where template was specified\n",
        "\n",
        "    # Method settings\n",
        "    stage=\"sft\",\n",
        "    do_train=True,\n",
        "    finetuning_type=\"lora\",\n",
        "    lora_rank=64,\n",
        "    lora_target=\"all\",\n",
        "\n",
        "    # Dataset settings\n",
        "    dataset=\"alpaca_en\",\n",
        "    cutoff_len=2048,\n",
        "    # max_samples=1000,\n",
        "    overwrite_cache=True,\n",
        "    preprocessing_num_workers=16,\n",
        "    dataloader_num_workers=4,\n",
        "\n",
        "    # Output and checkpointing\n",
        "    output_dir=\"InstructionTuning_LoRA_Alpaca\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    plot_loss=True,\n",
        "    overwrite_output_dir=True,\n",
        "    save_only_model=False,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Train settings\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=5.0e-5,\n",
        "    # embedding_learning_rate = 1e-5,\n",
        "    num_train_epochs=1.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    bf16=True,\n",
        "    ddp_timeout=180000000,\n",
        "    resume_from_checkpoint=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4YOv7aDmyf5"
      },
      "outputs": [],
      "source": [
        "json.dump(args, open(\"InstructionTuning_LoRA_Alpaca.json\", \"w\", encoding=\"utf-8\"), indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4No9ADemyf5",
        "outputId": "c496b031-c307-4096-ace7-88670be7098d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|2025-06-20 09:55:39] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,671 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,671 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,671 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,671 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,671 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,671 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 09:55:39,896 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 09:55:39,897 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 09:55:39,898 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,898 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,898 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,898 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,898 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,898 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 09:55:39,898 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 09:55:40,136 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-06-20 09:55:40] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-06-20 09:55:40] llamafactory.data.loader:143 >> Loading dataset alpaca_instruction_tuning.json...\n",
            "Converting format of dataset (num_proc=16): 100%|| 52002/52002 [00:00<00:00, 24\n",
            "Running tokenizer on dataset (num_proc=16): 100%|| 52002/52002 [00:02<00:00, 17\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 40633, 311, 279, 1217, 596, 1715, 13, 128009, 128006, 882, 128007, 271, 36227, 2380, 10631, 369, 19994, 9498, 627, 36227, 2380, 10631, 369, 19994, 9498, 13, 128009, 128006, 78191, 128007, 271, 16, 5253, 266, 264, 24770, 10173, 323, 1304, 2771, 311, 2997, 11510, 315, 26390, 323, 24822, 13, 720, 17, 13, 33918, 15870, 311, 2567, 701, 2547, 4642, 323, 3831, 13, 720, 18, 13, 2175, 3403, 6212, 323, 10519, 264, 13263, 6212, 9899, 13, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant. Respond to the user's request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Give three tips for staying healthy.\n",
            "Give three tips for staying healthy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 5253, 266, 264, 24770, 10173, 323, 1304, 2771, 311, 2997, 11510, 315, 26390, 323, 24822, 13, 720, 17, 13, 33918, 15870, 311, 2567, 701, 2547, 4642, 323, 3831, 13, 720, 18, 13, 2175, 3403, 6212, 323, 10519, 264, 13263, 6212, 9899, 13, 128009]\n",
            "labels:\n",
            "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule.<|eot_id|>\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 09:55:44,384 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 09:55:44,384 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 09:55:44] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-20 09:55:44,475 >> loading weights file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-20 09:55:44,475 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 09:55:44,476 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.06s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-20 09:55:48,714 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-20 09:55:48,714 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-20 09:55:48,715 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 09:55:48,715 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 09:55:48] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-06-20 09:55:48] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-20 09:55:48] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-06-20 09:55:48] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-06-20 09:55:48] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,o_proj,k_proj,down_proj,v_proj,up_proj,q_proj\n",
            "[INFO|2025-06-20 09:55:49] llamafactory.model.loader:143 >> trainable params: 97,255,424 || all params: 3,310,005,248 || trainable%: 2.9382\n",
            "[INFO|trainer.py:756] 2025-06-20 09:55:49,340 >> Using auto half precision backend\n",
            "[WARNING|2025-06-20 09:55:49] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.\n",
            "[INFO|trainer.py:2409] 2025-06-20 09:55:49,559 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-06-20 09:55:49,559 >>   Num examples = 52,002\n",
            "[INFO|trainer.py:2411] 2025-06-20 09:55:49,559 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2412] 2025-06-20 09:55:49,559 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2415] 2025-06-20 09:55:49,559 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2416] 2025-06-20 09:55:49,559 >>   Gradient Accumulation steps = 16\n",
            "[INFO|trainer.py:2417] 2025-06-20 09:55:49,559 >>   Total optimization steps = 3,251\n",
            "[INFO|trainer.py:2418] 2025-06-20 09:55:49,560 >>   Number of trainable parameters = 97,255,424\n",
            "{'loss': 1.5969, 'grad_norm': 1.5004292726516724, 'learning_rate': 1.3803680981595093e-06, 'epoch': 0.0}\n",
            "{'loss': 1.6987, 'grad_norm': 1.4765392541885376, 'learning_rate': 2.914110429447853e-06, 'epoch': 0.01}\n",
            "{'loss': 1.489, 'grad_norm': 1.7128969430923462, 'learning_rate': 4.447852760736196e-06, 'epoch': 0.01}\n",
            "{'loss': 1.5321, 'grad_norm': 1.1872761249542236, 'learning_rate': 5.98159509202454e-06, 'epoch': 0.01}\n",
            "{'loss': 1.5181, 'grad_norm': 1.080183506011963, 'learning_rate': 7.5153374233128836e-06, 'epoch': 0.02}\n",
            "{'loss': 1.4767, 'grad_norm': 1.2288795709609985, 'learning_rate': 9.049079754601228e-06, 'epoch': 0.02}\n",
            "{'loss': 1.3982, 'grad_norm': 0.9941940903663635, 'learning_rate': 1.0582822085889571e-05, 'epoch': 0.02}\n",
            "{'loss': 1.3841, 'grad_norm': 1.0961767435073853, 'learning_rate': 1.2116564417177914e-05, 'epoch': 0.02}\n",
            "{'loss': 1.4547, 'grad_norm': 1.1198890209197998, 'learning_rate': 1.3650306748466258e-05, 'epoch': 0.03}\n",
            "{'loss': 1.4596, 'grad_norm': 0.9893452525138855, 'learning_rate': 1.5184049079754603e-05, 'epoch': 0.03}\n",
            "{'loss': 1.3283, 'grad_norm': 0.807388186454773, 'learning_rate': 1.6717791411042948e-05, 'epoch': 0.03}\n",
            "{'loss': 1.3442, 'grad_norm': 1.1464101076126099, 'learning_rate': 1.825153374233129e-05, 'epoch': 0.04}\n",
            "{'loss': 1.3327, 'grad_norm': 1.8009483814239502, 'learning_rate': 1.978527607361963e-05, 'epoch': 0.04}\n",
            "{'loss': 1.3671, 'grad_norm': 1.187628984451294, 'learning_rate': 2.1319018404907976e-05, 'epoch': 0.04}\n",
            "{'loss': 1.3051, 'grad_norm': 0.900913417339325, 'learning_rate': 2.285276073619632e-05, 'epoch': 0.05}\n",
            "{'loss': 1.3761, 'grad_norm': 1.1544972658157349, 'learning_rate': 2.4386503067484666e-05, 'epoch': 0.05}\n",
            "{'loss': 1.3586, 'grad_norm': 0.8973464369773865, 'learning_rate': 2.5920245398773008e-05, 'epoch': 0.05}\n",
            "{'loss': 1.352, 'grad_norm': 0.959588348865509, 'learning_rate': 2.7453987730061353e-05, 'epoch': 0.06}\n",
            "{'loss': 1.3606, 'grad_norm': 0.9869799017906189, 'learning_rate': 2.8987730061349694e-05, 'epoch': 0.06}\n",
            "{'loss': 1.3524, 'grad_norm': 1.245803952217102, 'learning_rate': 3.052147239263804e-05, 'epoch': 0.06}\n",
            "{'loss': 1.3036, 'grad_norm': 0.7345926761627197, 'learning_rate': 3.205521472392638e-05, 'epoch': 0.06}\n",
            "{'loss': 1.3939, 'grad_norm': 1.0124375820159912, 'learning_rate': 3.3588957055214726e-05, 'epoch': 0.07}\n",
            "{'loss': 1.3604, 'grad_norm': 0.8076252937316895, 'learning_rate': 3.512269938650307e-05, 'epoch': 0.07}\n",
            "{'loss': 1.3147, 'grad_norm': 1.178832769393921, 'learning_rate': 3.665644171779141e-05, 'epoch': 0.07}\n",
            "{'loss': 1.3548, 'grad_norm': 0.9093260765075684, 'learning_rate': 3.819018404907976e-05, 'epoch': 0.08}\n",
            "{'loss': 1.3535, 'grad_norm': 0.976043701171875, 'learning_rate': 3.97239263803681e-05, 'epoch': 0.08}\n",
            "{'loss': 1.2522, 'grad_norm': 0.9428687691688538, 'learning_rate': 4.1257668711656444e-05, 'epoch': 0.08}\n",
            "{'loss': 1.2762, 'grad_norm': 0.806662917137146, 'learning_rate': 4.279141104294479e-05, 'epoch': 0.09}\n",
            "{'loss': 1.3417, 'grad_norm': 0.8675147294998169, 'learning_rate': 4.432515337423313e-05, 'epoch': 0.09}\n",
            "{'loss': 1.3486, 'grad_norm': 0.8513851761817932, 'learning_rate': 4.585889570552148e-05, 'epoch': 0.09}\n",
            "{'loss': 1.3942, 'grad_norm': 0.9588231444358826, 'learning_rate': 4.739263803680982e-05, 'epoch': 0.1}\n",
            "{'loss': 1.2764, 'grad_norm': 0.8616772890090942, 'learning_rate': 4.892638036809816e-05, 'epoch': 0.1}\n",
            "{'loss': 1.4092, 'grad_norm': 1.0554505586624146, 'learning_rate': 4.999987022227664e-05, 'epoch': 0.1}\n",
            "{'loss': 1.2999, 'grad_norm': 0.755549967288971, 'learning_rate': 4.999756310023261e-05, 'epoch': 0.1}\n",
            "{'loss': 1.3034, 'grad_norm': 0.720070481300354, 'learning_rate': 4.999237233512246e-05, 'epoch': 0.11}\n",
            "{'loss': 1.2476, 'grad_norm': 0.8173062205314636, 'learning_rate': 4.998429852573712e-05, 'epoch': 0.11}\n",
            "{'loss': 1.2704, 'grad_norm': 0.8331644535064697, 'learning_rate': 4.997334260344674e-05, 'epoch': 0.11}\n",
            "{'loss': 1.3186, 'grad_norm': 0.7423588037490845, 'learning_rate': 4.99595058320933e-05, 'epoch': 0.12}\n",
            "{'loss': 1.3051, 'grad_norm': 1.0405843257904053, 'learning_rate': 4.994278980784478e-05, 'epoch': 0.12}\n",
            "{'loss': 1.2907, 'grad_norm': 0.872518002986908, 'learning_rate': 4.992319645901102e-05, 'epoch': 0.12}\n",
            "{'loss': 1.2641, 'grad_norm': 0.6077193021774292, 'learning_rate': 4.990072804582134e-05, 'epoch': 0.13}\n",
            "{'loss': 1.2964, 'grad_norm': 0.6800562143325806, 'learning_rate': 4.9875387160163744e-05, 'epoch': 0.13}\n",
            "{'loss': 1.3282, 'grad_norm': 1.1761960983276367, 'learning_rate': 4.9847176725285934e-05, 'epoch': 0.13}\n",
            "{'loss': 1.2701, 'grad_norm': 1.0491682291030884, 'learning_rate': 4.981609999545812e-05, 'epoch': 0.14}\n",
            "{'loss': 1.2498, 'grad_norm': 0.7739600539207458, 'learning_rate': 4.978216055559761e-05, 'epoch': 0.14}\n",
            "{'loss': 1.2612, 'grad_norm': 0.6283634305000305, 'learning_rate': 4.974536232085526e-05, 'epoch': 0.14}\n",
            "{'loss': 1.2818, 'grad_norm': 0.9059051871299744, 'learning_rate': 4.9705709536163824e-05, 'epoch': 0.14}\n",
            "{'loss': 1.4095, 'grad_norm': 0.782609760761261, 'learning_rate': 4.966320677574827e-05, 'epoch': 0.15}\n",
            "{'loss': 1.2747, 'grad_norm': 0.7619616389274597, 'learning_rate': 4.961785894259816e-05, 'epoch': 0.15}\n",
            "{'loss': 1.2621, 'grad_norm': 0.7010527849197388, 'learning_rate': 4.956967126790198e-05, 'epoch': 0.15}\n",
            " 15%|                                 | 500/3251 [09:38<52:54,  1.15s/it][INFO|trainer.py:3993] 2025-06-20 10:05:27,975 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-500\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:05:27,980 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:05:27,980 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:05:28,447 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:05:28,448 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:05:28,448 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.2924, 'grad_norm': 0.912004292011261, 'learning_rate': 4.951864931044374e-05, 'epoch': 0.16}\n",
            "{'loss': 1.3897, 'grad_norm': 0.6586970686912537, 'learning_rate': 4.946479895596171e-05, 'epoch': 0.16}\n",
            "{'loss': 1.3272, 'grad_norm': 0.8901364803314209, 'learning_rate': 4.940812641646947e-05, 'epoch': 0.16}\n",
            "{'loss': 1.3178, 'grad_norm': 0.7853840589523315, 'learning_rate': 4.934863822953929e-05, 'epoch': 0.17}\n",
            "{'loss': 1.3521, 'grad_norm': 0.7608779668807983, 'learning_rate': 4.9286341257548006e-05, 'epoch': 0.17}\n",
            "{'loss': 1.263, 'grad_norm': 0.8413190245628357, 'learning_rate': 4.9221242686885374e-05, 'epoch': 0.17}\n",
            "{'loss': 1.3751, 'grad_norm': 0.6069070100784302, 'learning_rate': 4.9153350027125064e-05, 'epoch': 0.18}\n",
            "{'loss': 1.3162, 'grad_norm': 0.6096220016479492, 'learning_rate': 4.90826711101584e-05, 'epoch': 0.18}\n",
            "{'loss': 1.206, 'grad_norm': 0.8598476052284241, 'learning_rate': 4.900921408929089e-05, 'epoch': 0.18}\n",
            "{'loss': 1.3456, 'grad_norm': 0.6480036973953247, 'learning_rate': 4.893298743830168e-05, 'epoch': 0.18}\n",
            "{'loss': 1.2924, 'grad_norm': 0.833923876285553, 'learning_rate': 4.885399995046605e-05, 'epoch': 0.19}\n",
            "{'loss': 1.2839, 'grad_norm': 0.9669191241264343, 'learning_rate': 4.8772260737541035e-05, 'epoch': 0.19}\n",
            "{'loss': 1.3116, 'grad_norm': 0.6853195428848267, 'learning_rate': 4.868777922871434e-05, 'epoch': 0.19}\n",
            "{'loss': 1.3663, 'grad_norm': 0.6464191675186157, 'learning_rate': 4.860056516951661e-05, 'epoch': 0.2}\n",
            "{'loss': 1.3864, 'grad_norm': 0.7262828946113586, 'learning_rate': 4.85106286206972e-05, 'epoch': 0.2}\n",
            "{'loss': 1.3037, 'grad_norm': 0.7253714799880981, 'learning_rate': 4.8417979957063624e-05, 'epoch': 0.2}\n",
            "{'loss': 1.3592, 'grad_norm': 0.8878002166748047, 'learning_rate': 4.832262986628474e-05, 'epoch': 0.21}\n",
            "{'loss': 1.2548, 'grad_norm': 0.9471771717071533, 'learning_rate': 4.822458934765783e-05, 'epoch': 0.21}\n",
            "{'loss': 1.2917, 'grad_norm': 0.6972962617874146, 'learning_rate': 4.812386971083979e-05, 'epoch': 0.21}\n",
            "{'loss': 1.32, 'grad_norm': 0.7637922763824463, 'learning_rate': 4.802048257454246e-05, 'epoch': 0.22}\n",
            "{'loss': 1.3558, 'grad_norm': 0.679452121257782, 'learning_rate': 4.791443986519232e-05, 'epoch': 0.22}\n",
            "{'loss': 1.3434, 'grad_norm': 0.6742339730262756, 'learning_rate': 4.780575381555472e-05, 'epoch': 0.22}\n",
            "{'loss': 1.3771, 'grad_norm': 0.9300139546394348, 'learning_rate': 4.769443696332272e-05, 'epoch': 0.22}\n",
            "{'loss': 1.1927, 'grad_norm': 0.7424923777580261, 'learning_rate': 4.7580502149670784e-05, 'epoch': 0.23}\n",
            "{'loss': 1.3348, 'grad_norm': 0.6612588763237, 'learning_rate': 4.7463962517773474e-05, 'epoch': 0.23}\n",
            "{'loss': 1.3628, 'grad_norm': 0.7518802881240845, 'learning_rate': 4.7344831511289286e-05, 'epoch': 0.23}\n",
            "{'loss': 1.2472, 'grad_norm': 0.6745702028274536, 'learning_rate': 4.722312287280982e-05, 'epoch': 0.24}\n",
            "{'loss': 1.2965, 'grad_norm': 0.7001184225082397, 'learning_rate': 4.7098850642274496e-05, 'epoch': 0.24}\n",
            "{'loss': 1.3838, 'grad_norm': 0.6113878488540649, 'learning_rate': 4.697202915535093e-05, 'epoch': 0.24}\n",
            "{'loss': 1.33, 'grad_norm': 0.7473039627075195, 'learning_rate': 4.684267304178122e-05, 'epoch': 0.25}\n",
            "{'loss': 1.3095, 'grad_norm': 0.990020751953125, 'learning_rate': 4.6710797223694325e-05, 'epoch': 0.25}\n",
            "{'loss': 1.2942, 'grad_norm': 0.8054026961326599, 'learning_rate': 4.657641691388466e-05, 'epoch': 0.25}\n",
            "{'loss': 1.2853, 'grad_norm': 0.8701156973838806, 'learning_rate': 4.64395476140572e-05, 'epoch': 0.26}\n",
            "{'loss': 1.2596, 'grad_norm': 0.644564688205719, 'learning_rate': 4.63002051130393e-05, 'epoch': 0.26}\n",
            "{'loss': 1.3286, 'grad_norm': 0.7315897941589355, 'learning_rate': 4.6158405484959256e-05, 'epoch': 0.26}\n",
            "{'loss': 1.2854, 'grad_norm': 0.6173883676528931, 'learning_rate': 4.601416508739211e-05, 'epoch': 0.26}\n",
            "{'loss': 1.2356, 'grad_norm': 0.9024651646614075, 'learning_rate': 4.5867500559472676e-05, 'epoch': 0.27}\n",
            "{'loss': 1.2572, 'grad_norm': 0.7533137202262878, 'learning_rate': 4.5718428819976095e-05, 'epoch': 0.27}\n",
            "{'loss': 1.3409, 'grad_norm': 0.8293076753616333, 'learning_rate': 4.556696706536612e-05, 'epoch': 0.27}\n",
            "{'loss': 1.359, 'grad_norm': 0.9442306160926819, 'learning_rate': 4.54131327678114e-05, 'epoch': 0.28}\n",
            "{'loss': 1.2446, 'grad_norm': 0.8266235589981079, 'learning_rate': 4.525694367316996e-05, 'epoch': 0.28}\n",
            "{'loss': 1.3346, 'grad_norm': 0.5926026105880737, 'learning_rate': 4.5098417798942064e-05, 'epoch': 0.28}\n",
            "{'loss': 1.2104, 'grad_norm': 0.7711595892906189, 'learning_rate': 4.4937573432191766e-05, 'epoch': 0.29}\n",
            "{'loss': 1.213, 'grad_norm': 0.6691184639930725, 'learning_rate': 4.4774429127437424e-05, 'epoch': 0.29}\n",
            "{'loss': 1.2167, 'grad_norm': 0.7554508447647095, 'learning_rate': 4.460900370451123e-05, 'epoch': 0.29}\n",
            "{'loss': 1.2855, 'grad_norm': 0.7422290444374084, 'learning_rate': 4.444131624638828e-05, 'epoch': 0.3}\n",
            "{'loss': 1.3521, 'grad_norm': 0.5701771378517151, 'learning_rate': 4.427138609698517e-05, 'epoch': 0.3}\n",
            "{'loss': 1.325, 'grad_norm': 0.5862628817558289, 'learning_rate': 4.409923285892858e-05, 'epoch': 0.3}\n",
            "{'loss': 1.2822, 'grad_norm': 0.7762437462806702, 'learning_rate': 4.3924876391293915e-05, 'epoch': 0.3}\n",
            "{'loss': 1.2212, 'grad_norm': 0.7308928370475769, 'learning_rate': 4.374833680731451e-05, 'epoch': 0.31}\n",
            " 31%|                           | 1000/3251 [19:19<43:49,  1.17s/it][INFO|trainer.py:3993] 2025-06-20 10:15:09,430 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-1000\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:15:09,435 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:15:09,435 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:15:09,909 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-1000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:15:09,910 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:15:09,911 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.3051, 'grad_norm': 0.7297415733337402, 'learning_rate': 4.356963447206136e-05, 'epoch': 0.31}\n",
            "{'loss': 1.3041, 'grad_norm': 0.6881752610206604, 'learning_rate': 4.3388790000093885e-05, 'epoch': 0.31}\n",
            "{'loss': 1.3223, 'grad_norm': 0.8003470301628113, 'learning_rate': 4.3205824253081896e-05, 'epoch': 0.32}\n",
            "{'loss': 1.2361, 'grad_norm': 0.715890109539032, 'learning_rate': 4.302075833739907e-05, 'epoch': 0.32}\n",
            "{'loss': 1.3216, 'grad_norm': 0.7600797414779663, 'learning_rate': 4.283361360168817e-05, 'epoch': 0.32}\n",
            "{'loss': 1.2042, 'grad_norm': 0.5582646727561951, 'learning_rate': 4.264441163439834e-05, 'epoch': 0.33}\n",
            "{'loss': 1.2677, 'grad_norm': 0.7896692156791687, 'learning_rate': 4.245317426129469e-05, 'epoch': 0.33}\n",
            "{'loss': 1.2727, 'grad_norm': 0.6082053184509277, 'learning_rate': 4.225992354294061e-05, 'epoch': 0.33}\n",
            "{'loss': 1.3062, 'grad_norm': 0.6253966689109802, 'learning_rate': 4.206468177215287e-05, 'epoch': 0.34}\n",
            "{'loss': 1.2719, 'grad_norm': 0.7188650369644165, 'learning_rate': 4.186747147143001e-05, 'epoch': 0.34}\n",
            "{'loss': 1.3408, 'grad_norm': 0.6797708868980408, 'learning_rate': 4.166831539035423e-05, 'epoch': 0.34}\n",
            "{'loss': 1.2615, 'grad_norm': 0.7254276871681213, 'learning_rate': 4.146723650296701e-05, 'epoch': 0.34}\n",
            "{'loss': 1.2352, 'grad_norm': 0.8894206881523132, 'learning_rate': 4.1264258005118995e-05, 'epoch': 0.35}\n",
            "{'loss': 1.2913, 'grad_norm': 0.6271803379058838, 'learning_rate': 4.1059403311794085e-05, 'epoch': 0.35}\n",
            "{'loss': 1.3539, 'grad_norm': 0.9748306274414062, 'learning_rate': 4.085269605440842e-05, 'epoch': 0.35}\n",
            "{'loss': 1.2829, 'grad_norm': 0.6025514006614685, 'learning_rate': 4.0644160078084305e-05, 'epoch': 0.36}\n",
            "{'loss': 1.2512, 'grad_norm': 0.7158992886543274, 'learning_rate': 4.04338194388995e-05, 'epoch': 0.36}\n",
            "{'loss': 1.259, 'grad_norm': 0.7244558930397034, 'learning_rate': 4.022169840111224e-05, 'epoch': 0.36}\n",
            "{'loss': 1.3289, 'grad_norm': 0.7086434960365295, 'learning_rate': 4.000782143436211e-05, 'epoch': 0.37}\n",
            "{'loss': 1.2739, 'grad_norm': 0.6606099009513855, 'learning_rate': 3.979221321084734e-05, 'epoch': 0.37}\n",
            "{'loss': 1.2041, 'grad_norm': 0.7428708076477051, 'learning_rate': 3.957489860247869e-05, 'epoch': 0.37}\n",
            "{'loss': 1.3518, 'grad_norm': 0.7734151482582092, 'learning_rate': 3.935590267801032e-05, 'epoch': 0.38}\n",
            "{'loss': 1.2732, 'grad_norm': 0.5854929089546204, 'learning_rate': 3.913525070014789e-05, 'epoch': 0.38}\n",
            "{'loss': 1.2592, 'grad_norm': 0.7283006906509399, 'learning_rate': 3.891296812263437e-05, 'epoch': 0.38}\n",
            "{'loss': 1.2799, 'grad_norm': 0.6962164044380188, 'learning_rate': 3.868908058731376e-05, 'epoch': 0.38}\n",
            "{'loss': 1.2327, 'grad_norm': 0.7836632132530212, 'learning_rate': 3.846361392117312e-05, 'epoch': 0.39}\n",
            "{'loss': 1.234, 'grad_norm': 0.736956000328064, 'learning_rate': 3.823659413336327e-05, 'epoch': 0.39}\n",
            "{'loss': 1.3078, 'grad_norm': 0.7539713978767395, 'learning_rate': 3.80080474121984e-05, 'epoch': 0.39}\n",
            "{'loss': 1.2366, 'grad_norm': 0.8410632610321045, 'learning_rate': 3.777800012213514e-05, 'epoch': 0.4}\n",
            "{'loss': 1.3195, 'grad_norm': 0.7992151975631714, 'learning_rate': 3.754647880073117e-05, 'epoch': 0.4}\n",
            "{'loss': 1.1852, 'grad_norm': 0.7767837047576904, 'learning_rate': 3.731351015558396e-05, 'epoch': 0.4}\n",
            "{'loss': 1.3347, 'grad_norm': 0.8811294436454773, 'learning_rate': 3.707912106124988e-05, 'epoch': 0.41}\n",
            "{'loss': 1.3396, 'grad_norm': 0.9311949014663696, 'learning_rate': 3.684333855614398e-05, 'epoch': 0.41}\n",
            "{'loss': 1.3461, 'grad_norm': 0.6953355669975281, 'learning_rate': 3.6606189839420966e-05, 'epoch': 0.41}\n",
            "{'loss': 1.2712, 'grad_norm': 0.7668704390525818, 'learning_rate': 3.6367702267837564e-05, 'epoch': 0.42}\n",
            "{'loss': 1.199, 'grad_norm': 0.7858191728591919, 'learning_rate': 3.612790335259675e-05, 'epoch': 0.42}\n",
            "{'loss': 1.2926, 'grad_norm': 0.6585897207260132, 'learning_rate': 3.588682075617411e-05, 'epoch': 0.42}\n",
            "{'loss': 1.2269, 'grad_norm': 0.7652862071990967, 'learning_rate': 3.564448228912682e-05, 'epoch': 0.42}\n",
            "{'loss': 1.2677, 'grad_norm': 0.8731297254562378, 'learning_rate': 3.540091590688546e-05, 'epoch': 0.43}\n",
            "{'loss': 1.221, 'grad_norm': 0.5935651659965515, 'learning_rate': 3.515614970652919e-05, 'epoch': 0.43}\n",
            "{'loss': 1.3082, 'grad_norm': 0.6454752683639526, 'learning_rate': 3.491021192354455e-05, 'epoch': 0.43}\n",
            "{'loss': 1.3142, 'grad_norm': 0.6927536129951477, 'learning_rate': 3.4663130928568324e-05, 'epoch': 0.44}\n",
            "{'loss': 1.3725, 'grad_norm': 0.6873111724853516, 'learning_rate': 3.441493522411471e-05, 'epoch': 0.44}\n",
            "{'loss': 1.2757, 'grad_norm': 0.5540832281112671, 'learning_rate': 3.4165653441287444e-05, 'epoch': 0.44}\n",
            "{'loss': 1.3238, 'grad_norm': 0.7338401675224304, 'learning_rate': 3.391531433647698e-05, 'epoch': 0.45}\n",
            "{'loss': 1.3257, 'grad_norm': 0.6932085156440735, 'learning_rate': 3.366394678804322e-05, 'epoch': 0.45}\n",
            "{'loss': 1.2959, 'grad_norm': 0.7691246271133423, 'learning_rate': 3.3411579792984174e-05, 'epoch': 0.45}\n",
            "{'loss': 1.3001, 'grad_norm': 0.70415198802948, 'learning_rate': 3.315824246359103e-05, 'epoch': 0.46}\n",
            "{'loss': 1.3117, 'grad_norm': 0.6305437088012695, 'learning_rate': 3.290396402408977e-05, 'epoch': 0.46}\n",
            "{'loss': 1.366, 'grad_norm': 0.9217102527618408, 'learning_rate': 3.264877380726999e-05, 'epoch': 0.46}\n",
            " 46%|                     | 1500/3251 [29:02<33:35,  1.15s/it][INFO|trainer.py:3993] 2025-06-20 10:24:51,641 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-1500\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:24:51,650 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:24:51,651 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:24:52,049 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-1500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:24:52,051 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:24:52,051 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.2837, 'grad_norm': 0.7597267627716064, 'learning_rate': 3.239270125110117e-05, 'epoch': 0.46}\n",
            "{'loss': 1.2522, 'grad_norm': 0.7648651599884033, 'learning_rate': 3.213577589533676e-05, 'epoch': 0.47}\n",
            "{'loss': 1.2324, 'grad_norm': 0.6519696712493896, 'learning_rate': 3.187802737810659e-05, 'epoch': 0.47}\n",
            "{'loss': 1.276, 'grad_norm': 0.7172185778617859, 'learning_rate': 3.1619485432497906e-05, 'epoch': 0.47}\n",
            "{'loss': 1.2956, 'grad_norm': 0.5466824769973755, 'learning_rate': 3.136017988312544e-05, 'epoch': 0.48}\n",
            "{'loss': 1.2558, 'grad_norm': 0.8227092027664185, 'learning_rate': 3.110014064269094e-05, 'epoch': 0.48}\n",
            "{'loss': 1.2226, 'grad_norm': 0.6593570113182068, 'learning_rate': 3.083939770853254e-05, 'epoch': 0.48}\n",
            "{'loss': 1.2248, 'grad_norm': 0.7206066250801086, 'learning_rate': 3.0577981159164313e-05, 'epoch': 0.49}\n",
            "{'loss': 1.2607, 'grad_norm': 0.7062267661094666, 'learning_rate': 3.0315921150806593e-05, 'epoch': 0.49}\n",
            "{'loss': 1.2999, 'grad_norm': 0.7319441437721252, 'learning_rate': 3.0053247913907144e-05, 'epoch': 0.49}\n",
            "{'loss': 1.2765, 'grad_norm': 0.7649059891700745, 'learning_rate': 2.978999174965395e-05, 'epoch': 0.5}\n",
            "{'loss': 1.3049, 'grad_norm': 0.7563216686248779, 'learning_rate': 2.9526183026479702e-05, 'epoch': 0.5}\n",
            "{'loss': 1.2784, 'grad_norm': 1.071363091468811, 'learning_rate': 2.9261852176558614e-05, 'epoch': 0.5}\n",
            "{'loss': 1.2906, 'grad_norm': 0.9001853466033936, 'learning_rate': 2.8997029692295874e-05, 'epoch': 0.5}\n",
            "{'loss': 1.2773, 'grad_norm': 0.812362015247345, 'learning_rate': 2.8731746122810105e-05, 'epoch': 0.51}\n",
            "{'loss': 1.3229, 'grad_norm': 0.6980142593383789, 'learning_rate': 2.8466032070409325e-05, 'epoch': 0.51}\n",
            "{'loss': 1.2452, 'grad_norm': 0.9507542252540588, 'learning_rate': 2.8199918187060752e-05, 'epoch': 0.51}\n",
            "{'loss': 1.2741, 'grad_norm': 0.7545222640037537, 'learning_rate': 2.7933435170854906e-05, 'epoch': 0.52}\n",
            "{'loss': 1.3094, 'grad_norm': 0.7184224128723145, 'learning_rate': 2.7666613762464333e-05, 'epoch': 0.52}\n",
            "{'loss': 1.2441, 'grad_norm': 1.0891634225845337, 'learning_rate': 2.739948474159748e-05, 'epoch': 0.52}\n",
            "{'loss': 1.3327, 'grad_norm': 0.7132405042648315, 'learning_rate': 2.713207892344804e-05, 'epoch': 0.53}\n",
            "{'loss': 1.2437, 'grad_norm': 0.6861761808395386, 'learning_rate': 2.6864427155140204e-05, 'epoch': 0.53}\n",
            "{'loss': 1.2867, 'grad_norm': 0.7416762113571167, 'learning_rate': 2.6596560312170228e-05, 'epoch': 0.53}\n",
            "{'loss': 1.2502, 'grad_norm': 0.7861748337745667, 'learning_rate': 2.6328509294844717e-05, 'epoch': 0.54}\n",
            "{'loss': 1.2515, 'grad_norm': 0.672657310962677, 'learning_rate': 2.6060305024716085e-05, 'epoch': 0.54}\n",
            "{'loss': 1.2881, 'grad_norm': 0.6997958421707153, 'learning_rate': 2.5791978441015523e-05, 'epoch': 0.54}\n",
            "{'loss': 1.3325, 'grad_norm': 0.7255260348320007, 'learning_rate': 2.5523560497083926e-05, 'epoch': 0.54}\n",
            "{'loss': 1.3053, 'grad_norm': 0.7027861475944519, 'learning_rate': 2.5255082156801257e-05, 'epoch': 0.55}\n",
            "{'loss': 1.2635, 'grad_norm': 0.9302917718887329, 'learning_rate': 2.4986574391014587e-05, 'epoch': 0.55}\n",
            "{'loss': 1.2612, 'grad_norm': 0.620816171169281, 'learning_rate': 2.471806817396546e-05, 'epoch': 0.55}\n",
            "{'loss': 1.2545, 'grad_norm': 0.6524004936218262, 'learning_rate': 2.4449594479716734e-05, 'epoch': 0.56}\n",
            "{'loss': 1.3422, 'grad_norm': 0.69443279504776, 'learning_rate': 2.418118427857955e-05, 'epoch': 0.56}\n",
            "{'loss': 1.2768, 'grad_norm': 0.6264193058013916, 'learning_rate': 2.3912868533540665e-05, 'epoch': 0.56}\n",
            "{'loss': 1.2683, 'grad_norm': 0.6630963087081909, 'learning_rate': 2.364467819669068e-05, 'epoch': 0.57}\n",
            "{'loss': 1.2605, 'grad_norm': 0.7777931094169617, 'learning_rate': 2.3376644205653477e-05, 'epoch': 0.57}\n",
            "{'loss': 1.3376, 'grad_norm': 0.7450138926506042, 'learning_rate': 2.310879748001736e-05, 'epoch': 0.57}\n",
            "{'loss': 1.2123, 'grad_norm': 0.7659750580787659, 'learning_rate': 2.2841168917768265e-05, 'epoch': 0.58}\n",
            "{'loss': 1.329, 'grad_norm': 0.8391163945198059, 'learning_rate': 2.2573789391725464e-05, 'epoch': 0.58}\n",
            "{'loss': 1.2733, 'grad_norm': 0.693956196308136, 'learning_rate': 2.230668974598018e-05, 'epoch': 0.58}\n",
            "{'loss': 1.3022, 'grad_norm': 0.6947101354598999, 'learning_rate': 2.2039900792337474e-05, 'epoch': 0.58}\n",
            "{'loss': 1.2994, 'grad_norm': 0.6929218769073486, 'learning_rate': 2.1773453306761922e-05, 'epoch': 0.59}\n",
            "{'loss': 1.2069, 'grad_norm': 0.6721246242523193, 'learning_rate': 2.1507378025827356e-05, 'epoch': 0.59}\n",
            "{'loss': 1.276, 'grad_norm': 0.9394107460975647, 'learning_rate': 2.1241705643171233e-05, 'epoch': 0.59}\n",
            "{'loss': 1.3345, 'grad_norm': 0.7397481203079224, 'learning_rate': 2.0976466805953884e-05, 'epoch': 0.6}\n",
            "{'loss': 1.2755, 'grad_norm': 0.7936651706695557, 'learning_rate': 2.0711692111323143e-05, 'epoch': 0.6}\n",
            "{'loss': 1.1715, 'grad_norm': 0.8466300368309021, 'learning_rate': 2.0447412102884775e-05, 'epoch': 0.6}\n",
            "{'loss': 1.2364, 'grad_norm': 0.7646492719650269, 'learning_rate': 2.0183657267179023e-05, 'epoch': 0.61}\n",
            "{'loss': 1.3244, 'grad_norm': 0.9109629392623901, 'learning_rate': 1.992045803016384e-05, 'epoch': 0.61}\n",
            "{'loss': 1.364, 'grad_norm': 0.6600852608680725, 'learning_rate': 1.9657844753704957e-05, 'epoch': 0.61}\n",
            "{'loss': 1.3227, 'grad_norm': 0.6174657344818115, 'learning_rate': 1.9395847732073487e-05, 'epoch': 0.62}\n",
            " 62%|               | 2000/3251 [38:44<24:06,  1.16s/it][INFO|trainer.py:3993] 2025-06-20 10:34:34,298 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-2000\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:34:34,307 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:34:34,307 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:34:34,707 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-2000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:34:34,708 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:34:34,708 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.2633, 'grad_norm': 0.7445766925811768, 'learning_rate': 1.913449718845125e-05, 'epoch': 0.62}\n",
            "{'loss': 1.2865, 'grad_norm': 0.7664647698402405, 'learning_rate': 1.887382327144434e-05, 'epoch': 0.62}\n",
            "{'loss': 1.212, 'grad_norm': 0.7502866387367249, 'learning_rate': 1.8613856051605243e-05, 'epoch': 0.62}\n",
            "{'loss': 1.3323, 'grad_norm': 0.7491949200630188, 'learning_rate': 1.8354625517964e-05, 'epoch': 0.63}\n",
            "{'loss': 1.2046, 'grad_norm': 0.6749635338783264, 'learning_rate': 1.8096161574568775e-05, 'epoch': 0.63}\n",
            "{'loss': 1.264, 'grad_norm': 0.8271406292915344, 'learning_rate': 1.7838494037036203e-05, 'epoch': 0.63}\n",
            "{'loss': 1.2269, 'grad_norm': 0.7813931703567505, 'learning_rate': 1.7581652629111956e-05, 'epoch': 0.64}\n",
            "{'loss': 1.2401, 'grad_norm': 0.8902999758720398, 'learning_rate': 1.73256669792419e-05, 'epoch': 0.64}\n",
            "{'loss': 1.2977, 'grad_norm': 0.7860628366470337, 'learning_rate': 1.7070566617154242e-05, 'epoch': 0.64}\n",
            "{'loss': 1.2783, 'grad_norm': 0.8173573613166809, 'learning_rate': 1.6816380970453084e-05, 'epoch': 0.65}\n",
            "{'loss': 1.2161, 'grad_norm': 0.6690453886985779, 'learning_rate': 1.6563139361223723e-05, 'epoch': 0.65}\n",
            "{'loss': 1.3716, 'grad_norm': 0.7464216351509094, 'learning_rate': 1.6310871002650167e-05, 'epoch': 0.65}\n",
            "{'loss': 1.2984, 'grad_norm': 0.7107959985733032, 'learning_rate': 1.605960499564517e-05, 'epoch': 0.66}\n",
            "{'loss': 1.3407, 'grad_norm': 0.5668176412582397, 'learning_rate': 1.580937032549327e-05, 'epoch': 0.66}\n",
            "{'loss': 1.2739, 'grad_norm': 0.6518493890762329, 'learning_rate': 1.5560195858507083e-05, 'epoch': 0.66}\n",
            "{'loss': 1.2382, 'grad_norm': 0.7493031620979309, 'learning_rate': 1.5312110338697426e-05, 'epoch': 0.66}\n",
            "{'loss': 1.2726, 'grad_norm': 0.679719865322113, 'learning_rate': 1.5065142384457468e-05, 'epoch': 0.67}\n",
            "{'loss': 1.2744, 'grad_norm': 0.8209742307662964, 'learning_rate': 1.4819320485261395e-05, 'epoch': 0.67}\n",
            "{'loss': 1.321, 'grad_norm': 0.6904789805412292, 'learning_rate': 1.457467299837797e-05, 'epoch': 0.67}\n",
            "{'loss': 1.2137, 'grad_norm': 0.7599698305130005, 'learning_rate': 1.4331228145599306e-05, 'epoch': 0.68}\n",
            "{'loss': 1.2452, 'grad_norm': 0.9627902507781982, 'learning_rate': 1.4089014009985294e-05, 'epoch': 0.68}\n",
            "{'loss': 1.31, 'grad_norm': 0.9072927832603455, 'learning_rate': 1.3848058532624025e-05, 'epoch': 0.68}\n",
            "{'loss': 1.2738, 'grad_norm': 0.7695431113243103, 'learning_rate': 1.3608389509408594e-05, 'epoch': 0.69}\n",
            "{'loss': 1.2666, 'grad_norm': 0.7778801321983337, 'learning_rate': 1.3370034587830649e-05, 'epoch': 0.69}\n",
            "{'loss': 1.3439, 'grad_norm': 0.674649178981781, 'learning_rate': 1.3133021263791051e-05, 'epoch': 0.69}\n",
            "{'loss': 1.311, 'grad_norm': 0.656185507774353, 'learning_rate': 1.2897376878428041e-05, 'epoch': 0.7}\n",
            "{'loss': 1.306, 'grad_norm': 0.7140957713127136, 'learning_rate': 1.2663128614963254e-05, 'epoch': 0.7}\n",
            "{'loss': 1.275, 'grad_norm': 0.7466020584106445, 'learning_rate': 1.2430303495565928e-05, 'epoch': 0.7}\n",
            "{'loss': 1.2506, 'grad_norm': 0.5747889876365662, 'learning_rate': 1.2198928378235716e-05, 'epoch': 0.7}\n",
            "{'loss': 1.2362, 'grad_norm': 0.7768704891204834, 'learning_rate': 1.1969029953704439e-05, 'epoch': 0.71}\n",
            "{'loss': 1.2351, 'grad_norm': 0.74408358335495, 'learning_rate': 1.1740634742357104e-05, 'epoch': 0.71}\n",
            "{'loss': 1.2516, 'grad_norm': 0.6115484833717346, 'learning_rate': 1.1513769091172619e-05, 'epoch': 0.71}\n",
            "{'loss': 1.2264, 'grad_norm': 0.6455259323120117, 'learning_rate': 1.1288459170684444e-05, 'epoch': 0.72}\n",
            "{'loss': 1.2997, 'grad_norm': 0.7504639029502869, 'learning_rate': 1.1064730971961704e-05, 'epoch': 0.72}\n",
            "{'loss': 1.2508, 'grad_norm': 0.686194658279419, 'learning_rate': 1.0842610303610867e-05, 'epoch': 0.72}\n",
            "{'loss': 1.3061, 'grad_norm': 0.6416351199150085, 'learning_rate': 1.0622122788798586e-05, 'epoch': 0.73}\n",
            "{'loss': 1.2329, 'grad_norm': 0.7669675350189209, 'learning_rate': 1.0403293862295863e-05, 'epoch': 0.73}\n",
            "{'loss': 1.2651, 'grad_norm': 0.682199239730835, 'learning_rate': 1.0186148767543993e-05, 'epoch': 0.73}\n",
            "{'loss': 1.21, 'grad_norm': 0.5156998038291931, 'learning_rate': 9.970712553742548e-06, 'epoch': 0.74}\n",
            "{'loss': 1.2522, 'grad_norm': 0.8712400197982788, 'learning_rate': 9.757010072959768e-06, 'epoch': 0.74}\n",
            "{'loss': 1.2558, 'grad_norm': 0.7538755536079407, 'learning_rate': 9.545065977265727e-06, 'epoch': 0.74}\n",
            "{'loss': 1.1634, 'grad_norm': 0.7369321584701538, 'learning_rate': 9.334904715888495e-06, 'epoch': 0.74}\n",
            "{'loss': 1.2986, 'grad_norm': 0.6102929711341858, 'learning_rate': 9.12655053239381e-06, 'epoch': 0.75}\n",
            "{'loss': 1.2956, 'grad_norm': 0.6755381226539612, 'learning_rate': 8.920027461888386e-06, 'epoch': 0.75}\n",
            "{'loss': 1.2748, 'grad_norm': 0.7040630578994751, 'learning_rate': 8.71535932824729e-06, 'epoch': 0.75}\n",
            "{'loss': 1.2214, 'grad_norm': 0.6756685376167297, 'learning_rate': 8.512569741365691e-06, 'epoch': 0.76}\n",
            "{'loss': 1.3281, 'grad_norm': 0.703346848487854, 'learning_rate': 8.311682094435311e-06, 'epoch': 0.76}\n",
            "{'loss': 1.2495, 'grad_norm': 0.7915062308311462, 'learning_rate': 8.11271956124584e-06, 'epoch': 0.76}\n",
            "{'loss': 1.2642, 'grad_norm': 0.6540449857711792, 'learning_rate': 7.915705093511693e-06, 'epoch': 0.77}\n",
            "{'loss': 1.2645, 'grad_norm': 0.6819705963134766, 'learning_rate': 7.720661418224356e-06, 'epoch': 0.77}\n",
            " 77%|         | 2500/3251 [48:26<14:41,  1.17s/it][INFO|trainer.py:3993] 2025-06-20 10:44:16,461 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-2500\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:44:16,470 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:44:16,470 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:44:16,871 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-2500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:44:16,873 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:44:16,873 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.2951, 'grad_norm': 0.6707881689071655, 'learning_rate': 7.527611035030677e-06, 'epoch': 0.77}\n",
            "{'loss': 1.2969, 'grad_norm': 0.8762288689613342, 'learning_rate': 7.33657621363737e-06, 'epoch': 0.78}\n",
            "{'loss': 1.2344, 'grad_norm': 0.7198238968849182, 'learning_rate': 7.147578991242066e-06, 'epoch': 0.78}\n",
            "{'loss': 1.2911, 'grad_norm': 0.7875018119812012, 'learning_rate': 6.960641169991142e-06, 'epoch': 0.78}\n",
            "{'loss': 1.2166, 'grad_norm': 0.7111390829086304, 'learning_rate': 6.775784314464717e-06, 'epoch': 0.78}\n",
            "{'loss': 1.2621, 'grad_norm': 0.7743099331855774, 'learning_rate': 6.59302974918899e-06, 'epoch': 0.79}\n",
            "{'loss': 1.2877, 'grad_norm': 0.776849627494812, 'learning_rate': 6.412398556176357e-06, 'epoch': 0.79}\n",
            "{'loss': 1.2494, 'grad_norm': 0.7066407799720764, 'learning_rate': 6.233911572493423e-06, 'epoch': 0.79}\n",
            "{'loss': 1.3562, 'grad_norm': 0.8359635472297668, 'learning_rate': 6.057589387857304e-06, 'epoch': 0.8}\n",
            "{'loss': 1.2202, 'grad_norm': 0.829440176486969, 'learning_rate': 5.88345234226047e-06, 'epoch': 0.8}\n",
            "{'loss': 1.2523, 'grad_norm': 0.7483726143836975, 'learning_rate': 5.711520523624364e-06, 'epoch': 0.8}\n",
            "{'loss': 1.3169, 'grad_norm': 0.6846083998680115, 'learning_rate': 5.541813765482162e-06, 'epoch': 0.81}\n",
            "{'loss': 1.226, 'grad_norm': 0.7372167110443115, 'learning_rate': 5.374351644690787e-06, 'epoch': 0.81}\n",
            "{'loss': 1.3419, 'grad_norm': 0.6622819900512695, 'learning_rate': 5.209153479172607e-06, 'epoch': 0.81}\n",
            "{'loss': 1.1905, 'grad_norm': 0.7760494947433472, 'learning_rate': 5.046238325686984e-06, 'epoch': 0.82}\n",
            "{'loss': 1.2691, 'grad_norm': 0.7517784833908081, 'learning_rate': 4.885624977631925e-06, 'epoch': 0.82}\n",
            "{'loss': 1.3257, 'grad_norm': 0.605409562587738, 'learning_rate': 4.727331962876158e-06, 'epoch': 0.82}\n",
            "{'loss': 1.1931, 'grad_norm': 0.7921819090843201, 'learning_rate': 4.571377541621788e-06, 'epoch': 0.82}\n",
            "{'loss': 1.3075, 'grad_norm': 0.7537526488304138, 'learning_rate': 4.41777970429787e-06, 'epoch': 0.83}\n",
            "{'loss': 1.2534, 'grad_norm': 0.6394727826118469, 'learning_rate': 4.2665561694850785e-06, 'epoch': 0.83}\n",
            "{'loss': 1.2788, 'grad_norm': 0.6347246170043945, 'learning_rate': 4.11772438187176e-06, 'epoch': 0.83}\n",
            "{'loss': 1.2528, 'grad_norm': 0.7293198704719543, 'learning_rate': 3.971301510241554e-06, 'epoch': 0.84}\n",
            "{'loss': 1.2728, 'grad_norm': 0.6693453788757324, 'learning_rate': 3.827304445492855e-06, 'epoch': 0.84}\n",
            "{'loss': 1.2681, 'grad_norm': 0.7835639715194702, 'learning_rate': 3.6857497986903354e-06, 'epoch': 0.84}\n",
            "{'loss': 1.2197, 'grad_norm': 0.7484033703804016, 'learning_rate': 3.5466538991487415e-06, 'epoch': 0.85}\n",
            "{'loss': 1.2131, 'grad_norm': 1.26192045211792, 'learning_rate': 3.4100327925491894e-06, 'epoch': 0.85}\n",
            "{'loss': 1.3058, 'grad_norm': 0.6475418210029602, 'learning_rate': 3.275902239088191e-06, 'epoch': 0.85}\n",
            "{'loss': 1.3262, 'grad_norm': 0.5901222825050354, 'learning_rate': 3.1442777116596018e-06, 'epoch': 0.86}\n",
            "{'loss': 1.3278, 'grad_norm': 0.660539984703064, 'learning_rate': 3.0151743940697032e-06, 'epoch': 0.86}\n",
            "{'loss': 1.3417, 'grad_norm': 0.9463173151016235, 'learning_rate': 2.8886071792856768e-06, 'epoch': 0.86}\n",
            "{'loss': 1.2422, 'grad_norm': 0.7615957856178284, 'learning_rate': 2.764590667717562e-06, 'epoch': 0.86}\n",
            "{'loss': 1.2994, 'grad_norm': 0.8940548896789551, 'learning_rate': 2.6431391655340088e-06, 'epoch': 0.87}\n",
            "{'loss': 1.2962, 'grad_norm': 0.7933470606803894, 'learning_rate': 2.5242666830119695e-06, 'epoch': 0.87}\n",
            "{'loss': 1.2874, 'grad_norm': 0.7874031662940979, 'learning_rate': 2.407986932920514e-06, 'epoch': 0.87}\n",
            "{'loss': 1.3806, 'grad_norm': 0.5745418667793274, 'learning_rate': 2.294313328938949e-06, 'epoch': 0.88}\n",
            "{'loss': 1.232, 'grad_norm': 0.7744408249855042, 'learning_rate': 2.1832589841094902e-06, 'epoch': 0.88}\n",
            "{'loss': 1.2443, 'grad_norm': 0.7289494872093201, 'learning_rate': 2.074836709324535e-06, 'epoch': 0.88}\n",
            "{'loss': 1.2509, 'grad_norm': 1.2190864086151123, 'learning_rate': 1.969059011848881e-06, 'epoch': 0.89}\n",
            "{'loss': 1.2438, 'grad_norm': 0.7919538021087646, 'learning_rate': 1.8659380938769123e-06, 'epoch': 0.89}\n",
            "{'loss': 1.26, 'grad_norm': 0.760627031326294, 'learning_rate': 1.7654858511249705e-06, 'epoch': 0.89}\n",
            "{'loss': 1.2183, 'grad_norm': 0.807851254940033, 'learning_rate': 1.6677138714591312e-06, 'epoch': 0.9}\n",
            "{'loss': 1.3221, 'grad_norm': 0.8094848394393921, 'learning_rate': 1.5726334335584404e-06, 'epoch': 0.9}\n",
            "{'loss': 1.2748, 'grad_norm': 0.7517135143280029, 'learning_rate': 1.480255505613856e-06, 'epoch': 0.9}\n",
            "{'loss': 1.3129, 'grad_norm': 0.7986468076705933, 'learning_rate': 1.3905907440629752e-06, 'epoch': 0.9}\n",
            "{'loss': 1.2942, 'grad_norm': 0.6994711756706238, 'learning_rate': 1.3036494923607563e-06, 'epoch': 0.91}\n",
            "{'loss': 1.3221, 'grad_norm': 0.7856400609016418, 'learning_rate': 1.2194417797863178e-06, 'epoch': 0.91}\n",
            "{'loss': 1.3205, 'grad_norm': 0.6712641716003418, 'learning_rate': 1.1379773202859906e-06, 'epoch': 0.91}\n",
            "{'loss': 1.2483, 'grad_norm': 0.9908648729324341, 'learning_rate': 1.0592655113527667e-06, 'epoch': 0.92}\n",
            "{'loss': 1.2512, 'grad_norm': 0.6494804620742798, 'learning_rate': 9.833154329422001e-07, 'epoch': 0.92}\n",
            "{'loss': 1.2458, 'grad_norm': 0.7909372448921204, 'learning_rate': 9.101358464249921e-07, 'epoch': 0.92}\n",
            " 92%|   | 3000/3251 [58:09<04:54,  1.17s/it][INFO|trainer.py:3993] 2025-06-20 10:53:59,207 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-3000\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:53:59,216 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:53:59,216 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:53:59,622 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-3000/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:53:59,623 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:53:59,623 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 1.1735, 'grad_norm': 0.9757347106933594, 'learning_rate': 8.39735193576302e-07, 'epoch': 0.93}\n",
            "{'loss': 1.2115, 'grad_norm': 0.7764169573783875, 'learning_rate': 7.721215956019295e-07, 'epoch': 0.93}\n",
            "{'loss': 1.2237, 'grad_norm': 0.7194696068763733, 'learning_rate': 7.073028522014829e-07, 'epoch': 0.93}\n",
            "{'loss': 1.3113, 'grad_norm': 0.5661380290985107, 'learning_rate': 6.452864406686105e-07, 'epoch': 0.94}\n",
            "{'loss': 1.2588, 'grad_norm': 0.6133414506912231, 'learning_rate': 5.860795150284648e-07, 'epoch': 0.94}\n",
            "{'loss': 1.2547, 'grad_norm': 0.6716983914375305, 'learning_rate': 5.296889052124221e-07, 'epoch': 0.94}\n",
            "{'loss': 1.2066, 'grad_norm': 0.7727931141853333, 'learning_rate': 4.7612111627021175e-07, 'epoch': 0.94}\n",
            "{'loss': 1.3456, 'grad_norm': 0.6729395389556885, 'learning_rate': 4.2538232761950014e-07, 'epoch': 0.95}\n",
            "{'loss': 1.2489, 'grad_norm': 0.6684014797210693, 'learning_rate': 3.7747839233306935e-07, 'epoch': 0.95}\n",
            "{'loss': 1.304, 'grad_norm': 0.7516668438911438, 'learning_rate': 3.3241483646360704e-07, 'epoch': 0.95}\n",
            "{'loss': 1.2215, 'grad_norm': 0.71287602186203, 'learning_rate': 2.901968584062498e-07, 'epoch': 0.96}\n",
            "{'loss': 1.2865, 'grad_norm': 0.5841525197029114, 'learning_rate': 2.508293282989044e-07, 'epoch': 0.96}\n",
            "{'loss': 1.2115, 'grad_norm': 0.8832051753997803, 'learning_rate': 2.1431678746045015e-07, 'epoch': 0.96}\n",
            "{'loss': 1.326, 'grad_norm': 0.8293341398239136, 'learning_rate': 1.8066344786686619e-07, 'epoch': 0.97}\n",
            "{'loss': 1.2964, 'grad_norm': 0.6832204461097717, 'learning_rate': 1.4987319166533674e-07, 'epoch': 0.97}\n",
            "{'loss': 1.2873, 'grad_norm': 0.816582977771759, 'learning_rate': 1.2194957072644286e-07, 'epoch': 0.97}\n",
            "{'loss': 1.3175, 'grad_norm': 0.7637240886688232, 'learning_rate': 9.689580623440109e-08, 'epoch': 0.98}\n",
            "{'loss': 1.3009, 'grad_norm': 0.5593863129615784, 'learning_rate': 7.471478831550805e-08, 'epoch': 0.98}\n",
            "{'loss': 1.254, 'grad_norm': 0.8006445169448853, 'learning_rate': 5.5409075704712656e-08, 'epoch': 0.98}\n",
            "{'loss': 1.24, 'grad_norm': 0.9490498304367065, 'learning_rate': 3.8980895450474455e-08, 'epoch': 0.98}\n",
            "{'loss': 1.3181, 'grad_norm': 0.6333069801330566, 'learning_rate': 2.5432142657844103e-08, 'epoch': 0.99}\n",
            "{'loss': 1.2253, 'grad_norm': 0.8225286602973938, 'learning_rate': 1.4764380269852163e-08, 'epoch': 0.99}\n",
            "{'loss': 1.2972, 'grad_norm': 0.6674398183822632, 'learning_rate': 6.978838887214401e-09, 'epoch': 0.99}\n",
            "{'loss': 1.3105, 'grad_norm': 0.6700630784034729, 'learning_rate': 2.0764166263703698e-09, 'epoch': 1.0}\n",
            "{'loss': 1.3577, 'grad_norm': 0.5841853618621826, 'learning_rate': 5.767901588571434e-11, 'epoch': 1.0}\n",
            "100%|| 3251/3251 [1:03:02<00:00,  1.13it/s][INFO|trainer.py:3993] 2025-06-20 10:58:51,567 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca/checkpoint-3251\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:58:51,576 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:58:51,576 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:58:51,992 >> chat template saved in InstructionTuning_LoRA_Alpaca/checkpoint-3251/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:58:51,994 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/checkpoint-3251/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:58:51,994 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/checkpoint-3251/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-06-20 10:58:52,841 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3783.2815, 'train_samples_per_second': 13.745, 'train_steps_per_second': 0.859, 'train_loss': 1.2925277139105749, 'epoch': 1.0}\n",
            "100%|| 3251/3251 [1:03:03<00:00,  1.16s/it]\n",
            "[INFO|trainer.py:3993] 2025-06-20 10:58:52,843 >> Saving model checkpoint to InstructionTuning_LoRA_Alpaca\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 10:58:52,852 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 10:58:52,853 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-06-20 10:58:53,251 >> chat template saved in InstructionTuning_LoRA_Alpaca/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-06-20 10:58:53,252 >> tokenizer config file saved in InstructionTuning_LoRA_Alpaca/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-06-20 10:58:53,252 >> Special tokens file saved in InstructionTuning_LoRA_Alpaca/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =         1.0\n",
            "  total_flos               = 105575036GF\n",
            "  train_loss               =      1.2925\n",
            "  train_runtime            =  1:03:03.28\n",
            "  train_samples_per_second =      13.745\n",
            "  train_steps_per_second   =       0.859\n",
            "Figure saved at: InstructionTuning_LoRA_Alpaca/training_loss.png\n",
            "[WARNING|2025-06-20 10:58:53] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-06-20 10:58:53] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:450] 2025-06-20 10:58:53,414 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli train InstructionTuning_LoRA_Alpaca.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L6BAshMmyf6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpMggOmamyf6"
      },
      "source": [
        "## Perform Evalution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKSeUo2cmyf6"
      },
      "source": [
        "### Evalution check for pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szl8SjXZmyf6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# --- GeneralCapability Evaluation ---\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    model_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged\",\n",
        "    # adapter_name_or_path=\"InstructionTuning_LoRA_Alpaca\",\n",
        "    trust_remote_code=True,\n",
        "    template=\"fewshot\",         # fewshot prompt template\n",
        "\n",
        "    # Method settings\n",
        "    finetuning_type=\"freeze\",\n",
        "\n",
        "    # Dataset settings\n",
        "    task=\"mmlu_test\",           # or ceval_validation, cmmlu_test\n",
        "    lang=\"en\",\n",
        "    n_shot=5,\n",
        "\n",
        "    # Output and evaluation\n",
        "    save_dir=\"CPT_freeze/eval\",\n",
        "    batch_size=4,\n",
        "    seed=42,\n",
        "    # download_mode=\"reuse\",      # reuse existing data if present\n",
        ")\n",
        "\n",
        "# write out your eval config\n",
        "json.dump(args, open(\"CPT_freeze_eval.json\", \"w\"), indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5A44wmgmyf6",
        "outputId": "28c87714-009d-4702-d4b0-e7a6fbedc986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,342 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,342 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,342 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,342 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,342 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,342 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 11:28:02,563 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 11:28:02,564 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 11:28:02,565 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,566 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,566 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,566 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,566 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,566 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:28:02,566 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 11:28:02,799 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 11:28:02,816 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 11:28:02,817 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 11:28:02] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-20 11:28:02,903 >> loading weights file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-20 11:28:02,904 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 11:28:02,904 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.03s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-20 11:28:07,136 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-20 11:28:07,136 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-20 11:28:07,136 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 11:28:07,137 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 11:28:07] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-20 11:28:07] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
            "Processing subjects:   0%|                               | 0/57 [00:00<?, ?it/s]\n",
            "Generating test split: 100 examples [00:00, 20752.58 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 6935.86 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3822.04 examples/s]\n",
            "Processing subjects:   0%|             | 0/57 [00:00<?, ?it/s, abstract algebra]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  43%|            | 43/100 [00:00<00:00, 423.08it/s]\u001b[A\n",
            "Formatting batches:  92%| | 92/100 [00:00<00:00, 460.62it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A[WARNING|logging.py:313] 2025-06-20 11:28:07,407 >> You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:05,  4.39it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.51it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  9.97it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:01, 10.91it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:00<00:01, 11.48it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01, 11.92it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:00, 12.06it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:00, 12.17it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:01<00:00, 12.25it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:01<00:00, 12.32it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:01<00:00, 12.33it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:01<00:00, 12.35it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:02<00:00, 12.45it/s]\u001b[A\n",
            "Processing subjects:   2%|     | 1/57 [00:02<02:15,  2.42s/it, abstract algebra]\u001b[A\n",
            "Generating test split: 135 examples [00:00, 34396.25 examples/s]\n",
            "\n",
            "Generating validation split: 14 examples [00:00, 9022.78 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4101.61 examples/s]\n",
            "Processing subjects:   2%|             | 1/57 [00:02<02:15,  2.42s/it, anatomy]\n",
            "Formatting batches:   0%|                               | 0/135 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  37%|             | 50/135 [00:00<00:00, 493.61it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 101/135 [00:00<00:00, 498.02it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/34 [00:00<00:02, 13.44it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/34 [00:00<00:02, 12.54it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 6/34 [00:00<00:02, 12.95it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 8/34 [00:00<00:02, 12.73it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 10/34 [00:00<00:01, 12.98it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 12/34 [00:00<00:01, 12.78it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 14/34 [00:01<00:01, 12.66it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 16/34 [00:01<00:01, 12.64it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 18/34 [00:01<00:01, 12.58it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 20/34 [00:01<00:01, 12.33it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 22/34 [00:01<00:00, 12.45it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 24/34 [00:01<00:00, 12.27it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 26/34 [00:02<00:00, 12.40it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 28/34 [00:02<00:00, 12.45it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 30/34 [00:02<00:00, 12.67it/s]\u001b[A\n",
            "Predicting batches:  94%| | 32/34 [00:02<00:00, 12.78it/s]\u001b[A\n",
            "Predicting batches: 100%|| 34/34 [00:02<00:00, 13.42it/s]\u001b[A\n",
            "Processing subjects:   4%|             | 2/57 [00:05<02:30,  2.73s/it, anatomy]\u001b[A\n",
            "Generating test split: 152 examples [00:00, 36860.21 examples/s]\n",
            "\n",
            "Generating validation split: 16 examples [00:00, 10827.50 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4195.14 examples/s]\n",
            "Processing subjects:   4%|           | 2/57 [00:05<02:30,  2.73s/it, astronomy]\n",
            "Formatting batches:   0%|                               | 0/152 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 46/152 [00:00<00:00, 459.68it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 93/152 [00:00<00:00, 460.49it/s]\u001b[A\n",
            "Formatting batches:  92%| | 140/152 [00:00<00:00, 460.16it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/38 [00:00<00:04,  8.10it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/38 [00:00<00:04,  7.68it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/38 [00:00<00:04,  7.29it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/38 [00:00<00:04,  7.59it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 5/38 [00:00<00:04,  7.65it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/38 [00:00<00:04,  7.70it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 7/38 [00:00<00:03,  7.86it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 8/38 [00:01<00:03,  7.65it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/38 [00:01<00:03,  7.76it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 10/38 [00:01<00:03,  7.70it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 11/38 [00:01<00:03,  7.71it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/38 [00:01<00:03,  7.82it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 13/38 [00:01<00:03,  7.75it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 14/38 [00:01<00:03,  7.68it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 15/38 [00:01<00:02,  7.78it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 16/38 [00:02<00:02,  7.86it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 17/38 [00:02<00:02,  7.92it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 18/38 [00:02<00:02,  7.87it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 19/38 [00:02<00:02,  7.95it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 20/38 [00:02<00:02,  7.88it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 21/38 [00:02<00:02,  7.72it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 22/38 [00:02<00:02,  7.68it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 23/38 [00:02<00:01,  7.68it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 24/38 [00:03<00:01,  7.70it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 25/38 [00:03<00:01,  7.82it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 26/38 [00:03<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 27/38 [00:03<00:01,  7.93it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 28/38 [00:03<00:01,  7.81it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 29/38 [00:03<00:01,  7.79it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 30/38 [00:03<00:01,  7.46it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 31/38 [00:04<00:00,  7.48it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 32/38 [00:04<00:00,  7.48it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 33/38 [00:04<00:00,  7.49it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 34/38 [00:04<00:00,  7.50it/s]\u001b[A\n",
            "Predicting batches:  92%| | 35/38 [00:04<00:00,  7.67it/s]\u001b[A\n",
            "Predicting batches:  95%| | 36/38 [00:04<00:00,  7.68it/s]\u001b[A\n",
            "Predicting batches:  97%|| 37/38 [00:04<00:00,  7.82it/s]\u001b[A\n",
            "Predicting batches: 100%|| 38/38 [00:04<00:00,  7.91it/s]\u001b[A\n",
            "Processing subjects:   5%|           | 3/57 [00:10<03:30,  3.89s/it, astronomy]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 29618.70 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7980.86 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4251.27 examples/s]\n",
            "Processing subjects:   5%|     | 3/57 [00:10<03:30,  3.89s/it, business ethics]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  46%|           | 46/100 [00:00<00:00, 458.46it/s]\u001b[A\n",
            "Formatting batches:  93%| | 93/100 [00:00<00:00, 460.10it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  8.03it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  8.07it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  7.92it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  7.97it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:02,  7.88it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.82it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:01,  7.78it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.76it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.83it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.78it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.69it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:01,  7.56it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.72it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.71it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  7.70it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  7.64it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.65it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  7.78it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.63it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00,  7.84it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.91it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  7.77it/s]\u001b[A\n",
            "Processing subjects:   7%|     | 4/57 [00:14<03:16,  3.71s/it, business ethics]\u001b[A\n",
            "Generating test split: 265 examples [00:00, 45830.88 examples/s]\n",
            "\n",
            "Generating validation split: 29 examples [00:00, 17167.93 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4109.65 examples/s]\n",
            "Processing subjects:   7%|  | 4/57 [00:14<03:16,  3.71s/it, clinical knowledge]\n",
            "Formatting batches:   0%|                               | 0/265 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  19%|                 | 50/265 [00:00<00:00, 494.75it/s]\u001b[A\n",
            "Formatting batches:  38%|            | 100/265 [00:00<00:00, 495.41it/s]\u001b[A\n",
            "Formatting batches:  57%|        | 150/265 [00:00<00:00, 497.17it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 200/265 [00:00<00:00, 497.74it/s]\u001b[A\n",
            "Formatting batches:  94%| | 250/265 [00:00<00:00, 497.95it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/67 [00:00<00:05, 11.60it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/67 [00:00<00:05, 11.10it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 6/67 [00:00<00:05, 11.10it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/67 [00:00<00:05, 11.28it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/67 [00:00<00:05, 11.27it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/67 [00:01<00:04, 11.22it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/67 [00:01<00:04, 11.50it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/67 [00:01<00:04, 11.24it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 18/67 [00:01<00:04, 11.37it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 20/67 [00:01<00:04, 11.18it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 22/67 [00:01<00:03, 11.28it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 24/67 [00:02<00:03, 11.25it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 26/67 [00:02<00:03, 11.49it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 28/67 [00:02<00:03, 11.38it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 30/67 [00:02<00:03, 11.43it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 32/67 [00:02<00:03, 11.62it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 34/67 [00:02<00:02, 11.49it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 36/67 [00:03<00:02, 11.37it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 38/67 [00:03<00:02, 11.29it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 40/67 [00:03<00:02, 10.99it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 42/67 [00:03<00:02, 10.93it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 44/67 [00:03<00:02, 10.82it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 46/67 [00:04<00:01, 11.05it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 48/67 [00:04<00:01, 11.07it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 50/67 [00:04<00:01, 11.09it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 52/67 [00:04<00:01, 11.09it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 54/67 [00:04<00:01, 10.98it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 56/67 [00:04<00:00, 11.01it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 58/67 [00:05<00:00, 11.15it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 60/67 [00:05<00:00, 11.28it/s]\u001b[A\n",
            "Predicting batches:  93%| | 62/67 [00:05<00:00, 11.19it/s]\u001b[A\n",
            "Predicting batches:  96%| | 64/67 [00:05<00:00, 11.29it/s]\u001b[A\n",
            "Predicting batches:  99%|| 66/67 [00:05<00:00, 11.22it/s]\u001b[A\n",
            "Processing subjects:   9%|  | 5/57 [00:20<04:04,  4.70s/it, clinical knowledge]\u001b[A\n",
            "Generating test split: 144 examples [00:00, 35108.98 examples/s]\n",
            "\n",
            "Generating validation split: 16 examples [00:00, 9936.17 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3997.62 examples/s]\n",
            "Processing subjects:   9%|     | 5/57 [00:20<04:04,  4.70s/it, college biology]\n",
            "Formatting batches:   0%|                               | 0/144 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 48/144 [00:00<00:00, 472.87it/s]\u001b[A\n",
            "Formatting batches:  67%|       | 96/144 [00:00<00:00, 476.00it/s]\u001b[A\n",
            "Formatting batches: 100%|| 144/144 [00:00<00:00, 473.51it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/36 [00:00<00:03, 10.23it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/36 [00:00<00:03,  9.38it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 5/36 [00:00<00:03,  9.46it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 6/36 [00:00<00:03,  9.49it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 7/36 [00:00<00:03,  8.99it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 8/36 [00:00<00:03,  9.16it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 9/36 [00:00<00:02,  9.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 10/36 [00:01<00:02,  9.27it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 11/36 [00:01<00:02,  8.72it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 12/36 [00:01<00:02,  8.87it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 13/36 [00:01<00:02,  9.08it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 14/36 [00:01<00:02,  8.85it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 15/36 [00:01<00:02,  8.57it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 16/36 [00:01<00:02,  8.84it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 17/36 [00:01<00:02,  8.94it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 18/36 [00:01<00:02,  8.78it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 20/36 [00:02<00:01,  8.98it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 22/36 [00:02<00:01,  9.43it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 23/36 [00:02<00:01,  9.15it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 24/36 [00:02<00:01,  8.82it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 25/36 [00:02<00:01,  9.00it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 26/36 [00:02<00:01,  9.06it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 27/36 [00:02<00:00,  9.20it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 28/36 [00:03<00:00,  8.96it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 29/36 [00:03<00:00,  8.80it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 30/36 [00:03<00:00,  8.92it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 31/36 [00:03<00:00,  9.02it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 32/36 [00:03<00:00,  9.18it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 33/36 [00:03<00:00,  9.28it/s]\u001b[A\n",
            "Predicting batches:  94%| | 34/36 [00:03<00:00,  9.22it/s]\u001b[A\n",
            "Predicting batches:  97%|| 35/36 [00:03<00:00,  9.21it/s]\u001b[A\n",
            "Predicting batches: 100%|| 36/36 [00:03<00:00,  9.21it/s]\u001b[A\n",
            "Processing subjects:  11%|     | 6/57 [00:24<03:52,  4.56s/it, college biology]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 29955.03 examples/s]\n",
            "\n",
            "Generating validation split: 8 examples [00:00, 5927.30 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3091.32 examples/s]\n",
            "Processing subjects:  11%|   | 6/57 [00:24<03:52,  4.56s/it, college chemistry]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  48%|           | 48/100 [00:00<00:00, 471.37it/s]\u001b[A\n",
            "Formatting batches:  96%|| 96/100 [00:00<00:00, 475.13it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  8.32it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  8.37it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.19it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  8.10it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  8.09it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  8.01it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:02,  7.82it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.76it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:01,  7.92it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.83it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.97it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.99it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.70it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:01,  7.78it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.86it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.92it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  8.16it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.92it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  8.03it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.62it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00,  7.50it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.72it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  8.07it/s]\u001b[A\n",
            "Processing subjects:  12%|   | 7/57 [00:28<03:28,  4.17s/it, college chemistry]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 28042.41 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7850.49 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4024.47 examples/s]\n",
            "Processing subjects:  12%| | 7/57 [00:28<03:28,  4.17s/it, college computer scie\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  43%|            | 43/100 [00:00<00:00, 427.12it/s]\u001b[A\n",
            "Formatting batches:  86%|   | 86/100 [00:00<00:00, 428.54it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:04,  4.82it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:04,  4.96it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:04,  4.96it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:04,  5.00it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:03,  5.14it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:01<00:03,  5.30it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:01<00:03,  5.22it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:03,  5.03it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:03,  5.18it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:02,  5.25it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:02<00:02,  5.08it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:02<00:02,  4.95it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:02<00:02,  4.95it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:02<00:02,  5.00it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:02<00:01,  5.10it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:03<00:01,  4.97it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:03<00:01,  5.09it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:03<00:01,  5.03it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:03<00:01,  5.12it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:03<00:01,  4.99it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:04<00:00,  5.11it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:04<00:00,  5.09it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:04<00:00,  5.05it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:04<00:00,  5.02it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:04<00:00,  4.92it/s]\u001b[A\n",
            "Processing subjects:  14%|| 8/57 [00:33<03:40,  4.50s/it, college computer scie\u001b[A\n",
            "Generating test split: 100 examples [00:00, 30305.66 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 8022.49 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4198.50 examples/s]\n",
            "Processing subjects:  14%| | 8/57 [00:33<03:40,  4.50s/it, college mathematics]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  48%|           | 48/100 [00:00<00:00, 471.33it/s]\u001b[A\n",
            "Formatting batches:  96%|| 96/100 [00:00<00:00, 473.80it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:03,  7.74it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:03,  7.40it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  7.41it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  7.40it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  7.29it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  7.25it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  7.21it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:02,  7.21it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.04it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:02,  7.15it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.27it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.45it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.44it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.49it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:02<00:01,  7.52it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.42it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.41it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  7.20it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  7.20it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.39it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  7.54it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.42it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:03<00:00,  7.47it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.50it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  7.52it/s]\u001b[A\n",
            "Processing subjects:  16%| | 9/57 [00:37<03:22,  4.22s/it, college mathematics]\u001b[A\n",
            "Generating test split: 173 examples [00:00, 37725.62 examples/s]\n",
            "\n",
            "Generating validation split: 22 examples [00:00, 13429.59 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4094.40 examples/s]\n",
            "Processing subjects:  16%|    | 9/57 [00:37<03:22,  4.22s/it, college medicine]\n",
            "Formatting batches:   0%|                               | 0/173 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  27%|               | 47/173 [00:00<00:00, 468.05it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 94/173 [00:00<00:00, 466.35it/s]\u001b[A\n",
            "Formatting batches:  83%|   | 143/173 [00:00<00:00, 473.12it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/44 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/44 [00:00<00:05,  8.48it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 2/44 [00:00<00:10,  4.16it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/44 [00:00<00:07,  5.54it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 4/44 [00:00<00:06,  6.40it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 5/44 [00:00<00:05,  7.25it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/44 [00:00<00:04,  7.89it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 7/44 [00:00<00:04,  8.25it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 8/44 [00:01<00:04,  8.03it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 10/44 [00:01<00:03,  8.79it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 11/44 [00:01<00:03,  8.55it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 12/44 [00:01<00:03,  8.70it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 13/44 [00:01<00:05,  5.81it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 15/44 [00:02<00:04,  6.93it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 16/44 [00:02<00:03,  7.42it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 17/44 [00:02<00:05,  5.39it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 18/44 [00:02<00:04,  6.06it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 19/44 [00:02<00:03,  6.55it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 20/44 [00:02<00:03,  7.10it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 21/44 [00:03<00:03,  7.31it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 22/44 [00:03<00:02,  7.85it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 23/44 [00:03<00:02,  7.90it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 24/44 [00:03<00:03,  5.37it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 25/44 [00:03<00:03,  6.11it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 27/44 [00:03<00:02,  7.39it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 28/44 [00:03<00:02,  7.83it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 29/44 [00:04<00:01,  7.96it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 30/44 [00:04<00:01,  8.26it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 31/44 [00:04<00:01,  8.50it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 32/44 [00:04<00:01,  8.66it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 33/44 [00:04<00:01,  8.52it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 34/44 [00:04<00:01,  8.79it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 35/44 [00:04<00:01,  8.99it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 36/44 [00:04<00:00,  9.00it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 38/44 [00:05<00:00,  9.26it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 39/44 [00:05<00:00,  9.23it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 40/44 [00:05<00:00,  9.18it/s]\u001b[A\n",
            "Predicting batches:  93%| | 41/44 [00:05<00:00,  5.99it/s]\u001b[A\n",
            "Predicting batches:  95%| | 42/44 [00:05<00:00,  6.62it/s]\u001b[A\n",
            "Predicting batches:  98%|| 43/44 [00:05<00:00,  7.24it/s]\u001b[A\n",
            "Processing subjects:  18%|   | 10/57 [00:43<03:47,  4.85s/it, college medicine]\u001b[A\n",
            "Generating test split: 102 examples [00:00, 29443.84 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7886.73 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4181.76 examples/s]\n",
            "Processing subjects:  18%|    | 10/57 [00:43<03:47,  4.85s/it, college physics]\n",
            "Formatting batches:   0%|                               | 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  48%|           | 49/102 [00:00<00:00, 485.11it/s]\u001b[A\n",
            "Formatting batches:  96%|| 98/102 [00:00<00:00, 487.68it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/26 [00:00<00:02,  8.77it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/26 [00:00<00:02,  9.02it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/26 [00:00<00:02,  9.11it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/26 [00:00<00:02,  8.81it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 5/26 [00:00<00:02,  8.61it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 6/26 [00:00<00:02,  8.77it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 7/26 [00:00<00:02,  8.89it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 8/26 [00:00<00:02,  8.71it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 9/26 [00:01<00:01,  8.81it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 10/26 [00:01<00:01,  8.91it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 11/26 [00:01<00:01,  8.94it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 12/26 [00:01<00:01,  8.98it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 13/26 [00:01<00:01,  9.00it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 14/26 [00:01<00:01,  9.01it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 15/26 [00:01<00:01,  9.01it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 16/26 [00:01<00:01,  9.02it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 17/26 [00:01<00:01,  8.81it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 18/26 [00:02<00:00,  8.87it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 19/26 [00:02<00:00,  8.71it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 20/26 [00:02<00:00,  8.80it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 21/26 [00:02<00:00,  8.88it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 22/26 [00:02<00:00,  8.95it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 23/26 [00:02<00:00,  9.00it/s]\u001b[A\n",
            "Predicting batches:  92%| | 24/26 [00:02<00:00,  9.02it/s]\u001b[A\n",
            "Predicting batches:  96%| | 25/26 [00:02<00:00,  8.81it/s]\u001b[A\n",
            "Processing subjects:  19%|    | 11/57 [00:46<03:18,  4.32s/it, college physics]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 30419.96 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7855.84 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4142.93 examples/s]\n",
            "Processing subjects:  19%|  | 11/57 [00:46<03:18,  4.32s/it, computer security]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 50/100 [00:00<00:00, 495.60it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  9.65it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  9.59it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  9.02it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:01, 10.27it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:01, 10.56it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:00<00:01, 10.56it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01, 10.91it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01, 10.92it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:00, 11.37it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:01<00:00, 10.98it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:01<00:00, 10.96it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:01<00:00, 11.13it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00, 11.39it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:02<00:00, 11.44it/s]\u001b[A\n",
            "Processing subjects:  21%|  | 12/57 [00:48<02:49,  3.76s/it, computer security]\u001b[A\n",
            "Generating test split: 235 examples [00:00, 46738.18 examples/s]\n",
            "\n",
            "Generating validation split: 26 examples [00:00, 16288.56 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4183.43 examples/s]\n",
            "Processing subjects:  21%| | 12/57 [00:48<02:49,  3.76s/it, conceptual physics]\n",
            "Formatting batches:   0%|                               | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  23%|                | 53/235 [00:00<00:00, 521.19it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 106/235 [00:00<00:00, 523.33it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 159/235 [00:00<00:00, 522.79it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 212/235 [00:00<00:00, 522.19it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/59 [00:00<00:03, 15.37it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/59 [00:00<00:03, 15.02it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/59 [00:00<00:03, 15.08it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/59 [00:00<00:03, 14.62it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/59 [00:00<00:03, 14.63it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/59 [00:00<00:03, 14.67it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 14/59 [00:00<00:03, 14.78it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/59 [00:01<00:02, 14.72it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 18/59 [00:01<00:02, 14.65it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 20/59 [00:01<00:02, 14.79it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/59 [00:01<00:02, 14.57it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 24/59 [00:01<00:02, 14.40it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 26/59 [00:01<00:02, 14.45it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/59 [00:01<00:02, 14.66it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 30/59 [00:02<00:01, 14.56it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 32/59 [00:02<00:01, 14.60it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 34/59 [00:02<00:01, 14.88it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 36/59 [00:02<00:01, 14.96it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 38/59 [00:02<00:01, 14.83it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 40/59 [00:02<00:01, 14.57it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 42/59 [00:02<00:01, 14.61it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 44/59 [00:02<00:01, 14.65it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 46/59 [00:03<00:00, 14.65it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 48/59 [00:03<00:00, 14.45it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 50/59 [00:03<00:00, 14.49it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 52/59 [00:03<00:00, 14.64it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 54/59 [00:03<00:00, 14.61it/s]\u001b[A\n",
            "Predicting batches:  95%| | 56/59 [00:03<00:00, 14.76it/s]\u001b[A\n",
            "Predicting batches:  98%|| 58/59 [00:03<00:00, 14.78it/s]\u001b[A\n",
            "Processing subjects:  23%| | 13/57 [00:53<02:54,  3.98s/it, conceptual physics]\u001b[A\n",
            "Generating test split: 114 examples [00:00, 32680.65 examples/s]\n",
            "\n",
            "Generating validation split: 12 examples [00:00, 8504.84 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4206.08 examples/s]\n",
            "Processing subjects:  23%|      | 13/57 [00:53<02:54,  3.98s/it, econometrics]\n",
            "Formatting batches:   0%|                               | 0/114 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  41%|            | 47/114 [00:00<00:00, 464.56it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 94/114 [00:00<00:00, 465.86it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/29 [00:00<00:03,  7.51it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/29 [00:00<00:03,  7.34it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 3/29 [00:00<00:03,  7.04it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/29 [00:00<00:03,  6.89it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 5/29 [00:00<00:03,  6.82it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 6/29 [00:00<00:03,  6.94it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 7/29 [00:00<00:03,  7.22it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 8/29 [00:01<00:03,  6.92it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 9/29 [00:01<00:02,  6.99it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 10/29 [00:01<00:02,  7.01it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 11/29 [00:01<00:02,  6.91it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 12/29 [00:01<00:02,  6.96it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 13/29 [00:01<00:02,  6.98it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 14/29 [00:02<00:02,  6.89it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 15/29 [00:02<00:02,  6.84it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 16/29 [00:02<00:01,  6.79it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 17/29 [00:02<00:01,  6.75it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 18/29 [00:02<00:01,  6.64it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 19/29 [00:02<00:01,  6.84it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 20/29 [00:02<00:01,  6.78it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 21/29 [00:03<00:01,  6.75it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 22/29 [00:03<00:01,  6.93it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 23/29 [00:03<00:00,  6.96it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 24/29 [00:03<00:00,  6.98it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 25/29 [00:03<00:00,  6.89it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 26/29 [00:03<00:00,  7.02it/s]\u001b[A\n",
            "Predicting batches:  93%| | 27/29 [00:03<00:00,  6.81it/s]\u001b[A\n",
            "Predicting batches:  97%|| 28/29 [00:04<00:00,  6.78it/s]\u001b[A\n",
            "Processing subjects:  25%|      | 14/57 [00:57<02:56,  4.10s/it, econometrics]\u001b[A\n",
            "Generating test split: 145 examples [00:00, 37045.38 examples/s]\n",
            "\n",
            "Generating validation split: 16 examples [00:00, 10655.58 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4000.67 examples/s]\n",
            "Processing subjects:  25%|| 14/57 [00:57<02:56,  4.10s/it, electrical engineeri\n",
            "Formatting batches:   0%|                               | 0/145 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 48/145 [00:00<00:00, 478.77it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 98/145 [00:00<00:00, 485.85it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/37 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/37 [00:00<00:03,  9.94it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/37 [00:00<00:03, 10.16it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 5/37 [00:00<00:03, 10.15it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 7/37 [00:00<00:02, 10.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/37 [00:00<00:02, 10.12it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 11/37 [00:01<00:02, 10.10it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 13/37 [00:01<00:02, 10.12it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 15/37 [00:01<00:02, 10.32it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 17/37 [00:01<00:01, 10.36it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 19/37 [00:01<00:01, 10.48it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 21/37 [00:02<00:01, 10.35it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 23/37 [00:02<00:01, 10.28it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 25/37 [00:02<00:01, 10.23it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 27/37 [00:02<00:00, 10.21it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 29/37 [00:02<00:00, 10.20it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 31/37 [00:03<00:00, 10.18it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 33/37 [00:03<00:00, 10.05it/s]\u001b[A\n",
            "Predicting batches:  95%| | 35/37 [00:03<00:00, 10.18it/s]\u001b[A\n",
            "Predicting batches: 100%|| 37/37 [00:03<00:00, 11.23it/s]\u001b[A\n",
            "Processing subjects:  26%|| 15/57 [01:01<02:49,  4.03s/it, electrical engineeri\u001b[A\n",
            "Generating test split: 378 examples [00:00, 50070.96 examples/s]\n",
            "\n",
            "Generating validation split: 41 examples [00:00, 22149.21 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4133.13 examples/s]\n",
            "Processing subjects:  26%|| 15/57 [01:01<02:49,  4.03s/it, elementary mathemati\n",
            "Formatting batches:   0%|                               | 0/378 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  13%|                  | 49/378 [00:00<00:00, 481.01it/s]\u001b[A\n",
            "Formatting batches:  26%|               | 98/378 [00:00<00:00, 476.41it/s]\u001b[A\n",
            "Formatting batches:  39%|            | 147/378 [00:00<00:00, 478.61it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 196/378 [00:00<00:00, 480.30it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 245/378 [00:00<00:00, 480.65it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 294/378 [00:00<00:00, 480.16it/s]\u001b[A\n",
            "Formatting batches:  91%| | 343/378 [00:00<00:00, 481.22it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/95 [00:00<00:09,  9.53it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/95 [00:00<00:10,  8.82it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 3/95 [00:00<00:10,  8.38it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 4/95 [00:00<00:10,  8.37it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 5/95 [00:00<00:10,  8.24it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 6/95 [00:00<00:10,  8.28it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 7/95 [00:00<00:10,  8.53it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 8/95 [00:00<00:10,  8.44it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 9/95 [00:01<00:10,  8.36it/s]\u001b[A\n",
            "Predicting batches:  11%|                    | 10/95 [00:01<00:10,  8.21it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 11/95 [00:01<00:10,  8.13it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 12/95 [00:01<00:10,  8.05it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 13/95 [00:01<00:10,  8.13it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 14/95 [00:01<00:10,  7.94it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 15/95 [00:01<00:10,  7.95it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 16/95 [00:01<00:09,  8.02it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 17/95 [00:02<00:09,  8.08it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 18/95 [00:02<00:09,  8.14it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 19/95 [00:02<00:09,  7.96it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 20/95 [00:02<00:09,  8.04it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 21/95 [00:02<00:08,  8.33it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 22/95 [00:02<00:08,  8.29it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 23/95 [00:02<00:08,  8.20it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 24/95 [00:02<00:08,  8.20it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 25/95 [00:03<00:08,  8.24it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 26/95 [00:03<00:08,  8.16it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 27/95 [00:03<00:08,  8.08it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 28/95 [00:03<00:08,  8.04it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 29/95 [00:03<00:08,  8.12it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 30/95 [00:03<00:07,  8.18it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 31/95 [00:03<00:07,  8.12it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 32/95 [00:03<00:07,  8.08it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 33/95 [00:04<00:07,  7.78it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 34/95 [00:04<00:07,  7.84it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 35/95 [00:04<00:07,  7.94it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 36/95 [00:04<00:07,  8.02it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 37/95 [00:04<00:07,  8.06it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 38/95 [00:04<00:07,  8.08it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 39/95 [00:04<00:06,  8.04it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 40/95 [00:04<00:06,  8.02it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 41/95 [00:05<00:06,  8.12it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 42/95 [00:05<00:06,  8.19it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 43/95 [00:05<00:06,  8.22it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 44/95 [00:05<00:06,  8.46it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 45/95 [00:05<00:05,  8.39it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 46/95 [00:05<00:05,  8.26it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 47/95 [00:05<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 48/95 [00:05<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 49/95 [00:06<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 50/95 [00:06<00:05,  8.12it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 51/95 [00:06<00:05,  8.18it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 52/95 [00:06<00:05,  8.08it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 53/95 [00:06<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 54/95 [00:06<00:04,  8.21it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 55/95 [00:06<00:04,  8.25it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 56/95 [00:06<00:04,  8.28it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 57/95 [00:06<00:04,  8.05it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 58/95 [00:07<00:04,  8.00it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 59/95 [00:07<00:04,  8.09it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 60/95 [00:07<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 61/95 [00:07<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 62/95 [00:07<00:04,  8.19it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 63/95 [00:07<00:04,  7.98it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 64/95 [00:07<00:03,  8.08it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 65/95 [00:07<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 66/95 [00:08<00:03,  8.11it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 67/95 [00:08<00:03,  8.02it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 68/95 [00:08<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 69/95 [00:08<00:03,  8.13it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 70/95 [00:08<00:03,  8.16it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 71/95 [00:08<00:02,  8.18it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 72/95 [00:08<00:02,  8.12it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 73/95 [00:08<00:02,  8.18it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 74/95 [00:09<00:02,  8.23it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 75/95 [00:09<00:02,  8.26it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 76/95 [00:09<00:02,  8.13it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 77/95 [00:09<00:02,  8.08it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 78/95 [00:09<00:02,  8.14it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 79/95 [00:09<00:01,  8.38it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 80/95 [00:09<00:01,  8.24it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 81/95 [00:09<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 82/95 [00:10<00:01,  7.91it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 83/95 [00:10<00:01,  7.90it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 84/95 [00:10<00:01,  8.02it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 85/95 [00:10<00:01,  8.10it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 86/95 [00:10<00:01,  8.17it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 87/95 [00:10<00:00,  8.07it/s]\u001b[A\n",
            "Predicting batches:  93%| | 88/95 [00:10<00:00,  8.33it/s]\u001b[A\n",
            "Predicting batches:  94%| | 89/95 [00:10<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  95%| | 90/95 [00:11<00:00,  8.29it/s]\u001b[A\n",
            "Predicting batches:  96%| | 91/95 [00:11<00:00,  8.26it/s]\u001b[A\n",
            "Predicting batches:  97%|| 92/95 [00:11<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  98%|| 93/95 [00:11<00:00,  8.19it/s]\u001b[A\n",
            "Predicting batches:  99%|| 94/95 [00:11<00:00,  8.21it/s]\u001b[A\n",
            "Processing subjects:  28%|| 16/57 [01:14<04:28,  6.55s/it, elementary mathemati\u001b[A\n",
            "Generating test split: 126 examples [00:00, 32273.73 examples/s]\n",
            "\n",
            "Generating validation split: 14 examples [00:00, 9232.74 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4084.04 examples/s]\n",
            "Processing subjects:  28%|     | 16/57 [01:14<04:28,  6.55s/it, formal logic]\n",
            "Formatting batches:   0%|                               | 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  37%|             | 46/126 [00:00<00:00, 458.32it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 93/126 [00:00<00:00, 460.79it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/32 [00:00<00:04,  6.65it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/32 [00:00<00:04,  6.90it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 3/32 [00:00<00:04,  7.08it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/32 [00:00<00:03,  7.06it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 5/32 [00:00<00:03,  7.32it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 6/32 [00:00<00:03,  7.32it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 7/32 [00:00<00:03,  7.38it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 8/32 [00:01<00:03,  7.30it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 9/32 [00:01<00:03,  7.30it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 10/32 [00:01<00:03,  7.31it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 11/32 [00:01<00:02,  7.11it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 12/32 [00:01<00:02,  6.87it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 13/32 [00:01<00:02,  6.61it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 14/32 [00:01<00:02,  6.93it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 15/32 [00:02<00:02,  6.99it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 16/32 [00:02<00:02,  6.87it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 17/32 [00:02<00:02,  7.05it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 18/32 [00:02<00:02,  6.81it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 19/32 [00:02<00:01,  7.00it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 20/32 [00:02<00:01,  7.00it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 21/32 [00:02<00:01,  6.91it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 22/32 [00:03<00:01,  6.81it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 23/32 [00:03<00:01,  6.87it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 24/32 [00:03<00:01,  6.66it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 25/32 [00:03<00:01,  6.77it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 26/32 [00:03<00:00,  6.63it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 27/32 [00:03<00:00,  6.87it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 28/32 [00:04<00:00,  6.99it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 29/32 [00:04<00:00,  7.02it/s]\u001b[A\n",
            "Predicting batches:  94%| | 30/32 [00:04<00:00,  7.24it/s]\u001b[A\n",
            "Predicting batches:  97%|| 31/32 [00:04<00:00,  7.26it/s]\u001b[A\n",
            "Processing subjects:  30%|     | 17/57 [01:18<04:00,  6.02s/it, formal logic]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 30815.55 examples/s]\n",
            "\n",
            "Generating validation split: 10 examples [00:00, 7302.06 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4123.38 examples/s]\n",
            "Processing subjects:  30%|     | 17/57 [01:18<04:00,  6.02s/it, global facts]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 49/100 [00:00<00:00, 484.31it/s]\u001b[A\n",
            "Formatting batches:  99%|| 99/100 [00:00<00:00, 490.61it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:01, 11.64it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:01, 11.53it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 11.48it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 11.24it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 11.12it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01, 11.05it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01, 10.99it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 10.68it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 10.73it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 10.92it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00, 10.68it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:02<00:00, 10.90it/s]\u001b[A\n",
            "Processing subjects:  32%|     | 18/57 [01:21<03:13,  4.96s/it, global facts]\u001b[A\n",
            "Generating test split: 310 examples [00:00, 46478.44 examples/s]\n",
            "\n",
            "Generating validation split: 32 examples [00:00, 18040.02 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4156.89 examples/s]\n",
            "Processing subjects:  32%|| 18/57 [01:21<03:13,  4.96s/it, high school biology]\n",
            "Formatting batches:   0%|                               | 0/310 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 47/310 [00:00<00:00, 465.87it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 94/310 [00:00<00:00, 467.04it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 141/310 [00:00<00:00, 467.98it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 189/310 [00:00<00:00, 470.21it/s]\u001b[A\n",
            "Formatting batches:  76%|    | 237/310 [00:00<00:00, 472.19it/s]\u001b[A\n",
            "Formatting batches:  92%| | 285/310 [00:00<00:00, 472.93it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/78 [00:00<00:08,  9.53it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/78 [00:00<00:08,  9.33it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/78 [00:00<00:08,  8.62it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/78 [00:00<00:08,  8.77it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/78 [00:00<00:08,  8.45it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/78 [00:00<00:08,  8.40it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 7/78 [00:00<00:08,  8.38it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/78 [00:00<00:08,  8.37it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 9/78 [00:01<00:08,  8.57it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/78 [00:01<00:07,  8.69it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 11/78 [00:01<00:07,  8.40it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/78 [00:01<00:07,  8.26it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 13/78 [00:01<00:07,  8.47it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/78 [00:01<00:07,  8.62it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/78 [00:01<00:07,  8.41it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/78 [00:01<00:07,  8.38it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 17/78 [00:01<00:07,  8.56it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/78 [00:02<00:07,  8.22it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 19/78 [00:02<00:07,  8.21it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/78 [00:02<00:06,  8.43it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 21/78 [00:02<00:06,  8.35it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 22/78 [00:02<00:06,  8.33it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 23/78 [00:02<00:06,  8.51it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 24/78 [00:02<00:06,  8.64it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 25/78 [00:02<00:06,  8.73it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 26/78 [00:03<00:06,  8.32it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 27/78 [00:03<00:06,  8.17it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/78 [00:03<00:06,  8.06it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 29/78 [00:03<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 30/78 [00:03<00:05,  8.27it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 31/78 [00:03<00:05,  8.27it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 32/78 [00:03<00:05,  8.46it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 33/78 [00:03<00:05,  8.40it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 34/78 [00:04<00:05,  8.33it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 35/78 [00:04<00:05,  8.28it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 36/78 [00:04<00:04,  8.47it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 37/78 [00:04<00:05,  8.08it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/78 [00:04<00:04,  8.10it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 39/78 [00:04<00:04,  8.34it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 40/78 [00:04<00:04,  8.21it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 41/78 [00:04<00:04,  8.19it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 42/78 [00:05<00:04,  8.22it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 43/78 [00:05<00:04,  8.23it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 44/78 [00:05<00:04,  8.25it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 45/78 [00:05<00:04,  8.23it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 46/78 [00:05<00:04,  7.99it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 47/78 [00:05<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/78 [00:05<00:03,  8.10it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 49/78 [00:05<00:03,  8.36it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 50/78 [00:05<00:03,  8.56it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 51/78 [00:06<00:03,  8.33it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 52/78 [00:06<00:03,  8.20it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 53/78 [00:06<00:03,  8.09it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 54/78 [00:06<00:02,  8.01it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 55/78 [00:06<00:02,  8.08it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 56/78 [00:06<00:02,  8.14it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 57/78 [00:06<00:02,  8.17it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 58/78 [00:06<00:02,  8.40it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 59/78 [00:07<00:02,  8.35it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 60/78 [00:07<00:02,  8.53it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 61/78 [00:07<00:02,  8.39it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 62/78 [00:07<00:01,  8.23it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 63/78 [00:07<00:01,  8.12it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 64/78 [00:07<00:01,  8.17it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 65/78 [00:07<00:01,  8.05it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 66/78 [00:07<00:01,  8.29it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 67/78 [00:08<00:01,  8.15it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 68/78 [00:08<00:01,  8.08it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 69/78 [00:08<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 70/78 [00:08<00:01,  7.86it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 71/78 [00:08<00:00,  8.16it/s]\u001b[A\n",
            "Predicting batches:  92%| | 72/78 [00:08<00:00,  8.38it/s]\u001b[A\n",
            "Predicting batches:  94%| | 73/78 [00:08<00:00,  8.34it/s]\u001b[A\n",
            "Predicting batches:  95%| | 74/78 [00:08<00:00,  8.27it/s]\u001b[A\n",
            "Predicting batches:  96%| | 75/78 [00:09<00:00,  8.57it/s]\u001b[A\n",
            "Predicting batches:  97%|| 76/78 [00:09<00:00,  8.46it/s]\u001b[A\n",
            "Predicting batches:  99%|| 77/78 [00:09<00:00,  8.40it/s]\u001b[A\n",
            "Processing subjects:  33%|| 19/57 [01:31<04:06,  6.47s/it, high school biology]\u001b[A\n",
            "Generating test split: 203 examples [00:00, 40866.03 examples/s]\n",
            "\n",
            "Generating validation split: 22 examples [00:00, 13449.16 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4107.23 examples/s]\n",
            "Processing subjects:  33%|| 19/57 [01:31<04:06,  6.47s/it, high school chemistr\n",
            "Formatting batches:   0%|                               | 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  24%|                | 48/203 [00:00<00:00, 478.99it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 96/203 [00:00<00:00, 476.01it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 145/203 [00:00<00:00, 478.63it/s]\u001b[A\n",
            "Formatting batches:  96%| | 194/203 [00:00<00:00, 483.02it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/51 [00:00<00:05,  9.62it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:00<00:05,  9.29it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/51 [00:00<00:05,  9.16it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:00<00:05,  8.72it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:00<00:05,  8.97it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:00<00:04,  9.11it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:00<00:04,  9.27it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:00<00:04,  8.88it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:01<00:04,  8.90it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:01<00:04,  9.03it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:01<00:04,  8.99it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:01<00:04,  8.99it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:01<00:04,  9.10it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:01<00:04,  8.71it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:01<00:04,  8.57it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:01<00:03,  8.80it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:02<00:03,  8.47it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:02<00:03,  8.40it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:02<00:03,  8.23it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:02<00:03,  7.98it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:02<00:03,  7.96it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:02<00:03,  8.04it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:02<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:02<00:03,  8.39it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:03<00:02,  8.34it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:03<00:02,  8.62it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:03<00:02,  8.38it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:03<00:02,  8.19it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:03<00:02,  8.41it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:03<00:02,  8.68it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:03<00:02,  8.89it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:03<00:02,  8.53it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:03<00:02,  8.30it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:04<00:01,  8.60it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:04<00:01,  8.48it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:04<00:01,  8.25it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 38/51 [00:04<00:01,  8.20it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:04<00:01,  8.11it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:04<00:01,  8.05it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:04<00:01,  8.32it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:04<00:01,  8.17it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:05<00:00,  8.38it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:05<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:05<00:00,  8.47it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:05<00:00,  8.63it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:05<00:00,  8.75it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:05<00:00,  8.81it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:05<00:00,  8.85it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:05<00:00,  8.65it/s]\u001b[A\n",
            "Processing subjects:  35%|| 20/57 [01:37<03:58,  6.44s/it, high school chemistr\u001b[A\n",
            "Generating test split: 100 examples [00:00, 28542.39 examples/s]\n",
            "\n",
            "Generating validation split: 9 examples [00:00, 6320.95 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4036.87 examples/s]\n",
            "Processing subjects:  35%|| 20/57 [01:37<03:58,  6.44s/it, high school computer\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 40/100 [00:00<00:00, 398.22it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 82/100 [00:00<00:00, 408.12it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:05,  4.46it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:05,  4.58it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:04,  4.75it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:04,  4.58it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:01<00:04,  4.60it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:01<00:04,  4.68it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:01<00:03,  4.72it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:03,  4.31it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:03,  4.36it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:02<00:03,  4.53it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:02<00:03,  4.56it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:02<00:02,  4.75it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:02<00:02,  4.78it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:03<00:02,  4.67it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:03<00:02,  4.52it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:03<00:01,  4.66it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:03<00:01,  4.55it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:03<00:01,  4.56it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:04<00:01,  4.51it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:04<00:01,  4.54it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:04<00:00,  4.51it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:04<00:00,  4.71it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:04<00:00,  4.75it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:05<00:00,  4.81it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:05<00:00,  4.82it/s]\u001b[A\n",
            "Processing subjects:  37%|| 21/57 [01:43<03:43,  6.21s/it, high school computer\u001b[A\n",
            "Generating test split: 165 examples [00:00, 29718.73 examples/s]\n",
            "\n",
            "Generating validation split: 18 examples [00:00, 11029.58 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3939.79 examples/s]\n",
            "Processing subjects:  37%|| 21/57 [01:43<03:43,  6.21s/it, high school european\n",
            "Formatting batches:   0%|                               | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 25/165 [00:00<00:00, 246.94it/s]\u001b[A\n",
            "Formatting batches:  31%|              | 51/165 [00:00<00:00, 253.29it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 77/165 [00:00<00:00, 252.51it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 103/165 [00:00<00:00, 249.93it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 128/165 [00:00<00:00, 246.39it/s]\u001b[A\n",
            "Formatting batches:  93%| | 154/165 [00:00<00:00, 249.82it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/42 [00:00<00:34,  1.18it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/42 [00:01<00:33,  1.18it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/42 [00:02<00:32,  1.20it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/42 [00:03<00:31,  1.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 5/42 [00:04<00:31,  1.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/42 [00:05<00:29,  1.20it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 7/42 [00:05<00:28,  1.22it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/42 [00:06<00:28,  1.20it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 9/42 [00:07<00:28,  1.17it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/42 [00:08<00:27,  1.18it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 11/42 [00:09<00:26,  1.19it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/42 [00:10<00:25,  1.18it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 13/42 [00:10<00:23,  1.21it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/42 [00:11<00:22,  1.23it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 15/42 [00:12<00:22,  1.22it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 16/42 [00:13<00:20,  1.24it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 17/42 [00:14<00:20,  1.22it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 18/42 [00:14<00:20,  1.20it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 19/42 [00:15<00:19,  1.19it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 20/42 [00:16<00:18,  1.19it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 21/42 [00:17<00:17,  1.20it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 22/42 [00:18<00:16,  1.22it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 23/42 [00:19<00:15,  1.23it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 24/42 [00:19<00:14,  1.21it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 25/42 [00:20<00:14,  1.19it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 26/42 [00:21<00:13,  1.19it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 27/42 [00:22<00:12,  1.18it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 28/42 [00:23<00:11,  1.17it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 29/42 [00:24<00:10,  1.19it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 30/42 [00:25<00:09,  1.20it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 31/42 [00:25<00:09,  1.20it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 32/42 [00:26<00:08,  1.19it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 33/42 [00:27<00:07,  1.21it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 34/42 [00:28<00:06,  1.21it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 35/42 [00:29<00:05,  1.19it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 36/42 [00:30<00:05,  1.18it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 37/42 [00:30<00:04,  1.19it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 38/42 [00:31<00:03,  1.19it/s]\u001b[A\n",
            "Predicting batches:  93%| | 39/42 [00:32<00:02,  1.20it/s]\u001b[A\n",
            "Predicting batches:  95%| | 40/42 [00:33<00:01,  1.20it/s]\u001b[A\n",
            "Predicting batches:  98%|| 41/42 [00:34<00:00,  1.23it/s]\u001b[A\n",
            "Predicting batches: 100%|| 42/42 [00:34<00:00,  1.62it/s]\u001b[A\n",
            "Processing subjects:  39%|| 22/57 [02:18<08:39, 14.84s/it, high school european\u001b[A\n",
            "Generating test split: 198 examples [00:00, 41308.80 examples/s]\n",
            "\n",
            "Generating validation split: 22 examples [00:00, 13832.21 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4096.80 examples/s]\n",
            "Processing subjects:  39%|| 22/57 [02:18<08:39, 14.84s/it, high school geograph\n",
            "Formatting batches:   0%|                               | 0/198 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  25%|               | 49/198 [00:00<00:00, 486.99it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 99/198 [00:00<00:00, 492.50it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 149/198 [00:00<00:00, 492.27it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/50 [00:00<00:03, 12.58it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/50 [00:00<00:03, 12.31it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/50 [00:00<00:03, 12.06it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/50 [00:00<00:03, 12.13it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/50 [00:00<00:03, 11.80it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/50 [00:01<00:03, 11.88it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 14/50 [00:01<00:03, 11.95it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 16/50 [00:01<00:02, 11.93it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 18/50 [00:01<00:02, 11.99it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 20/50 [00:01<00:02, 11.96it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 22/50 [00:01<00:02, 11.92it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 24/50 [00:02<00:02, 11.56it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 26/50 [00:02<00:02, 11.66it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 28/50 [00:02<00:01, 11.74it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 30/50 [00:02<00:01, 11.80it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 32/50 [00:02<00:01, 11.83it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 34/50 [00:02<00:01, 11.83it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 36/50 [00:03<00:01, 11.93it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 38/50 [00:03<00:00, 12.01it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 40/50 [00:03<00:00, 11.78it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 42/50 [00:03<00:00, 11.79it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 44/50 [00:03<00:00, 11.92it/s]\u001b[A\n",
            "Predicting batches:  92%| | 46/50 [00:03<00:00, 11.88it/s]\u001b[A\n",
            "Predicting batches:  96%| | 48/50 [00:04<00:00, 11.68it/s]\u001b[A\n",
            "Predicting batches: 100%|| 50/50 [00:04<00:00, 12.42it/s]\u001b[A\n",
            "Processing subjects:  40%|| 23/57 [02:22<06:40, 11.77s/it, high school geograph\u001b[A\n",
            "Generating test split: 193 examples [00:00, 41679.57 examples/s]\n",
            "\n",
            "Generating validation split: 21 examples [00:00, 13702.61 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4023.70 examples/s]\n",
            "Processing subjects:  40%|| 23/57 [02:22<06:40, 11.77s/it, high school governme\n",
            "Formatting batches:   0%|                               | 0/193 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  24%|                | 47/193 [00:00<00:00, 464.73it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 95/193 [00:00<00:00, 472.13it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 143/193 [00:00<00:00, 471.18it/s]\u001b[A\n",
            "Formatting batches:  99%|| 191/193 [00:00<00:00, 473.46it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/49 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/49 [00:00<00:05,  9.52it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/49 [00:00<00:04,  9.51it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/49 [00:00<00:04,  9.45it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/49 [00:00<00:04,  9.41it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/49 [00:00<00:04,  9.38it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/49 [00:00<00:04,  9.38it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/49 [00:00<00:04,  9.55it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/49 [00:00<00:04,  9.49it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/49 [00:00<00:04,  9.45it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/49 [00:01<00:04,  9.28it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 11/49 [00:01<00:04,  9.15it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/49 [00:01<00:04,  9.21it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 13/49 [00:01<00:03,  9.23it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 14/49 [00:01<00:03,  9.25it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 15/49 [00:01<00:03,  9.16it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 16/49 [00:01<00:03,  9.20it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 17/49 [00:01<00:03,  9.42it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 18/49 [00:01<00:03,  9.39it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 19/49 [00:02<00:03,  9.37it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 20/49 [00:02<00:03,  9.36it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 21/49 [00:02<00:03,  9.33it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 22/49 [00:02<00:02,  9.32it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 23/49 [00:02<00:02,  9.31it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 24/49 [00:02<00:02,  9.31it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 25/49 [00:02<00:02,  9.31it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 26/49 [00:02<00:02,  9.21it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 27/49 [00:02<00:02,  9.23it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 28/49 [00:03<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 29/49 [00:03<00:02,  9.28it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 30/49 [00:03<00:02,  9.29it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 31/49 [00:03<00:01,  9.31it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 32/49 [00:03<00:01,  9.31it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 33/49 [00:03<00:01,  9.47it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 34/49 [00:03<00:01,  9.42it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 35/49 [00:03<00:01,  9.38it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 36/49 [00:03<00:01,  9.37it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 37/49 [00:03<00:01,  9.37it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 38/49 [00:04<00:01,  9.34it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 39/49 [00:04<00:01,  9.33it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 40/49 [00:04<00:01,  8.96it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 41/49 [00:04<00:00,  8.96it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 42/49 [00:04<00:00,  8.67it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 43/49 [00:04<00:00,  8.85it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 44/49 [00:04<00:00,  8.98it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 45/49 [00:04<00:00,  9.08it/s]\u001b[A\n",
            "Predicting batches:  94%| | 46/49 [00:04<00:00,  9.15it/s]\u001b[A\n",
            "Predicting batches:  96%| | 47/49 [00:05<00:00,  9.10it/s]\u001b[A\n",
            "Predicting batches:  98%|| 48/49 [00:05<00:00,  9.07it/s]\u001b[A\n",
            "Processing subjects:  42%|| 24/57 [02:28<05:27,  9.93s/it, high school governme\u001b[A\n",
            "Generating test split: 390 examples [00:00, 50993.78 examples/s]\n",
            "\n",
            "Generating validation split: 43 examples [00:00, 21391.90 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3942.02 examples/s]\n",
            "Processing subjects:  42%|| 24/57 [02:28<05:27,  9.93s/it, high school macroeco\n",
            "Formatting batches:   0%|                               | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  13%|                  | 50/390 [00:00<00:00, 490.77it/s]\u001b[A\n",
            "Formatting batches:  26%|              | 100/390 [00:00<00:00, 494.24it/s]\u001b[A\n",
            "Formatting batches:  38%|            | 150/390 [00:00<00:00, 492.80it/s]\u001b[A\n",
            "Formatting batches:  51%|         | 200/390 [00:00<00:00, 494.02it/s]\u001b[A\n",
            "Formatting batches:  64%|       | 250/390 [00:00<00:00, 491.94it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 301/390 [00:00<00:00, 495.52it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 351/390 [00:00<00:00, 492.05it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/98 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/98 [00:00<00:07, 12.10it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 4/98 [00:00<00:07, 12.17it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 6/98 [00:00<00:07, 11.54it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 8/98 [00:00<00:07, 11.69it/s]\u001b[A\n",
            "Predicting batches:  10%|                    | 10/98 [00:00<00:07, 11.74it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 12/98 [00:01<00:07, 11.79it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 14/98 [00:01<00:07, 11.48it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 16/98 [00:01<00:07, 11.62it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 18/98 [00:01<00:06, 11.71it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 20/98 [00:01<00:06, 11.62it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 22/98 [00:01<00:06, 11.68it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 24/98 [00:02<00:06, 11.74it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 26/98 [00:02<00:06, 11.44it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 28/98 [00:02<00:05, 11.68it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 30/98 [00:02<00:05, 11.83it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 32/98 [00:02<00:05, 11.77it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 34/98 [00:02<00:05, 11.47it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 36/98 [00:03<00:05, 11.56it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 38/98 [00:03<00:05, 11.32it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 40/98 [00:03<00:05, 11.27it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 42/98 [00:03<00:04, 11.43it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 44/98 [00:03<00:04, 11.73it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 46/98 [00:03<00:04, 11.57it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 48/98 [00:04<00:04, 11.47it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 50/98 [00:04<00:04, 11.59it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 52/98 [00:04<00:04, 11.46it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 54/98 [00:04<00:03, 11.58it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 56/98 [00:04<00:03, 11.66it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 58/98 [00:04<00:03, 11.81it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 60/98 [00:05<00:03, 11.82it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 62/98 [00:05<00:03, 11.64it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 64/98 [00:05<00:02, 11.74it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 66/98 [00:05<00:02, 11.68it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 68/98 [00:05<00:02, 11.72it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 70/98 [00:06<00:02, 11.43it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 72/98 [00:06<00:02, 11.39it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 74/98 [00:06<00:02, 11.58it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 76/98 [00:06<00:01, 11.83it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 78/98 [00:06<00:01, 11.84it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 80/98 [00:06<00:01, 11.85it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 82/98 [00:07<00:01, 11.66it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 84/98 [00:07<00:01, 11.67it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 86/98 [00:07<00:01, 11.41it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 88/98 [00:07<00:00, 11.44it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 90/98 [00:07<00:00, 11.54it/s]\u001b[A\n",
            "Predicting batches:  94%| | 92/98 [00:07<00:00, 11.64it/s]\u001b[A\n",
            "Predicting batches:  96%| | 94/98 [00:08<00:00, 11.69it/s]\u001b[A\n",
            "Predicting batches:  98%|| 96/98 [00:08<00:00, 11.70it/s]\u001b[A\n",
            "Predicting batches: 100%|| 98/98 [00:08<00:00, 12.64it/s]\u001b[A\n",
            "Processing subjects:  44%|| 25/57 [02:37<05:10,  9.71s/it, high school macroeco\u001b[A\n",
            "Generating test split: 270 examples [00:00, 46943.38 examples/s]\n",
            "\n",
            "Generating validation split: 29 examples [00:00, 17136.49 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4065.04 examples/s]\n",
            "Processing subjects:  44%|| 25/57 [02:37<05:10,  9.71s/it, high school mathemat\n",
            "Formatting batches:   0%|                               | 0/270 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 47/270 [00:00<00:00, 461.43it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 94/270 [00:00<00:00, 465.89it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 141/270 [00:00<00:00, 467.02it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 189/270 [00:00<00:00, 468.63it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 236/270 [00:00<00:00, 468.86it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/68 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/68 [00:00<00:07,  8.39it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/68 [00:00<00:08,  7.72it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/68 [00:00<00:08,  7.94it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/68 [00:00<00:08,  7.92it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/68 [00:00<00:07,  8.01it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 6/68 [00:00<00:07,  8.28it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/68 [00:00<00:07,  8.21it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/68 [00:00<00:07,  8.19it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 9/68 [00:01<00:07,  8.15it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/68 [00:01<00:07,  8.13it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 11/68 [00:01<00:07,  7.90it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/68 [00:01<00:07,  7.89it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 13/68 [00:01<00:07,  7.75it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/68 [00:01<00:06,  7.76it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 15/68 [00:01<00:06,  8.07it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/68 [00:01<00:06,  7.97it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 17/68 [00:02<00:06,  8.00it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 18/68 [00:02<00:06,  8.03it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 19/68 [00:02<00:06,  8.06it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 20/68 [00:02<00:06,  7.97it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 21/68 [00:02<00:05,  8.21it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 22/68 [00:02<00:05,  8.09it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 23/68 [00:02<00:05,  8.02it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 24/68 [00:02<00:05,  7.96it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 25/68 [00:03<00:05,  8.21it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 26/68 [00:03<00:05,  8.38it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 27/68 [00:03<00:04,  8.32it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 28/68 [00:03<00:04,  8.25it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 29/68 [00:03<00:04,  8.23it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 30/68 [00:03<00:04,  8.10it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 31/68 [00:03<00:04,  8.10it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 32/68 [00:03<00:04,  8.03it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 33/68 [00:04<00:04,  8.08it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 34/68 [00:04<00:04,  7.97it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 35/68 [00:04<00:04,  8.00it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 36/68 [00:04<00:04,  7.69it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 37/68 [00:04<00:03,  7.83it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 38/68 [00:04<00:03,  7.94it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 39/68 [00:04<00:03,  8.21it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 40/68 [00:04<00:03,  8.18it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 41/68 [00:05<00:03,  8.38it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 42/68 [00:05<00:03,  8.18it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 43/68 [00:05<00:03,  8.17it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 44/68 [00:05<00:02,  8.07it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 45/68 [00:05<00:02,  8.07it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 46/68 [00:05<00:02,  8.10it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 47/68 [00:05<00:02,  8.09it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 48/68 [00:05<00:02,  8.07it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 49/68 [00:06<00:02,  7.99it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 50/68 [00:06<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 51/68 [00:06<00:02,  8.08it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 52/68 [00:06<00:01,  8.08it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 53/68 [00:06<00:01,  8.01it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 54/68 [00:06<00:01,  8.06it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 55/68 [00:06<00:01,  8.06it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 56/68 [00:06<00:01,  7.98it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 57/68 [00:07<00:01,  7.93it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 58/68 [00:07<00:01,  8.01it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 59/68 [00:07<00:01,  7.93it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 60/68 [00:07<00:01,  7.91it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 61/68 [00:07<00:00,  7.99it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 62/68 [00:07<00:00,  7.82it/s]\u001b[A\n",
            "Predicting batches:  93%| | 63/68 [00:07<00:00,  7.93it/s]\u001b[A\n",
            "Predicting batches:  94%| | 64/68 [00:07<00:00,  7.91it/s]\u001b[A\n",
            "Predicting batches:  96%| | 65/68 [00:08<00:00,  8.00it/s]\u001b[A\n",
            "Predicting batches:  97%|| 66/68 [00:08<00:00,  8.06it/s]\u001b[A\n",
            "Predicting batches:  99%|| 67/68 [00:08<00:00,  7.71it/s]\u001b[A\n",
            "Processing subjects:  46%|| 26/57 [02:46<04:54,  9.50s/it, high school mathemat\u001b[A\n",
            "Generating test split: 238 examples [00:00, 44117.40 examples/s]\n",
            "\n",
            "Generating validation split: 26 examples [00:00, 16120.02 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4072.14 examples/s]\n",
            "Processing subjects:  46%|| 26/57 [02:46<04:54,  9.50s/it, high school microeco\n",
            "Formatting batches:   0%|                               | 0/238 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  21%|                | 50/238 [00:00<00:00, 492.65it/s]\u001b[A\n",
            "Formatting batches:  42%|           | 101/238 [00:00<00:00, 497.55it/s]\u001b[A\n",
            "Formatting batches:  63%|       | 151/238 [00:00<00:00, 498.19it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 201/238 [00:00<00:00, 498.76it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/60 [00:00<00:05,  9.88it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/60 [00:00<00:05, 10.59it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/60 [00:00<00:05, 10.30it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 7/60 [00:00<00:05, 10.52it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/60 [00:00<00:04, 10.31it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 11/60 [00:01<00:04, 10.49it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 13/60 [00:01<00:04, 10.73it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 15/60 [00:01<00:04, 10.49it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 17/60 [00:01<00:03, 10.85it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 19/60 [00:01<00:03, 10.97it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 21/60 [00:01<00:03, 10.78it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 23/60 [00:02<00:03, 11.06it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 25/60 [00:02<00:03, 10.97it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 27/60 [00:02<00:02, 11.05it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 29/60 [00:02<00:02, 10.96it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 31/60 [00:02<00:02, 11.03it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 33/60 [00:03<00:02, 10.81it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 35/60 [00:03<00:02, 10.79it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 37/60 [00:03<00:02, 10.63it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 39/60 [00:03<00:01, 10.67it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 41/60 [00:03<00:01, 10.68it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 43/60 [00:03<00:01, 10.85it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 45/60 [00:04<00:01, 10.83it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 47/60 [00:04<00:01, 10.57it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 49/60 [00:04<00:01, 10.62it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 51/60 [00:04<00:00, 10.69it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 53/60 [00:04<00:00, 10.84it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 55/60 [00:05<00:00, 10.93it/s]\u001b[A\n",
            "Predicting batches:  95%| | 57/60 [00:05<00:00, 10.63it/s]\u001b[A\n",
            "Predicting batches:  98%|| 59/60 [00:05<00:00, 10.40it/s]\u001b[A\n",
            "Processing subjects:  47%|| 27/57 [02:52<04:14,  8.47s/it, high school microeco\u001b[A\n",
            "Generating test split: 151 examples [00:00, 33794.35 examples/s]\n",
            "\n",
            "Generating validation split: 17 examples [00:00, 10666.14 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4127.44 examples/s]\n",
            "Processing subjects:  47%|| 27/57 [02:52<04:14,  8.47s/it, high school physics]\n",
            "Formatting batches:   0%|                               | 0/151 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 45/151 [00:00<00:00, 441.65it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 92/151 [00:00<00:00, 454.11it/s]\u001b[A\n",
            "Formatting batches:  92%| | 139/151 [00:00<00:00, 460.54it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/38 [00:00<00:04,  7.58it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/38 [00:00<00:05,  6.76it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/38 [00:00<00:04,  7.06it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/38 [00:00<00:04,  7.06it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 5/38 [00:00<00:04,  7.19it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/38 [00:00<00:04,  7.40it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 7/38 [00:00<00:04,  7.41it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 8/38 [00:01<00:04,  7.42it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/38 [00:01<00:03,  7.55it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 10/38 [00:01<00:03,  7.60it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 11/38 [00:01<00:03,  7.54it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/38 [00:01<00:03,  7.69it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 13/38 [00:01<00:03,  7.22it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 14/38 [00:01<00:03,  7.46it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 15/38 [00:02<00:03,  7.45it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 16/38 [00:02<00:02,  7.63it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 17/38 [00:02<00:02,  7.67it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 18/38 [00:02<00:02,  7.46it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 19/38 [00:02<00:02,  7.57it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 20/38 [00:02<00:02,  7.41it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 21/38 [00:02<00:02,  7.54it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 22/38 [00:02<00:02,  7.63it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 23/38 [00:03<00:01,  7.67it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 24/38 [00:03<00:01,  7.70it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 25/38 [00:03<00:01,  7.70it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 26/38 [00:03<00:01,  7.61it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 27/38 [00:03<00:01,  7.65it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 28/38 [00:03<00:01,  7.58it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 29/38 [00:03<00:01,  7.41it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 30/38 [00:04<00:01,  7.13it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 31/38 [00:04<00:00,  7.39it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 32/38 [00:04<00:00,  7.25it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 33/38 [00:04<00:00,  7.41it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 34/38 [00:04<00:00,  7.62it/s]\u001b[A\n",
            "Predicting batches:  92%| | 35/38 [00:04<00:00,  7.68it/s]\u001b[A\n",
            "Predicting batches:  95%| | 36/38 [00:04<00:00,  7.60it/s]\u001b[A\n",
            "Predicting batches:  97%|| 37/38 [00:04<00:00,  7.65it/s]\u001b[A\n",
            "Predicting batches: 100%|| 38/38 [00:05<00:00,  8.18it/s]\u001b[A\n",
            "Processing subjects:  49%|| 28/57 [02:58<03:38,  7.54s/it, high school physics]\u001b[A\n",
            "Generating test split: 545 examples [00:00, 54716.61 examples/s]\n",
            "\n",
            "Generating validation split: 60 examples [00:00, 26840.68 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4017.53 examples/s]\n",
            "Processing subjects:  49%|| 28/57 [02:58<03:38,  7.54s/it, high school psycholo\n",
            "Formatting batches:   0%|                               | 0/545 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   8%|                   | 45/545 [00:00<00:01, 447.19it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 93/545 [00:00<00:00, 463.82it/s]\u001b[A\n",
            "Formatting batches:  26%|              | 140/545 [00:00<00:00, 465.76it/s]\u001b[A\n",
            "Formatting batches:  34%|             | 188/545 [00:00<00:00, 467.81it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 236/545 [00:00<00:00, 468.83it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 284/545 [00:00<00:00, 469.78it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 332/545 [00:00<00:00, 469.92it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 380/545 [00:00<00:00, 470.40it/s]\u001b[A\n",
            "Formatting batches:  79%|    | 428/545 [00:00<00:00, 471.09it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 476/545 [00:01<00:00, 471.72it/s]\u001b[A\n",
            "Formatting batches:  96%|| 524/545 [00:01<00:00, 471.35it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/137 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 1/137 [00:00<00:13,  9.89it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/137 [00:00<00:15,  8.85it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 3/137 [00:00<00:14,  8.96it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 4/137 [00:00<00:14,  8.95it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 5/137 [00:00<00:14,  8.95it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 6/137 [00:00<00:14,  8.99it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 7/137 [00:00<00:14,  9.12it/s]\u001b[A\n",
            "Predicting batches:   6%|                     | 8/137 [00:00<00:14,  8.64it/s]\u001b[A\n",
            "Predicting batches:   7%|                     | 9/137 [00:01<00:15,  8.50it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 10/137 [00:01<00:16,  7.79it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 11/137 [00:01<00:15,  8.19it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 12/137 [00:01<00:14,  8.40it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 13/137 [00:01<00:14,  8.33it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 14/137 [00:01<00:14,  8.51it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 15/137 [00:01<00:15,  7.98it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 16/137 [00:01<00:14,  8.22it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 17/137 [00:01<00:14,  8.51it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 18/137 [00:02<00:13,  8.60it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 19/137 [00:02<00:13,  8.80it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 20/137 [00:02<00:13,  8.57it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 21/137 [00:02<00:13,  8.67it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 22/137 [00:02<00:13,  8.73it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 23/137 [00:02<00:12,  8.80it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 24/137 [00:02<00:12,  8.83it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 25/137 [00:02<00:12,  8.97it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 26/137 [00:03<00:12,  8.93it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 27/137 [00:03<00:12,  9.04it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 28/137 [00:03<00:12,  8.70it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 29/137 [00:03<00:12,  8.85it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 30/137 [00:03<00:12,  8.86it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 31/137 [00:03<00:12,  8.52it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 32/137 [00:03<00:12,  8.63it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 33/137 [00:03<00:12,  8.45it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 34/137 [00:03<00:12,  8.58it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 35/137 [00:04<00:11,  8.65it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 36/137 [00:04<00:11,  8.74it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 37/137 [00:04<00:11,  8.77it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 38/137 [00:04<00:11,  8.92it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 39/137 [00:04<00:10,  9.02it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 40/137 [00:04<00:10,  8.97it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 41/137 [00:04<00:10,  9.06it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 42/137 [00:04<00:11,  8.63it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 43/137 [00:04<00:10,  8.73it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 44/137 [00:05<00:10,  8.77it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 45/137 [00:05<00:10,  8.81it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 46/137 [00:05<00:10,  8.94it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 47/137 [00:05<00:10,  8.70it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 48/137 [00:05<00:10,  8.75it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 49/137 [00:05<00:10,  8.78it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 50/137 [00:05<00:09,  8.93it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 51/137 [00:05<00:09,  9.02it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 52/137 [00:05<00:09,  8.75it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 53/137 [00:06<00:09,  8.53it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 54/137 [00:06<00:09,  8.62it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 55/137 [00:06<00:09,  8.47it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 56/137 [00:06<00:09,  8.59it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 57/137 [00:06<00:09,  8.79it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 58/137 [00:06<00:08,  8.82it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 59/137 [00:06<00:08,  8.86it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 60/137 [00:06<00:09,  8.30it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 61/137 [00:07<00:08,  8.57it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 62/137 [00:07<00:09,  8.04it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 63/137 [00:07<00:08,  8.27it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 64/137 [00:07<00:08,  8.46it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 65/137 [00:07<00:08,  8.61it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 66/137 [00:07<00:08,  8.72it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 67/137 [00:07<00:07,  8.77it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 68/137 [00:07<00:08,  8.55it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 69/137 [00:07<00:07,  8.76it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 70/137 [00:08<00:07,  8.82it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 71/137 [00:08<00:07,  8.57it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 72/137 [00:08<00:07,  8.67it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 73/137 [00:08<00:07,  8.84it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 74/137 [00:08<00:07,  8.62it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 75/137 [00:08<00:07,  8.46it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 76/137 [00:08<00:07,  8.59it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 77/137 [00:08<00:06,  8.80it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 78/137 [00:08<00:06,  8.94it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 79/137 [00:09<00:06,  9.03it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 80/137 [00:09<00:06,  8.75it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 81/137 [00:09<00:06,  8.89it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 82/137 [00:09<00:06,  8.89it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 83/137 [00:09<00:06,  8.38it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 84/137 [00:09<00:06,  8.63it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 85/137 [00:09<00:06,  8.46it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 86/137 [00:09<00:06,  8.39it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 87/137 [00:10<00:05,  8.64it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 88/137 [00:10<00:05,  8.49it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 89/137 [00:10<00:05,  8.38it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 90/137 [00:10<00:05,  8.54it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 91/137 [00:10<00:05,  8.42it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 92/137 [00:10<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 93/137 [00:10<00:05,  8.40it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 94/137 [00:10<00:05,  8.54it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 95/137 [00:10<00:04,  8.75it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 96/137 [00:11<00:04,  8.78it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 97/137 [00:11<00:04,  8.23it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 98/137 [00:11<00:04,  8.44it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 99/137 [00:11<00:04,  8.60it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 100/137 [00:11<00:04,  8.42it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 101/137 [00:11<00:04,  8.66it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 102/137 [00:11<00:03,  8.84it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 103/137 [00:11<00:03,  8.63it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 104/137 [00:12<00:03,  8.82it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 105/137 [00:12<00:03,  8.27it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 106/137 [00:12<00:03,  8.47it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 107/137 [00:12<00:03,  8.32it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 108/137 [00:12<00:03,  8.15it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 109/137 [00:12<00:03,  8.46it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 110/137 [00:12<00:03,  8.58it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 111/137 [00:12<00:02,  8.77it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 112/137 [00:12<00:02,  8.92it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 113/137 [00:13<00:02,  9.03it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 114/137 [00:13<00:02,  9.01it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 115/137 [00:13<00:02,  8.97it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 116/137 [00:13<00:02,  8.71it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 117/137 [00:13<00:02,  8.54it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 118/137 [00:13<00:02,  8.75it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 119/137 [00:13<00:02,  8.58it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 120/137 [00:13<00:01,  8.70it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 121/137 [00:13<00:01,  8.86it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 122/137 [00:14<00:01,  8.54it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 123/137 [00:14<00:01,  8.65it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 124/137 [00:14<00:01,  8.71it/s]\u001b[A\n",
            "Predicting batches:  91%| | 125/137 [00:14<00:01,  8.87it/s]\u001b[A\n",
            "Predicting batches:  92%| | 126/137 [00:14<00:01,  8.21it/s]\u001b[A\n",
            "Predicting batches:  93%| | 127/137 [00:14<00:01,  8.41it/s]\u001b[A\n",
            "Predicting batches:  93%| | 128/137 [00:14<00:01,  8.57it/s]\u001b[A\n",
            "Predicting batches:  94%| | 129/137 [00:14<00:00,  8.45it/s]\u001b[A\n",
            "Predicting batches:  95%| | 130/137 [00:15<00:00,  8.25it/s]\u001b[A\n",
            "Predicting batches:  96%| | 131/137 [00:15<00:00,  8.45it/s]\u001b[A\n",
            "Predicting batches:  96%|| 132/137 [00:15<00:00,  8.35it/s]\u001b[A\n",
            "Predicting batches:  97%|| 133/137 [00:15<00:00,  8.50it/s]\u001b[A\n",
            "Predicting batches:  98%|| 134/137 [00:15<00:00,  8.59it/s]\u001b[A\n",
            "Predicting batches:  99%|| 135/137 [00:15<00:00,  8.68it/s]\u001b[A\n",
            "Predicting batches:  99%|| 136/137 [00:15<00:00,  8.40it/s]\u001b[A\n",
            "Processing subjects:  51%|| 29/57 [03:15<04:50, 10.37s/it, high school psycholo\u001b[A\n",
            "Generating test split: 216 examples [00:00, 39648.56 examples/s]\n",
            "\n",
            "Generating validation split: 23 examples [00:00, 14387.62 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4053.25 examples/s]\n",
            "Processing subjects:  51%|| 29/57 [03:15<04:50, 10.37s/it, high school statisti\n",
            "Formatting batches:   0%|                               | 0/216 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  20%|                | 43/216 [00:00<00:00, 424.10it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 86/216 [00:00<00:00, 426.48it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 129/216 [00:00<00:00, 427.34it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 172/216 [00:00<00:00, 427.10it/s]\u001b[A\n",
            "Formatting batches: 100%|| 216/216 [00:00<00:00, 429.72it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/54 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/54 [00:00<00:09,  5.51it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/54 [00:00<00:10,  5.13it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/54 [00:00<00:09,  5.28it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/54 [00:00<00:09,  5.47it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 5/54 [00:00<00:09,  5.44it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 6/54 [00:01<00:08,  5.44it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 7/54 [00:01<00:08,  5.39it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 8/54 [00:01<00:08,  5.21it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 9/54 [00:01<00:08,  5.15it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 10/54 [00:01<00:08,  5.03it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 11/54 [00:02<00:08,  5.13it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 12/54 [00:02<00:08,  5.09it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 13/54 [00:02<00:08,  4.99it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 14/54 [00:02<00:07,  5.10it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 15/54 [00:02<00:07,  5.20it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 16/54 [00:03<00:07,  5.14it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 17/54 [00:03<00:07,  5.20it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 18/54 [00:03<00:06,  5.22it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 19/54 [00:03<00:06,  5.29it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 20/54 [00:03<00:06,  5.33it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 21/54 [00:04<00:06,  5.30it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 22/54 [00:04<00:06,  5.13it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 23/54 [00:04<00:06,  5.09it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 24/54 [00:04<00:05,  5.06it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 25/54 [00:04<00:05,  5.10it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 26/54 [00:04<00:05,  5.20it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 27/54 [00:05<00:05,  5.24it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 28/54 [00:05<00:04,  5.28it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 29/54 [00:05<00:04,  5.25it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 30/54 [00:05<00:04,  5.25it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 31/54 [00:05<00:04,  5.17it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 32/54 [00:06<00:04,  5.22it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 33/54 [00:06<00:04,  5.15it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 34/54 [00:06<00:03,  5.18it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 35/54 [00:06<00:03,  4.81it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 36/54 [00:06<00:03,  4.92it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 37/54 [00:07<00:03,  4.91it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 38/54 [00:07<00:03,  4.91it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 39/54 [00:07<00:03,  4.86it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 40/54 [00:07<00:02,  5.10it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 41/54 [00:07<00:02,  5.14it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 42/54 [00:08<00:02,  5.22it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 43/54 [00:08<00:02,  5.12it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 44/54 [00:08<00:01,  5.30it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 45/54 [00:08<00:01,  5.27it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 46/54 [00:08<00:01,  5.16it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 47/54 [00:09<00:01,  5.18it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 48/54 [00:09<00:01,  5.20it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 49/54 [00:09<00:00,  5.25it/s]\u001b[A\n",
            "Predicting batches:  93%| | 50/54 [00:09<00:00,  5.41it/s]\u001b[A\n",
            "Predicting batches:  94%| | 51/54 [00:09<00:00,  5.35it/s]\u001b[A\n",
            "Predicting batches:  96%|| 52/54 [00:10<00:00,  5.31it/s]\u001b[A\n",
            "Predicting batches:  98%|| 53/54 [00:10<00:00,  5.45it/s]\u001b[A\n",
            "Predicting batches: 100%|| 54/54 [00:10<00:00,  5.30it/s]\u001b[A\n",
            "Processing subjects:  53%|| 30/57 [03:26<04:44, 10.54s/it, high school statisti\u001b[A\n",
            "Generating test split: 204 examples [00:00, 32256.58 examples/s]\n",
            "\n",
            "Generating validation split: 22 examples [00:00, 12527.11 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3925.78 examples/s]\n",
            "Processing subjects:  53%|| 30/57 [03:26<04:44, 10.54s/it, high school us histo\n",
            "Formatting batches:   0%|                               | 0/204 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  13%|                  | 27/204 [00:00<00:00, 268.32it/s]\u001b[A\n",
            "Formatting batches:  27%|               | 56/204 [00:00<00:00, 279.52it/s]\u001b[A\n",
            "Formatting batches:  42%|            | 86/204 [00:00<00:00, 284.86it/s]\u001b[A\n",
            "Formatting batches:  56%|        | 115/204 [00:00<00:00, 286.40it/s]\u001b[A\n",
            "Formatting batches:  71%|      | 144/204 [00:00<00:00, 287.14it/s]\u001b[A\n",
            "Formatting batches:  85%|   | 174/204 [00:00<00:00, 288.35it/s]\u001b[A\n",
            "Formatting batches: 100%|| 203/204 [00:00<00:00, 286.83it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/51 [00:00<00:30,  1.64it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:01<00:29,  1.64it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/51 [00:01<00:29,  1.64it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:02<00:28,  1.64it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:03<00:28,  1.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:03<00:27,  1.63it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:04<00:26,  1.63it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:04<00:26,  1.61it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:05<00:26,  1.59it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:06<00:26,  1.57it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:06<00:25,  1.56it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:07<00:24,  1.59it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:09<00:22,  1.59it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:10<00:22,  1.58it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:10<00:21,  1.57it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:11<00:21,  1.56it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:11<00:20,  1.58it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:12<00:19,  1.60it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:13<00:18,  1.60it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:13<00:18,  1.58it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:14<00:17,  1.60it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:15<00:16,  1.60it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:15<00:16,  1.61it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:16<00:15,  1.61it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:16<00:15,  1.59it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:17<00:14,  1.60it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:18<00:13,  1.58it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:18<00:13,  1.56it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:19<00:12,  1.58it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:20<00:11,  1.60it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:20<00:11,  1.60it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:21<00:10,  1.61it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:21<00:10,  1.59it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:22<00:09,  1.60it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:23<00:08,  1.62it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 38/51 [00:23<00:08,  1.61it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:24<00:07,  1.61it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:25<00:06,  1.60it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:25<00:06,  1.58it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:26<00:05,  1.58it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:26<00:05,  1.59it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:27<00:04,  1.56it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:28<00:03,  1.56it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:28<00:03,  1.56it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:29<00:02,  1.55it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:30<00:01,  1.57it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:30<00:01,  1.58it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:31<00:00,  1.59it/s]\u001b[A\n",
            "Predicting batches: 100%|| 51/51 [00:32<00:00,  1.60it/s]\u001b[A\n",
            "Processing subjects:  54%|| 31/57 [03:58<07:27, 17.20s/it, high school us histo\u001b[A\n",
            "Generating test split: 237 examples [00:00, 32344.71 examples/s]\n",
            "\n",
            "Generating validation split: 26 examples [00:00, 14891.70 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4139.66 examples/s]\n",
            "Processing subjects:  54%|| 31/57 [03:58<07:27, 17.20s/it, high school world hi\n",
            "Formatting batches:   0%|                               | 0/237 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                  | 35/237 [00:00<00:00, 347.14it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 71/237 [00:00<00:00, 350.33it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 107/237 [00:00<00:00, 348.88it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 142/237 [00:00<00:00, 349.05it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 177/237 [00:00<00:00, 349.31it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 212/237 [00:00<00:00, 346.49it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/60 [00:00<00:23,  2.52it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/60 [00:00<00:21,  2.70it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/60 [00:01<00:22,  2.57it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/60 [00:01<00:21,  2.59it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/60 [00:01<00:21,  2.59it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/60 [00:02<00:22,  2.42it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 7/60 [00:02<00:21,  2.42it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 8/60 [00:03<00:20,  2.54it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/60 [00:03<00:21,  2.41it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/60 [00:04<00:20,  2.42it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 11/60 [00:04<00:19,  2.48it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/60 [00:04<00:19,  2.51it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 13/60 [00:05<00:18,  2.50it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 14/60 [00:05<00:19,  2.41it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 15/60 [00:06<00:18,  2.50it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/60 [00:06<00:17,  2.53it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 17/60 [00:06<00:16,  2.62it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 18/60 [00:07<00:16,  2.62it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 19/60 [00:07<00:16,  2.52it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 20/60 [00:07<00:15,  2.55it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 21/60 [00:08<00:15,  2.51it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/60 [00:08<00:16,  2.36it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 23/60 [00:09<00:15,  2.43it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 24/60 [00:09<00:14,  2.44it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 25/60 [00:10<00:14,  2.48it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 26/60 [00:10<00:13,  2.53it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 27/60 [00:10<00:13,  2.42it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/60 [00:11<00:13,  2.36it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 29/60 [00:11<00:13,  2.33it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 30/60 [00:12<00:12,  2.44it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 31/60 [00:12<00:11,  2.50it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 32/60 [00:12<00:11,  2.45it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 33/60 [00:13<00:10,  2.48it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 34/60 [00:13<00:10,  2.51it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 35/60 [00:14<00:10,  2.46it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 36/60 [00:14<00:09,  2.51it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 37/60 [00:14<00:09,  2.56it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 38/60 [00:15<00:08,  2.61it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 39/60 [00:15<00:08,  2.55it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 40/60 [00:16<00:07,  2.58it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 41/60 [00:16<00:07,  2.59it/s]\u001b[A\n",
            "Predicting batches:  70%|       | 42/60 [00:16<00:07,  2.46it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 43/60 [00:17<00:07,  2.41it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 44/60 [00:17<00:06,  2.41it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 45/60 [00:18<00:06,  2.39it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 46/60 [00:18<00:05,  2.48it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 47/60 [00:18<00:05,  2.50it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 48/60 [00:19<00:04,  2.42it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 49/60 [00:19<00:04,  2.53it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 50/60 [00:20<00:03,  2.57it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 51/60 [00:20<00:03,  2.58it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 52/60 [00:20<00:03,  2.47it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 53/60 [00:21<00:02,  2.46it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 54/60 [00:21<00:02,  2.41it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 55/60 [00:22<00:02,  2.49it/s]\u001b[A\n",
            "Predicting batches:  93%| | 56/60 [00:22<00:01,  2.56it/s]\u001b[A\n",
            "Predicting batches:  95%| | 57/60 [00:22<00:01,  2.46it/s]\u001b[A\n",
            "Predicting batches:  97%|| 58/60 [00:23<00:00,  2.52it/s]\u001b[A\n",
            "Predicting batches:  98%|| 59/60 [00:23<00:00,  2.64it/s]\u001b[A\n",
            "Processing subjects:  56%|| 32/57 [04:23<08:04, 19.38s/it, high school world hi\u001b[A\n",
            "Generating test split: 223 examples [00:00, 37264.14 examples/s]\n",
            "\n",
            "Generating validation split: 23 examples [00:00, 13797.05 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4107.23 examples/s]\n",
            "Processing subjects:  56%|    | 32/57 [04:23<08:04, 19.38s/it, human aging]\n",
            "Formatting batches:   0%|                               | 0/223 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  22%|                | 49/223 [00:00<00:00, 489.36it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 100/223 [00:00<00:00, 499.06it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 151/223 [00:00<00:00, 502.12it/s]\u001b[A\n",
            "Formatting batches:  91%|  | 202/223 [00:00<00:00, 501.42it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/56 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/56 [00:00<00:03, 14.11it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/56 [00:00<00:03, 13.62it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 6/56 [00:00<00:03, 13.58it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/56 [00:00<00:03, 13.63it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 10/56 [00:00<00:03, 13.82it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 12/56 [00:00<00:03, 13.73it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 14/56 [00:01<00:03, 13.87it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 16/56 [00:01<00:02, 13.75it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 18/56 [00:01<00:02, 13.66it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 20/56 [00:01<00:02, 13.83it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 22/56 [00:01<00:02, 13.77it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 24/56 [00:01<00:02, 13.83it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 26/56 [00:01<00:02, 13.85it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 28/56 [00:02<00:02, 13.94it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 30/56 [00:02<00:01, 13.97it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 32/56 [00:02<00:01, 13.84it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 34/56 [00:02<00:01, 13.87it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 36/56 [00:02<00:01, 13.75it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 38/56 [00:02<00:01, 13.64it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 40/56 [00:02<00:01, 13.53it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 42/56 [00:03<00:01, 13.55it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 44/56 [00:03<00:00, 13.66it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 46/56 [00:03<00:00, 13.81it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 48/56 [00:03<00:00, 13.73it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 50/56 [00:03<00:00, 13.50it/s]\u001b[A\n",
            "Predicting batches:  93%| | 52/56 [00:03<00:00, 13.66it/s]\u001b[A\n",
            "Predicting batches:  96%|| 54/56 [00:03<00:00, 13.77it/s]\u001b[A\n",
            "Predicting batches: 100%|| 56/56 [00:04<00:00, 14.26it/s]\u001b[A\n",
            "Processing subjects:  58%|   | 33/57 [04:27<05:58, 14.92s/it, human aging]\u001b[A\n",
            "Generating test split: 131 examples [00:00, 34720.62 examples/s]\n",
            "\n",
            "Generating validation split: 12 examples [00:00, 8304.18 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4027.56 examples/s]\n",
            "Processing subjects:  58%|  | 33/57 [04:27<05:58, 14.92s/it, human sexuality]\n",
            "Formatting batches:   0%|                               | 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  37%|             | 48/131 [00:00<00:00, 478.82it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 98/131 [00:00<00:00, 490.09it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/33 [00:00<00:02, 12.70it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/33 [00:00<00:02, 12.46it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 6/33 [00:00<00:02, 12.72it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 8/33 [00:00<00:02, 12.35it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 10/33 [00:00<00:01, 12.49it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 12/33 [00:00<00:01, 12.47it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 14/33 [00:01<00:01, 12.75it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 16/33 [00:01<00:01, 12.66it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 18/33 [00:01<00:01, 12.69it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 20/33 [00:01<00:01, 12.49it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 22/33 [00:01<00:00, 12.69it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 24/33 [00:01<00:00, 12.58it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 26/33 [00:02<00:00, 11.83it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 28/33 [00:02<00:00, 11.92it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 30/33 [00:02<00:00, 11.99it/s]\u001b[A\n",
            "Predicting batches:  97%|| 32/33 [00:02<00:00, 12.10it/s]\u001b[A\n",
            "Processing subjects:  60%|  | 34/57 [04:30<04:20, 11.32s/it, human sexuality]\u001b[A\n",
            "Generating test split: 121 examples [00:00, 32568.23 examples/s]\n",
            "\n",
            "Generating validation split: 13 examples [00:00, 8929.90 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4073.72 examples/s]\n",
            "Processing subjects:  60%| | 34/57 [04:30<04:20, 11.32s/it, international law]\n",
            "Formatting batches:   0%|                               | 0/121 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  37%|             | 45/121 [00:00<00:00, 442.11it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 90/121 [00:00<00:00, 445.34it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/31 [00:00<00:04,  6.86it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/31 [00:00<00:04,  6.71it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 3/31 [00:00<00:04,  6.84it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 4/31 [00:00<00:04,  6.72it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 5/31 [00:00<00:03,  6.54it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 6/31 [00:00<00:03,  6.55it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 7/31 [00:01<00:03,  6.45it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 8/31 [00:01<00:03,  6.49it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 9/31 [00:01<00:03,  6.51it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 10/31 [00:01<00:03,  6.66it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 11/31 [00:01<00:03,  6.64it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 12/31 [00:01<00:02,  6.61it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 13/31 [00:01<00:02,  6.70it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 14/31 [00:02<00:02,  6.78it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 15/31 [00:02<00:02,  6.70it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 16/31 [00:02<00:02,  6.66it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 17/31 [00:02<00:02,  6.63it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 18/31 [00:02<00:01,  6.70it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 19/31 [00:02<00:01,  6.63it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 20/31 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 21/31 [00:03<00:01,  6.46it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 22/31 [00:03<00:01,  6.60it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 23/31 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 24/31 [00:03<00:01,  6.67it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 25/31 [00:03<00:00,  6.73it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 26/31 [00:03<00:00,  6.66it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 27/31 [00:04<00:00,  6.72it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 28/31 [00:04<00:00,  6.77it/s]\u001b[A\n",
            "Predicting batches:  94%| | 29/31 [00:04<00:00,  6.69it/s]\u001b[A\n",
            "Predicting batches:  97%|| 30/31 [00:04<00:00,  6.65it/s]\u001b[A\n",
            "Processing subjects:  61%| | 35/57 [04:35<03:26,  9.38s/it, international law]\u001b[A\n",
            "Generating test split: 108 examples [00:00, 30098.66 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7600.88 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3784.79 examples/s]\n",
            "Processing subjects:  61%|  | 35/57 [04:35<03:26,  9.38s/it, jurisprudence]\n",
            "Formatting batches:   0%|                               | 0/108 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 48/108 [00:00<00:00, 477.00it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 97/108 [00:00<00:00, 482.43it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/27 [00:00<00:02, 11.13it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/27 [00:00<00:02, 10.40it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 6/27 [00:00<00:01, 10.51it/s]\u001b[A\n",
            "Predicting batches:  30%|                 | 8/27 [00:00<00:01, 10.72it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 10/27 [00:00<00:01, 10.70it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 12/27 [00:01<00:01, 10.84it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 14/27 [00:01<00:01, 10.94it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 16/27 [00:01<00:01, 10.59it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 18/27 [00:01<00:00, 10.64it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 20/27 [00:01<00:00, 10.41it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 22/27 [00:02<00:00, 10.75it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 24/27 [00:02<00:00, 11.04it/s]\u001b[A\n",
            "Predicting batches:  96%|| 26/27 [00:02<00:00, 10.93it/s]\u001b[A\n",
            "Processing subjects:  63%|  | 36/57 [04:38<02:35,  7.39s/it, jurisprudence]\u001b[A\n",
            "Generating test split: 163 examples [00:00, 37328.50 examples/s]\n",
            "\n",
            "Generating validation split: 18 examples [00:00, 11663.44 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4113.68 examples/s]\n",
            "Processing subjects:  63%| | 36/57 [04:38<02:35,  7.39s/it, logical fallacies]\n",
            "Formatting batches:   0%|                               | 0/163 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  29%|              | 48/163 [00:00<00:00, 471.96it/s]\u001b[A\n",
            "Formatting batches:  59%|        | 96/163 [00:00<00:00, 473.22it/s]\u001b[A\n",
            "Formatting batches:  88%|  | 144/163 [00:00<00:00, 475.74it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/41 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/41 [00:00<00:04,  9.90it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/41 [00:00<00:04,  9.58it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/41 [00:00<00:03,  9.76it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/41 [00:00<00:03,  9.84it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 5/41 [00:00<00:03,  9.65it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 6/41 [00:00<00:03,  9.53it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 7/41 [00:00<00:03,  9.64it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 8/41 [00:00<00:03,  9.40it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 9/41 [00:00<00:03,  9.34it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/41 [00:01<00:03,  9.32it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 11/41 [00:01<00:03,  9.18it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/41 [00:01<00:03,  9.21it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 13/41 [00:01<00:03,  9.24it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 14/41 [00:01<00:02,  9.25it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 15/41 [00:01<00:02,  9.27it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 16/41 [00:01<00:02,  9.17it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 17/41 [00:01<00:02,  9.39it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 18/41 [00:01<00:02,  9.37it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 19/41 [00:02<00:02,  9.24it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 20/41 [00:02<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 21/41 [00:02<00:02,  9.46it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 22/41 [00:02<00:02,  9.42it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 23/41 [00:02<00:01,  9.27it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 24/41 [00:02<00:01,  9.26it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 25/41 [00:02<00:01,  9.42it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 26/41 [00:02<00:01,  9.37it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 27/41 [00:02<00:01,  9.34it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 28/41 [00:02<00:01,  9.30it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 29/41 [00:03<00:01,  9.46it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 30/41 [00:03<00:01,  9.56it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 31/41 [00:03<00:01,  9.45it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 32/41 [00:03<00:00,  9.01it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 33/41 [00:03<00:00,  9.28it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 34/41 [00:03<00:00,  9.28it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 35/41 [00:03<00:00,  9.26it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 36/41 [00:03<00:00,  9.44it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 37/41 [00:03<00:00,  9.24it/s]\u001b[A\n",
            "Predicting batches:  93%| | 38/41 [00:04<00:00,  9.13it/s]\u001b[A\n",
            "Predicting batches:  95%| | 39/41 [00:04<00:00,  9.17it/s]\u001b[A\n",
            "Predicting batches:  98%|| 40/41 [00:04<00:00,  9.20it/s]\u001b[A\n",
            "Processing subjects:  65%| | 37/57 [04:43<02:11,  6.59s/it, logical fallacies]\u001b[A\n",
            "Generating test split: 112 examples [00:00, 30771.78 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7695.97 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4032.98 examples/s]\n",
            "Processing subjects:  65%| | 37/57 [04:43<02:11,  6.59s/it, machine learning]\n",
            "Formatting batches:   0%|                               | 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  38%|             | 43/112 [00:00<00:00, 421.23it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 87/112 [00:00<00:00, 432.01it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/28 [00:00<00:03,  6.81it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/28 [00:00<00:03,  6.68it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 3/28 [00:00<00:03,  6.65it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/28 [00:00<00:03,  6.61it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 5/28 [00:00<00:03,  6.57it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 6/28 [00:00<00:03,  6.55it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 7/28 [00:01<00:03,  6.55it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 8/28 [00:01<00:03,  6.34it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 9/28 [00:01<00:02,  6.38it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 10/28 [00:01<00:02,  6.42it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 11/28 [00:01<00:02,  6.26it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 12/28 [00:01<00:02,  6.33it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 13/28 [00:02<00:02,  6.31it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 14/28 [00:02<00:02,  6.48it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 15/28 [00:02<00:02,  6.40it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 16/28 [00:02<00:01,  6.43it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 17/28 [00:02<00:01,  6.29it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 18/28 [00:02<00:01,  6.35it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 19/28 [00:02<00:01,  6.40it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 20/28 [00:03<00:01,  6.36it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 21/28 [00:03<00:01,  6.40it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 22/28 [00:03<00:00,  6.43it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 23/28 [00:03<00:00,  6.47it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 24/28 [00:03<00:00,  6.40it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 25/28 [00:03<00:00,  6.35it/s]\u001b[A\n",
            "Predicting batches:  93%| | 26/28 [00:04<00:00,  5.97it/s]\u001b[A\n",
            "Predicting batches:  96%|| 27/28 [00:04<00:00,  5.78it/s]\u001b[A\n",
            "Predicting batches: 100%|| 28/28 [00:04<00:00,  5.89it/s]\u001b[A\n",
            "Processing subjects:  67%| | 38/57 [04:47<01:54,  6.03s/it, machine learning]\u001b[A\n",
            "Generating test split: 103 examples [00:00, 30660.99 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7615.94 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4096.80 examples/s]\n",
            "Processing subjects:  67%|   | 38/57 [04:47<01:54,  6.03s/it, management]\n",
            "Formatting batches:   0%|                               | 0/103 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  48%|           | 49/103 [00:00<00:00, 488.67it/s]\u001b[A\n",
            "Formatting batches:  98%|| 101/103 [00:00<00:00, 503.39it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/26 [00:00<00:01, 15.84it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/26 [00:00<00:01, 15.22it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 6/26 [00:00<00:01, 15.24it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 8/26 [00:00<00:01, 15.20it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 10/26 [00:00<00:01, 15.20it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 12/26 [00:00<00:00, 15.20it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 14/26 [00:00<00:00, 15.16it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 16/26 [00:01<00:00, 15.16it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 18/26 [00:01<00:00, 15.18it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 20/26 [00:01<00:00, 15.03it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 22/26 [00:01<00:00, 14.91it/s]\u001b[A\n",
            "Predicting batches:  92%| | 24/26 [00:01<00:00, 14.98it/s]\u001b[A\n",
            "Predicting batches: 100%|| 26/26 [00:01<00:00, 15.51it/s]\u001b[A\n",
            "Processing subjects:  68%|   | 39/57 [04:49<01:26,  4.80s/it, management]\u001b[A\n",
            "Generating test split: 234 examples [00:00, 44320.03 examples/s]\n",
            "\n",
            "Generating validation split: 25 examples [00:00, 15504.60 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4090.41 examples/s]\n",
            "Processing subjects:  68%|   | 39/57 [04:49<01:26,  4.80s/it, marketing]\n",
            "Formatting batches:   0%|                               | 0/234 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  21%|                | 48/234 [00:00<00:00, 479.54it/s]\u001b[A\n",
            "Formatting batches:  41%|            | 97/234 [00:00<00:00, 481.43it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 146/234 [00:00<00:00, 482.50it/s]\u001b[A\n",
            "Formatting batches:  83%|   | 195/234 [00:00<00:00, 484.01it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/59 [00:00<00:05, 10.72it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/59 [00:00<00:05, 10.80it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/59 [00:00<00:04, 10.81it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/59 [00:00<00:04, 10.80it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/59 [00:00<00:04, 10.80it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/59 [00:01<00:04, 10.80it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 14/59 [00:01<00:04, 10.91it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/59 [00:01<00:03, 10.88it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 18/59 [00:01<00:03, 10.83it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 20/59 [00:01<00:03, 10.91it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/59 [00:02<00:03, 10.70it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 24/59 [00:02<00:03, 10.61it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 26/59 [00:02<00:03, 10.66it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/59 [00:02<00:02, 10.56it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 30/59 [00:02<00:02, 10.73it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 32/59 [00:02<00:02, 10.86it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 34/59 [00:03<00:02, 10.68it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 36/59 [00:03<00:02, 10.68it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 38/59 [00:03<00:01, 10.95it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 40/59 [00:03<00:01, 10.74it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 42/59 [00:03<00:01, 10.73it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 44/59 [00:04<00:01, 10.62it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 46/59 [00:04<00:01, 10.55it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 48/59 [00:04<00:01, 10.74it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 50/59 [00:04<00:00, 10.72it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 52/59 [00:04<00:00, 10.72it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 54/59 [00:05<00:00, 10.60it/s]\u001b[A\n",
            "Predicting batches:  95%| | 56/59 [00:05<00:00, 10.64it/s]\u001b[A\n",
            "Predicting batches:  98%|| 58/59 [00:05<00:00, 10.78it/s]\u001b[A\n",
            "Processing subjects:  70%|   | 40/57 [04:55<01:27,  5.14s/it, marketing]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 29910.18 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7653.84 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4048.56 examples/s]\n",
            "Processing subjects:  70%| | 40/57 [04:55<01:27,  5.14s/it, medical genetics]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 44/100 [00:00<00:00, 437.23it/s]\u001b[A\n",
            "Formatting batches:  95%| | 95/100 [00:00<00:00, 475.38it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:01, 13.25it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:01, 13.17it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 13.00it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 12.81it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 12.70it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:00<00:01, 12.64it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:00, 12.49it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 12.41it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 12.73it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 12.79it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:01<00:00, 12.88it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:01<00:00, 12.93it/s]\u001b[A\n",
            "Processing subjects:  72%| | 41/57 [04:57<01:08,  4.25s/it, medical genetics]\u001b[A\n",
            "Generating test split: 783 examples [00:00, 59654.15 examples/s]\n",
            "\n",
            "Generating validation split: 86 examples [00:00, 34464.95 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4060.31 examples/s]\n",
            "Processing subjects:  72%|  | 41/57 [04:57<01:08,  4.25s/it, miscellaneous]\n",
            "Formatting batches:   0%|                               | 0/783 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   7%|                   | 53/783 [00:00<00:01, 520.50it/s]\u001b[A\n",
            "Formatting batches:  14%|                 | 106/783 [00:00<00:01, 521.56it/s]\u001b[A\n",
            "Formatting batches:  20%|                | 159/783 [00:00<00:01, 520.39it/s]\u001b[A\n",
            "Formatting batches:  27%|              | 212/783 [00:00<00:01, 520.65it/s]\u001b[A\n",
            "Formatting batches:  34%|             | 265/783 [00:00<00:01, 512.15it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 317/783 [00:00<00:00, 513.98it/s]\u001b[A\n",
            "Formatting batches:  47%|          | 369/783 [00:00<00:00, 511.26it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 421/783 [00:00<00:00, 509.86it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 472/783 [00:00<00:00, 509.90it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 523/783 [00:01<00:00, 509.63it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 575/783 [00:01<00:00, 511.69it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 627/783 [00:01<00:00, 512.27it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 679/783 [00:01<00:00, 513.84it/s]\u001b[A\n",
            "Formatting batches:  93%| | 731/783 [00:01<00:00, 514.66it/s]\u001b[A\n",
            "Formatting batches: 100%|| 783/783 [00:01<00:00, 515.33it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/196 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/196 [00:00<00:13, 14.80it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 4/196 [00:00<00:13, 14.50it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 6/196 [00:00<00:13, 14.53it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 8/196 [00:00<00:12, 14.86it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 10/196 [00:00<00:12, 15.03it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 12/196 [00:00<00:12, 14.81it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 14/196 [00:00<00:12, 14.83it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 16/196 [00:01<00:12, 14.59it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 18/196 [00:01<00:12, 14.54it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 20/196 [00:01<00:12, 13.65it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 22/196 [00:01<00:12, 14.13it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 24/196 [00:01<00:12, 14.21it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 26/196 [00:01<00:11, 14.52it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 28/196 [00:01<00:11, 14.35it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 30/196 [00:02<00:12, 13.42it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 32/196 [00:02<00:11, 13.91it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 34/196 [00:02<00:11, 13.95it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 36/196 [00:02<00:11, 14.04it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 38/196 [00:02<00:11, 14.24it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 40/196 [00:02<00:10, 14.54it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 42/196 [00:02<00:10, 14.39it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 44/196 [00:03<00:10, 14.31it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 46/196 [00:03<00:10, 13.95it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 48/196 [00:03<00:10, 13.88it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 50/196 [00:03<00:10, 14.09it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 52/196 [00:03<00:10, 14.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 54/196 [00:03<00:10, 13.81it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 56/196 [00:03<00:10, 13.51it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 58/196 [00:04<00:09, 13.87it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 60/196 [00:04<00:09, 14.25it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 62/196 [00:04<00:09, 14.39it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 64/196 [00:04<00:09, 14.18it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 66/196 [00:04<00:09, 13.52it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 68/196 [00:04<00:09, 13.69it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 70/196 [00:04<00:08, 14.13it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 72/196 [00:05<00:08, 14.27it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 74/196 [00:05<00:08, 14.55it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 76/196 [00:05<00:08, 14.74it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 78/196 [00:05<00:08, 14.58it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 80/196 [00:05<00:08, 14.46it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 82/196 [00:05<00:07, 14.68it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 84/196 [00:05<00:07, 14.70it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 86/196 [00:06<00:07, 14.68it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 88/196 [00:06<00:07, 14.53it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 90/196 [00:06<00:07, 14.36it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 92/196 [00:06<00:07, 14.19it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 94/196 [00:06<00:07, 14.47it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 96/196 [00:06<00:06, 14.51it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 98/196 [00:06<00:06, 14.72it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 100/196 [00:06<00:06, 14.50it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 102/196 [00:07<00:06, 14.52it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 104/196 [00:07<00:06, 14.16it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 106/196 [00:07<00:06, 14.26it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 108/196 [00:07<00:06, 13.97it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 110/196 [00:07<00:06, 14.00it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 112/196 [00:07<00:06, 13.99it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 114/196 [00:07<00:05, 14.34it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 116/196 [00:08<00:05, 14.58it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 118/196 [00:08<00:05, 13.74it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 120/196 [00:08<00:05, 13.74it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 122/196 [00:08<00:05, 13.45it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 124/196 [00:08<00:05, 13.83it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 126/196 [00:08<00:05, 13.50it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 128/196 [00:09<00:05, 13.57it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 130/196 [00:09<00:05, 13.09it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 132/196 [00:09<00:04, 13.46it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 134/196 [00:09<00:04, 13.92it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 136/196 [00:09<00:04, 13.93it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 138/196 [00:09<00:04, 13.01it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 140/196 [00:09<00:04, 13.42it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 142/196 [00:10<00:03, 13.51it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 144/196 [00:10<00:03, 13.28it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 146/196 [00:10<00:03, 13.52it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 148/196 [00:10<00:03, 12.98it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 150/196 [00:10<00:03, 12.86it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 152/196 [00:10<00:03, 12.42it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 154/196 [00:10<00:03, 13.00it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 156/196 [00:11<00:02, 13.45it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 158/196 [00:11<00:02, 13.93it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 160/196 [00:11<00:02, 14.10it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 162/196 [00:11<00:02, 12.93it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 164/196 [00:11<00:02, 13.14it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 166/196 [00:11<00:02, 13.54it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 168/196 [00:12<00:02, 13.69it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 170/196 [00:12<00:01, 14.12it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 172/196 [00:12<00:01, 14.08it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 174/196 [00:12<00:01, 13.84it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 176/196 [00:12<00:01, 14.06it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 178/196 [00:12<00:01, 14.39it/s]\u001b[A\n",
            "Predicting batches:  92%| | 180/196 [00:12<00:01, 14.01it/s]\u001b[A\n",
            "Predicting batches:  93%| | 182/196 [00:12<00:00, 14.37it/s]\u001b[A\n",
            "Predicting batches:  94%| | 184/196 [00:13<00:00, 14.62it/s]\u001b[A\n",
            "Predicting batches:  95%| | 186/196 [00:13<00:00, 14.59it/s]\u001b[A\n",
            "Predicting batches:  96%|| 188/196 [00:13<00:00, 14.78it/s]\u001b[A\n",
            "Predicting batches:  97%|| 190/196 [00:13<00:00, 14.54it/s]\u001b[A\n",
            "Predicting batches:  98%|| 192/196 [00:13<00:00, 14.34it/s]\u001b[A\n",
            "Predicting batches:  99%|| 194/196 [00:13<00:00, 14.00it/s]\u001b[A\n",
            "Predicting batches: 100%|| 196/196 [00:13<00:00, 14.61it/s]\u001b[A\n",
            "Processing subjects:  74%| | 42/57 [05:13<01:54,  7.63s/it, miscellaneous]\u001b[A\n",
            "Generating test split: 346 examples [00:00, 48930.48 examples/s]\n",
            "\n",
            "Generating validation split: 38 examples [00:00, 20592.19 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3990.77 examples/s]\n",
            "Processing subjects:  74%| | 42/57 [05:13<01:54,  7.63s/it, moral disputes]\n",
            "Formatting batches:   0%|                               | 0/346 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  14%|                  | 47/346 [00:00<00:00, 460.40it/s]\u001b[A\n",
            "Formatting batches:  27%|               | 95/346 [00:00<00:00, 466.01it/s]\u001b[A\n",
            "Formatting batches:  41%|           | 142/346 [00:00<00:00, 467.34it/s]\u001b[A\n",
            "Formatting batches:  55%|         | 190/346 [00:00<00:00, 468.55it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 237/346 [00:00<00:00, 459.04it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 284/346 [00:00<00:00, 459.98it/s]\u001b[A\n",
            "Formatting batches:  96%|| 331/346 [00:00<00:00, 462.96it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/87 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/87 [00:00<00:09,  9.51it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/87 [00:00<00:08,  9.48it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 3/87 [00:00<00:09,  9.26it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 4/87 [00:00<00:08,  9.30it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/87 [00:00<00:08,  9.15it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 6/87 [00:00<00:08,  9.20it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 7/87 [00:00<00:08,  9.09it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 8/87 [00:00<00:08,  9.04it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 9/87 [00:00<00:08,  8.71it/s]\u001b[A\n",
            "Predicting batches:  11%|                    | 10/87 [00:01<00:08,  8.87it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 11/87 [00:01<00:08,  8.91it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 12/87 [00:01<00:08,  9.02it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 13/87 [00:01<00:08,  8.74it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 14/87 [00:01<00:08,  8.79it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 15/87 [00:01<00:08,  8.91it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 16/87 [00:01<00:07,  9.00it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 17/87 [00:01<00:07,  9.05it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 18/87 [00:01<00:07,  8.97it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 19/87 [00:02<00:07,  9.06it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 20/87 [00:02<00:07,  9.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 21/87 [00:02<00:07,  9.16it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 22/87 [00:02<00:07,  9.05it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 23/87 [00:02<00:07,  9.01it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 24/87 [00:02<00:06,  9.07it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 25/87 [00:02<00:06,  9.11it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 26/87 [00:02<00:06,  9.06it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 27/87 [00:02<00:06,  9.01it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 28/87 [00:03<00:06,  8.97it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 29/87 [00:03<00:06,  9.05it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 30/87 [00:03<00:06,  9.10it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 31/87 [00:03<00:06,  9.01it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 32/87 [00:03<00:06,  8.96it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 33/87 [00:03<00:06,  8.94it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 34/87 [00:03<00:05,  8.90it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 35/87 [00:03<00:05,  8.99it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 36/87 [00:03<00:05,  8.94it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 37/87 [00:04<00:05,  9.02it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 38/87 [00:04<00:05,  8.70it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 39/87 [00:04<00:05,  8.85it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 40/87 [00:04<00:05,  8.96it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 41/87 [00:04<00:05,  9.02it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 42/87 [00:04<00:04,  9.07it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 43/87 [00:04<00:04,  9.02it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 44/87 [00:04<00:04,  8.97it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 45/87 [00:04<00:04,  9.04it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 46/87 [00:05<00:04,  9.11it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 47/87 [00:05<00:04,  8.75it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 48/87 [00:05<00:04,  8.89it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 49/87 [00:05<00:04,  8.98it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 50/87 [00:05<00:04,  8.95it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 51/87 [00:05<00:03,  9.02it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 52/87 [00:05<00:03,  9.06it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 53/87 [00:05<00:03,  9.01it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 54/87 [00:05<00:03,  9.06it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 55/87 [00:06<00:03,  9.09it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 56/87 [00:06<00:03,  9.02it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 57/87 [00:06<00:03,  9.08it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 58/87 [00:06<00:03,  8.99it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 59/87 [00:06<00:03,  8.95it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 60/87 [00:06<00:03,  8.94it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 61/87 [00:06<00:02,  8.92it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 62/87 [00:06<00:02,  9.01it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 63/87 [00:06<00:02,  9.07it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 64/87 [00:07<00:02,  9.01it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 65/87 [00:07<00:02,  8.67it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 66/87 [00:07<00:02,  8.81it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 67/87 [00:07<00:02,  8.82it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 68/87 [00:07<00:02,  8.93it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 69/87 [00:07<00:02,  8.87it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 70/87 [00:07<00:01,  8.62it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 71/87 [00:07<00:01,  8.68it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 72/87 [00:08<00:01,  8.74it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 73/87 [00:08<00:01,  8.87it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 74/87 [00:08<00:01,  8.87it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 75/87 [00:08<00:01,  8.95it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 76/87 [00:08<00:01,  8.92it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 77/87 [00:08<00:01,  8.99it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 78/87 [00:08<00:00,  9.05it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 79/87 [00:08<00:00,  9.09it/s]\u001b[A\n",
            "Predicting batches:  92%| | 80/87 [00:08<00:00,  8.99it/s]\u001b[A\n",
            "Predicting batches:  93%| | 81/87 [00:09<00:00,  9.04it/s]\u001b[A\n",
            "Predicting batches:  94%| | 82/87 [00:09<00:00,  9.07it/s]\u001b[A\n",
            "Predicting batches:  95%| | 83/87 [00:09<00:00,  9.09it/s]\u001b[A\n",
            "Predicting batches:  97%|| 84/87 [00:09<00:00,  9.12it/s]\u001b[A\n",
            "Predicting batches:  98%|| 85/87 [00:09<00:00,  9.13it/s]\u001b[A\n",
            "Predicting batches:  99%|| 86/87 [00:09<00:00,  9.02it/s]\u001b[A\n",
            "Processing subjects:  75%| | 43/57 [05:23<01:58,  8.46s/it, moral disputes]\u001b[A\n",
            "Generating test split: 895 examples [00:00, 57901.10 examples/s]\n",
            "\n",
            "Generating validation split: 100 examples [00:00, 32448.58 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3649.12 examples/s]\n",
            "Processing subjects:  75%| | 43/57 [05:23<01:58,  8.46s/it, moral scenarios]\n",
            "Formatting batches:   0%|                               | 0/895 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   5%|                    | 44/895 [00:00<00:01, 437.52it/s]\u001b[A\n",
            "Formatting batches:  10%|                   | 89/895 [00:00<00:01, 443.22it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 134/895 [00:00<00:01, 444.68it/s]\u001b[A\n",
            "Formatting batches:  20%|                | 179/895 [00:00<00:01, 428.61it/s]\u001b[A\n",
            "Formatting batches:  25%|               | 222/895 [00:00<00:01, 418.05it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 268/895 [00:00<00:01, 430.49it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 313/895 [00:00<00:01, 435.75it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 358/895 [00:00<00:01, 439.66it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 403/895 [00:00<00:01, 441.69it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 448/895 [00:01<00:01, 442.33it/s]\u001b[A\n",
            "Formatting batches:  55%|         | 493/895 [00:01<00:00, 442.67it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 538/895 [00:01<00:00, 444.83it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 583/895 [00:01<00:00, 445.12it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 628/895 [00:01<00:00, 445.79it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 673/895 [00:01<00:00, 446.99it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 718/895 [00:01<00:00, 442.92it/s]\u001b[A\n",
            "Formatting batches:  85%|   | 763/895 [00:01<00:00, 440.37it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 808/895 [00:01<00:00, 442.59it/s]\u001b[A\n",
            "Formatting batches:  95%| | 853/895 [00:01<00:00, 442.41it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/224 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   0%|                       | 1/224 [00:00<00:30,  7.28it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/224 [00:00<00:32,  6.88it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 3/224 [00:00<00:31,  6.92it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 4/224 [00:00<00:32,  6.77it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 5/224 [00:00<00:32,  6.82it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 6/224 [00:00<00:32,  6.72it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 7/224 [00:01<00:32,  6.68it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 8/224 [00:01<00:32,  6.65it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 9/224 [00:01<00:32,  6.62it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 10/224 [00:01<00:32,  6.61it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 11/224 [00:01<00:32,  6.61it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 12/224 [00:01<00:32,  6.60it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 13/224 [00:01<00:31,  6.59it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 14/224 [00:02<00:31,  6.69it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 15/224 [00:02<00:30,  6.75it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 16/224 [00:02<00:31,  6.69it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 17/224 [00:02<00:31,  6.66it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 18/224 [00:02<00:31,  6.63it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 19/224 [00:02<00:30,  6.71it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 20/224 [00:02<00:30,  6.66it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 21/224 [00:03<00:30,  6.72it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 22/224 [00:03<00:29,  6.77it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 23/224 [00:03<00:30,  6.69it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 24/224 [00:03<00:30,  6.64it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 25/224 [00:03<00:30,  6.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 26/224 [00:03<00:29,  6.60it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 27/224 [00:04<00:29,  6.59it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 28/224 [00:04<00:29,  6.58it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 29/224 [00:04<00:29,  6.57it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 30/224 [00:04<00:29,  6.56it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 31/224 [00:04<00:29,  6.65it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 32/224 [00:04<00:29,  6.61it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 33/224 [00:04<00:28,  6.60it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 34/224 [00:05<00:28,  6.58it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 35/224 [00:05<00:28,  6.58it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 36/224 [00:05<00:28,  6.57it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 37/224 [00:05<00:28,  6.56it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 38/224 [00:05<00:28,  6.57it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 39/224 [00:05<00:28,  6.54it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 40/224 [00:06<00:27,  6.63it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 41/224 [00:06<00:27,  6.61it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 42/224 [00:06<00:27,  6.59it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 43/224 [00:06<00:27,  6.59it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 44/224 [00:06<00:27,  6.58it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 45/224 [00:06<00:27,  6.58it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 46/224 [00:06<00:27,  6.56it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 47/224 [00:07<00:27,  6.54it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 48/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 49/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 50/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 51/224 [00:07<00:26,  6.56it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 52/224 [00:07<00:26,  6.56it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 53/224 [00:07<00:25,  6.65it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 54/224 [00:08<00:25,  6.62it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 55/224 [00:08<00:25,  6.69it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 56/224 [00:08<00:24,  6.74it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 57/224 [00:08<00:24,  6.69it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 58/224 [00:08<00:24,  6.64it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 59/224 [00:08<00:24,  6.62it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 60/224 [00:09<00:24,  6.59it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 61/224 [00:09<00:24,  6.58it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 62/224 [00:09<00:24,  6.67it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 63/224 [00:09<00:23,  6.73it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 64/224 [00:09<00:23,  6.67it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 65/224 [00:09<00:23,  6.64it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 66/224 [00:09<00:23,  6.61it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 67/224 [00:10<00:23,  6.69it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 68/224 [00:10<00:23,  6.74it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 69/224 [00:10<00:23,  6.67it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 70/224 [00:10<00:23,  6.63it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 71/224 [00:10<00:23,  6.61it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 72/224 [00:10<00:22,  6.68it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 73/224 [00:10<00:22,  6.63it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 74/224 [00:11<00:22,  6.60it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 75/224 [00:11<00:22,  6.58it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 76/224 [00:11<00:22,  6.67it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 77/224 [00:11<00:22,  6.61it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 78/224 [00:11<00:22,  6.59it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 79/224 [00:11<00:22,  6.58it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 80/224 [00:12<00:21,  6.56it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 81/224 [00:12<00:21,  6.65it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 82/224 [00:12<00:21,  6.61it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 83/224 [00:12<00:21,  6.59it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 84/224 [00:12<00:21,  6.58it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 85/224 [00:12<00:21,  6.57it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 86/224 [00:12<00:21,  6.55it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 87/224 [00:13<00:20,  6.55it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 88/224 [00:13<00:20,  6.54it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 89/224 [00:13<00:20,  6.55it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 90/224 [00:13<00:20,  6.54it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 91/224 [00:13<00:20,  6.64it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 92/224 [00:13<00:19,  6.61it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 93/224 [00:14<00:19,  6.59it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 94/224 [00:14<00:19,  6.58it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 95/224 [00:14<00:19,  6.57it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 96/224 [00:14<00:19,  6.57it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 97/224 [00:14<00:19,  6.56it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 98/224 [00:14<00:18,  6.65it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 99/224 [00:14<00:18,  6.61it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 100/224 [00:15<00:18,  6.69it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 101/224 [00:15<00:18,  6.64it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 102/224 [00:15<00:18,  6.62it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 103/224 [00:15<00:18,  6.60it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 104/224 [00:15<00:18,  6.58it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 105/224 [00:15<00:18,  6.57it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 106/224 [00:16<00:18,  6.55it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 107/224 [00:16<00:17,  6.55it/s]\u001b[A\n",
            "Predicting batches:  48%|          | 108/224 [00:16<00:17,  6.54it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 109/224 [00:16<00:17,  6.54it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 110/224 [00:16<00:17,  6.64it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 111/224 [00:16<00:17,  6.61it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 112/224 [00:16<00:17,  6.59it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 113/224 [00:17<00:16,  6.66it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 114/224 [00:17<00:16,  6.62it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 115/224 [00:17<00:16,  6.69it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 116/224 [00:17<00:16,  6.64it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 117/224 [00:17<00:16,  6.60it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 118/224 [00:17<00:15,  6.68it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 119/224 [00:17<00:15,  6.73it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 120/224 [00:18<00:15,  6.67it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 121/224 [00:18<00:15,  6.72it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 122/224 [00:18<00:15,  6.64it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 123/224 [00:18<00:15,  6.61it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 124/224 [00:18<00:15,  6.57it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 125/224 [00:18<00:15,  6.55it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 126/224 [00:19<00:14,  6.55it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 127/224 [00:19<00:14,  6.54it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 128/224 [00:19<00:14,  6.64it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 129/224 [00:19<00:14,  6.59it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 130/224 [00:19<00:14,  6.57it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 131/224 [00:19<00:14,  6.56it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 132/224 [00:19<00:13,  6.65it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 133/224 [00:20<00:13,  6.71it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 134/224 [00:20<00:13,  6.65it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 135/224 [00:20<00:13,  6.61it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 136/224 [00:20<00:13,  6.68it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 137/224 [00:20<00:12,  6.73it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 138/224 [00:20<00:12,  6.67it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 139/224 [00:20<00:12,  6.60it/s]\u001b[A\n",
            "Predicting batches:  62%|       | 140/224 [00:21<00:12,  6.57it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 141/224 [00:21<00:12,  6.57it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 142/224 [00:21<00:12,  6.56it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 143/224 [00:21<00:12,  6.53it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 144/224 [00:21<00:12,  6.53it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 145/224 [00:21<00:12,  6.53it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 146/224 [00:22<00:11,  6.53it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 147/224 [00:22<00:11,  6.54it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 148/224 [00:22<00:11,  6.53it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 149/224 [00:22<00:11,  6.62it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 150/224 [00:22<00:11,  6.68it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 151/224 [00:22<00:10,  6.72it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 152/224 [00:22<00:10,  6.65it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 153/224 [00:23<00:10,  6.71it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 154/224 [00:23<00:10,  6.65it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 155/224 [00:23<00:10,  6.62it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 156/224 [00:23<00:10,  6.59it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 157/224 [00:23<00:10,  6.57it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 158/224 [00:23<00:10,  6.56it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 159/224 [00:24<00:09,  6.64it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 160/224 [00:24<00:09,  6.70it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 161/224 [00:24<00:09,  6.63it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 162/224 [00:24<00:09,  6.60it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 163/224 [00:24<00:09,  6.58it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 164/224 [00:24<00:09,  6.66it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 165/224 [00:24<00:08,  6.71it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 166/224 [00:25<00:08,  6.75it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 167/224 [00:25<00:08,  6.77it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 168/224 [00:25<00:08,  6.70it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 169/224 [00:25<00:08,  6.74it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 170/224 [00:25<00:08,  6.67it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 171/224 [00:25<00:08,  6.62it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 172/224 [00:25<00:07,  6.60it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 173/224 [00:26<00:07,  6.58it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 174/224 [00:26<00:07,  6.65it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 175/224 [00:26<00:07,  6.61it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 176/224 [00:26<00:07,  6.58it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 177/224 [00:26<00:07,  6.55it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 178/224 [00:26<00:07,  6.54it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 179/224 [00:27<00:06,  6.63it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 180/224 [00:27<00:06,  6.59it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 181/224 [00:27<00:06,  6.57it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 182/224 [00:27<00:06,  6.55it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 183/224 [00:27<00:06,  6.54it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 184/224 [00:27<00:06,  6.54it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 185/224 [00:27<00:05,  6.53it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 186/224 [00:28<00:05,  6.53it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 187/224 [00:28<00:05,  6.61it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 188/224 [00:28<00:05,  6.58it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 189/224 [00:28<00:05,  6.56it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 190/224 [00:28<00:05,  6.64it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 191/224 [00:28<00:05,  6.59it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 192/224 [00:29<00:04,  6.57it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 193/224 [00:29<00:04,  6.65it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 194/224 [00:29<00:04,  6.61it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 195/224 [00:29<00:04,  6.59it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 196/224 [00:29<00:04,  6.57it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 197/224 [00:29<00:04,  6.65it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 198/224 [00:29<00:03,  6.60it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 199/224 [00:30<00:03,  6.58it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 200/224 [00:30<00:03,  6.56it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 201/224 [00:30<00:03,  6.64it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 202/224 [00:30<00:03,  6.60it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 203/224 [00:30<00:03,  6.58it/s]\u001b[A\n",
            "Predicting batches:  91%| | 204/224 [00:30<00:03,  6.55it/s]\u001b[A\n",
            "Predicting batches:  92%| | 205/224 [00:30<00:02,  6.64it/s]\u001b[A\n",
            "Predicting batches:  92%| | 206/224 [00:31<00:02,  6.60it/s]\u001b[A\n",
            "Predicting batches:  92%| | 207/224 [00:31<00:02,  6.57it/s]\u001b[A\n",
            "Predicting batches:  93%| | 208/224 [00:31<00:02,  6.56it/s]\u001b[A\n",
            "Predicting batches:  93%| | 209/224 [00:31<00:02,  6.55it/s]\u001b[A\n",
            "Predicting batches:  94%| | 210/224 [00:31<00:02,  6.63it/s]\u001b[A\n",
            "Predicting batches:  94%| | 211/224 [00:31<00:01,  6.59it/s]\u001b[A\n",
            "Predicting batches:  95%| | 212/224 [00:32<00:01,  6.66it/s]\u001b[A\n",
            "Predicting batches:  95%| | 213/224 [00:32<00:01,  6.72it/s]\u001b[A\n",
            "Predicting batches:  96%| | 214/224 [00:32<00:01,  6.65it/s]\u001b[A\n",
            "Predicting batches:  96%|| 215/224 [00:32<00:01,  6.61it/s]\u001b[A\n",
            "Predicting batches:  96%|| 216/224 [00:32<00:01,  6.58it/s]\u001b[A\n",
            "Predicting batches:  97%|| 217/224 [00:32<00:01,  6.56it/s]\u001b[A\n",
            "Predicting batches:  97%|| 218/224 [00:32<00:00,  6.64it/s]\u001b[A\n",
            "Predicting batches:  98%|| 219/224 [00:33<00:00,  6.59it/s]\u001b[A\n",
            "Predicting batches:  98%|| 220/224 [00:33<00:00,  6.57it/s]\u001b[A\n",
            "Predicting batches:  99%|| 221/224 [00:33<00:00,  6.55it/s]\u001b[A\n",
            "Predicting batches:  99%|| 222/224 [00:33<00:00,  6.54it/s]\u001b[A\n",
            "Predicting batches: 100%|| 223/224 [00:33<00:00,  6.54it/s]\u001b[A\n",
            "Predicting batches: 100%|| 224/224 [00:33<00:00,  7.05it/s]\u001b[A\n",
            "Processing subjects:  77%| | 44/57 [05:59<03:36, 16.69s/it, moral scenarios]\u001b[A\n",
            "Generating test split: 306 examples [00:00, 46392.81 examples/s]\n",
            "\n",
            "Generating validation split: 33 examples [00:00, 18029.44 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3933.88 examples/s]\n",
            "Processing subjects:  77%|  | 44/57 [05:59<03:36, 16.69s/it, nutrition]\n",
            "Formatting batches:   0%|                               | 0/306 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  14%|                  | 43/306 [00:00<00:00, 424.49it/s]\u001b[A\n",
            "Formatting batches:  29%|               | 88/306 [00:00<00:00, 439.36it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 133/306 [00:00<00:00, 443.82it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 178/306 [00:00<00:00, 446.06it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 223/306 [00:00<00:00, 442.61it/s]\u001b[A\n",
            "Formatting batches:  88%|  | 268/306 [00:00<00:00, 438.02it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/77 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/77 [00:00<00:09,  7.72it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/77 [00:00<00:09,  7.81it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/77 [00:00<00:09,  7.52it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/77 [00:00<00:10,  7.10it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/77 [00:00<00:10,  7.05it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/77 [00:00<00:10,  7.01it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 7/77 [00:00<00:09,  7.21it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/77 [00:01<00:09,  7.19it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 9/77 [00:01<00:09,  7.18it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/77 [00:01<00:09,  7.12it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 11/77 [00:01<00:09,  7.08it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 12/77 [00:01<00:09,  7.03it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 13/77 [00:01<00:09,  7.07it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/77 [00:01<00:08,  7.23it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/77 [00:02<00:08,  7.27it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/77 [00:02<00:08,  7.29it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 17/77 [00:02<00:08,  7.40it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/77 [00:02<00:07,  7.48it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 19/77 [00:02<00:07,  7.56it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/77 [00:02<00:07,  7.44it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 21/77 [00:02<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 22/77 [00:03<00:07,  7.49it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 23/77 [00:03<00:07,  7.29it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 24/77 [00:03<00:07,  7.19it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 25/77 [00:03<00:07,  7.23it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 26/77 [00:03<00:07,  7.12it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 27/77 [00:03<00:06,  7.17it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/77 [00:03<00:06,  7.23it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 29/77 [00:03<00:06,  7.38it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 30/77 [00:04<00:06,  7.37it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 31/77 [00:04<00:06,  7.24it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 32/77 [00:04<00:06,  7.21it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 33/77 [00:04<00:06,  7.20it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 34/77 [00:04<00:06,  7.12it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 35/77 [00:04<00:05,  7.27it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 36/77 [00:04<00:05,  7.02it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 37/77 [00:05<00:05,  7.12it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/77 [00:05<00:05,  7.07it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 39/77 [00:05<00:05,  7.00it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 40/77 [00:05<00:05,  7.04it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 41/77 [00:05<00:04,  7.21it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 42/77 [00:05<00:04,  7.37it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 43/77 [00:05<00:04,  7.44it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 44/77 [00:06<00:04,  7.50it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 45/77 [00:06<00:04,  7.40it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 46/77 [00:06<00:04,  7.23it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 47/77 [00:06<00:04,  7.21it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/77 [00:06<00:04,  7.12it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 49/77 [00:06<00:03,  7.05it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 50/77 [00:06<00:03,  6.88it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 51/77 [00:07<00:03,  6.90it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 52/77 [00:07<00:03,  6.88it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 53/77 [00:07<00:03,  6.95it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 54/77 [00:07<00:03,  6.95it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 55/77 [00:07<00:03,  7.01it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 56/77 [00:07<00:02,  7.11it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 57/77 [00:07<00:02,  7.12it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 58/77 [00:08<00:02,  7.26it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 59/77 [00:08<00:02,  7.28it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 60/77 [00:08<00:02,  7.29it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 61/77 [00:08<00:02,  7.25it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 62/77 [00:08<00:02,  7.12it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 63/77 [00:08<00:01,  7.18it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 64/77 [00:08<00:01,  7.17it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 65/77 [00:09<00:01,  7.22it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 66/77 [00:09<00:01,  7.37it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 67/77 [00:09<00:01,  7.30it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 68/77 [00:09<00:01,  7.19it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 69/77 [00:09<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 70/77 [00:09<00:00,  7.07it/s]\u001b[A\n",
            "Predicting batches:  92%| | 71/77 [00:09<00:00,  7.10it/s]\u001b[A\n",
            "Predicting batches:  94%| | 72/77 [00:10<00:00,  7.06it/s]\u001b[A\n",
            "Predicting batches:  95%| | 73/77 [00:10<00:00,  7.01it/s]\u001b[A\n",
            "Predicting batches:  96%| | 74/77 [00:10<00:00,  7.10it/s]\u001b[A\n",
            "Predicting batches:  97%|| 75/77 [00:10<00:00,  7.17it/s]\u001b[A\n",
            "Predicting batches:  99%|| 76/77 [00:10<00:00,  7.21it/s]\u001b[A\n",
            "Processing subjects:  79%|  | 45/57 [06:10<03:01, 15.09s/it, nutrition]\u001b[A\n",
            "Generating test split: 311 examples [00:00, 44205.93 examples/s]\n",
            "\n",
            "Generating validation split: 34 examples [00:00, 18438.88 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4058.74 examples/s]\n",
            "Processing subjects:  79%|  | 45/57 [06:10<03:01, 15.09s/it, philosophy]\n",
            "Formatting batches:   0%|                               | 0/311 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 50/311 [00:00<00:00, 490.39it/s]\u001b[A\n",
            "Formatting batches:  32%|             | 100/311 [00:00<00:00, 492.90it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 151/311 [00:00<00:00, 497.51it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 202/311 [00:00<00:00, 500.56it/s]\u001b[A\n",
            "Formatting batches:  81%|   | 253/311 [00:00<00:00, 502.05it/s]\u001b[A\n",
            "Formatting batches:  98%|| 304/311 [00:00<00:00, 500.47it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/78 [00:00<00:05, 14.23it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/78 [00:00<00:05, 13.98it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/78 [00:00<00:05, 12.96it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/78 [00:00<00:05, 12.04it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/78 [00:00<00:05, 12.68it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/78 [00:00<00:05, 12.79it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/78 [00:01<00:05, 12.38it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/78 [00:01<00:04, 12.48it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/78 [00:01<00:04, 12.90it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/78 [00:01<00:04, 12.96it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 22/78 [00:01<00:04, 13.12it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 24/78 [00:01<00:04, 12.62it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 26/78 [00:02<00:04, 12.78it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/78 [00:02<00:03, 12.84it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 30/78 [00:02<00:03, 12.98it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 32/78 [00:02<00:03, 12.69it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 34/78 [00:02<00:03, 11.83it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 36/78 [00:02<00:03, 12.41it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/78 [00:03<00:03, 11.99it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 40/78 [00:03<00:03, 12.47it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 42/78 [00:03<00:02, 12.52it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 44/78 [00:03<00:02, 12.77it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 46/78 [00:03<00:02, 12.77it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/78 [00:03<00:02, 13.07it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 50/78 [00:03<00:02, 13.15it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 52/78 [00:04<00:01, 13.24it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 54/78 [00:04<00:01, 13.32it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 56/78 [00:04<00:01, 13.21it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 58/78 [00:04<00:01, 13.29it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 60/78 [00:04<00:01, 13.21it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 62/78 [00:04<00:01, 12.81it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 64/78 [00:05<00:01, 12.58it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 66/78 [00:05<00:00, 12.14it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 68/78 [00:05<00:00, 12.47it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 70/78 [00:05<00:00, 12.91it/s]\u001b[A\n",
            "Predicting batches:  92%| | 72/78 [00:05<00:00, 13.06it/s]\u001b[A\n",
            "Predicting batches:  95%| | 74/78 [00:05<00:00, 12.58it/s]\u001b[A\n",
            "Predicting batches:  97%|| 76/78 [00:05<00:00, 12.81it/s]\u001b[A\n",
            "Predicting batches: 100%|| 78/78 [00:06<00:00, 13.08it/s]\u001b[A\n",
            "Processing subjects:  81%|  | 46/57 [06:17<02:18, 12.58s/it, philosophy]\u001b[A\n",
            "Generating test split: 324 examples [00:00, 48191.58 examples/s]\n",
            "\n",
            "Generating validation split: 35 examples [00:00, 18893.26 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3959.88 examples/s]\n",
            "Processing subjects:  81%|  | 46/57 [06:17<02:18, 12.58s/it, prehistory]\n",
            "Formatting batches:   0%|                               | 0/324 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  14%|                  | 46/324 [00:00<00:00, 452.25it/s]\u001b[A\n",
            "Formatting batches:  28%|               | 92/324 [00:00<00:00, 455.29it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 138/324 [00:00<00:00, 447.46it/s]\u001b[A\n",
            "Formatting batches:  57%|        | 185/324 [00:00<00:00, 452.77it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 231/324 [00:00<00:00, 454.23it/s]\u001b[A\n",
            "Formatting batches:  86%|  | 278/324 [00:00<00:00, 456.76it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/81 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/81 [00:00<00:09,  8.66it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/81 [00:00<00:08,  8.84it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/81 [00:00<00:09,  8.28it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/81 [00:00<00:09,  8.18it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/81 [00:00<00:09,  8.41it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 6/81 [00:00<00:08,  8.57it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 7/81 [00:00<00:08,  8.42it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/81 [00:00<00:08,  8.55it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 9/81 [00:01<00:08,  8.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 10/81 [00:01<00:08,  8.69it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 11/81 [00:01<00:08,  8.47it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/81 [00:01<00:08,  8.56it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 13/81 [00:01<00:07,  8.64it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 14/81 [00:01<00:07,  8.48it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/81 [00:01<00:08,  8.25it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 16/81 [00:01<00:08,  8.06it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 17/81 [00:02<00:08,  7.83it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 18/81 [00:02<00:07,  8.13it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 19/81 [00:02<00:07,  8.09it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 20/81 [00:02<00:07,  8.28it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 21/81 [00:02<00:07,  8.41it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 22/81 [00:02<00:07,  8.26it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 23/81 [00:02<00:07,  8.08it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 24/81 [00:02<00:06,  8.28it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 25/81 [00:03<00:06,  8.19it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 26/81 [00:03<00:06,  8.13it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 27/81 [00:03<00:06,  8.29it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 28/81 [00:03<00:06,  8.09it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 29/81 [00:03<00:06,  8.06it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 30/81 [00:03<00:06,  7.94it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 31/81 [00:03<00:06,  7.99it/s]\u001b[A\n",
            "Predicting batches:  40%|              | 32/81 [00:03<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 33/81 [00:03<00:05,  8.24it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 34/81 [00:04<00:05,  8.18it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 35/81 [00:04<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 36/81 [00:04<00:05,  8.28it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 37/81 [00:04<00:05,  8.18it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 38/81 [00:04<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 39/81 [00:04<00:05,  8.17it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 40/81 [00:04<00:04,  8.36it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 41/81 [00:04<00:04,  8.29it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 42/81 [00:05<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 43/81 [00:05<00:04,  8.36it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 44/81 [00:05<00:04,  8.14it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 45/81 [00:05<00:04,  8.32it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 46/81 [00:05<00:04,  8.25it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 47/81 [00:05<00:04,  7.97it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 48/81 [00:05<00:04,  7.98it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 49/81 [00:05<00:04,  7.77it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 50/81 [00:06<00:03,  7.83it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 51/81 [00:06<00:03,  7.90it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 52/81 [00:06<00:03,  8.17it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 53/81 [00:06<00:03,  8.04it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 54/81 [00:06<00:03,  8.03it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 55/81 [00:06<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 56/81 [00:06<00:03,  7.97it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 57/81 [00:06<00:02,  8.19it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 58/81 [00:07<00:02,  8.34it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 59/81 [00:07<00:02,  8.15it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 60/81 [00:07<00:02,  8.13it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 61/81 [00:07<00:02,  8.13it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 62/81 [00:07<00:02,  8.32it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 63/81 [00:07<00:02,  8.46it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 64/81 [00:07<00:01,  8.54it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 65/81 [00:07<00:01,  8.40it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 66/81 [00:08<00:01,  8.52it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 67/81 [00:08<00:01,  8.59it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 68/81 [00:08<00:01,  8.43it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 69/81 [00:08<00:01,  8.29it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 70/81 [00:08<00:01,  8.46it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 71/81 [00:08<00:01,  8.32it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 72/81 [00:08<00:01,  8.24it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 73/81 [00:08<00:00,  8.20it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 74/81 [00:08<00:00,  8.38it/s]\u001b[A\n",
            "Predicting batches:  93%| | 75/81 [00:09<00:00,  8.51it/s]\u001b[A\n",
            "Predicting batches:  94%| | 76/81 [00:09<00:00,  8.61it/s]\u001b[A\n",
            "Predicting batches:  95%| | 77/81 [00:09<00:00,  8.67it/s]\u001b[A\n",
            "Predicting batches:  96%|| 78/81 [00:09<00:00,  8.70it/s]\u001b[A\n",
            "Predicting batches:  98%|| 79/81 [00:09<00:00,  8.81it/s]\u001b[A\n",
            "Predicting batches:  99%|| 80/81 [00:09<00:00,  8.57it/s]\u001b[A\n",
            "Predicting batches: 100%|| 81/81 [00:09<00:00,  8.38it/s]\u001b[A\n",
            "Processing subjects:  82%| | 47/57 [06:28<01:59, 11.96s/it, prehistory]\u001b[A\n",
            "Generating test split: 282 examples [00:00, 44140.68 examples/s]\n",
            "\n",
            "Generating validation split: 31 examples [00:00, 17242.20 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 4004.49 examples/s]\n",
            "Processing subjects:  82%|| 47/57 [06:28<01:59, 11.96s/it, professional account\n",
            "Formatting batches:   0%|                               | 0/282 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 43/282 [00:00<00:00, 421.64it/s]\u001b[A\n",
            "Formatting batches:  31%|              | 87/282 [00:00<00:00, 428.41it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 131/282 [00:00<00:00, 432.58it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 175/282 [00:00<00:00, 434.90it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 219/282 [00:00<00:00, 435.08it/s]\u001b[A\n",
            "Formatting batches:  93%| | 263/282 [00:00<00:00, 435.99it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/71 [00:00<00:10,  6.83it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/71 [00:00<00:10,  6.71it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/71 [00:00<00:10,  6.62it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/71 [00:00<00:10,  6.45it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/71 [00:00<00:10,  6.26it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/71 [00:00<00:10,  6.26it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/71 [00:01<00:10,  6.10it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 8/71 [00:01<00:10,  6.22it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 9/71 [00:01<00:09,  6.22it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 10/71 [00:01<00:09,  6.30it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 11/71 [00:01<00:09,  6.37it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 12/71 [00:01<00:09,  6.41it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 13/71 [00:02<00:09,  6.36it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 14/71 [00:02<00:08,  6.41it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 15/71 [00:02<00:08,  6.33it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 16/71 [00:02<00:08,  6.37it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 17/71 [00:02<00:08,  6.33it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 18/71 [00:02<00:08,  6.21it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 19/71 [00:03<00:08,  6.20it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 20/71 [00:03<00:08,  6.26it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 21/71 [00:03<00:07,  6.32it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 22/71 [00:03<00:07,  6.29it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 23/71 [00:03<00:07,  6.26it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 24/71 [00:03<00:07,  6.43it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 25/71 [00:03<00:07,  6.57it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 26/71 [00:04<00:07,  6.35it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 27/71 [00:04<00:06,  6.39it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 28/71 [00:04<00:06,  6.54it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 29/71 [00:04<00:06,  6.53it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 30/71 [00:04<00:06,  6.53it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 31/71 [00:04<00:06,  6.63it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 32/71 [00:04<00:05,  6.71it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 33/71 [00:05<00:05,  6.45it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 34/71 [00:05<00:05,  6.47it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 35/71 [00:05<00:05,  6.46it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 36/71 [00:05<00:05,  6.48it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 37/71 [00:05<00:05,  6.38it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 38/71 [00:05<00:05,  6.50it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 39/71 [00:06<00:05,  6.39it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 40/71 [00:06<00:04,  6.33it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 41/71 [00:06<00:04,  6.37it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 42/71 [00:06<00:04,  6.40it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 43/71 [00:06<00:04,  6.52it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 44/71 [00:06<00:04,  6.50it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 45/71 [00:07<00:04,  6.31it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 46/71 [00:07<00:03,  6.37it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 47/71 [00:07<00:03,  6.33it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 48/71 [00:07<00:03,  6.38it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 49/71 [00:07<00:03,  6.41it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 50/71 [00:07<00:03,  6.35it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 51/71 [00:07<00:03,  6.40it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 52/71 [00:08<00:02,  6.34it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 53/71 [00:08<00:02,  6.39it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 54/71 [00:08<00:02,  6.25it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 55/71 [00:08<00:02,  6.43it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 56/71 [00:08<00:02,  6.34it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 57/71 [00:08<00:02,  6.38it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 58/71 [00:09<00:02,  6.39it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 59/71 [00:09<00:01,  6.41it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 60/71 [00:09<00:01,  6.44it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 61/71 [00:09<00:01,  6.35it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 62/71 [00:09<00:01,  6.31it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 63/71 [00:09<00:01,  6.37it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 64/71 [00:10<00:01,  6.23it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 65/71 [00:10<00:00,  6.31it/s]\u001b[A\n",
            "Predicting batches:  93%| | 66/71 [00:10<00:00,  6.27it/s]\u001b[A\n",
            "Predicting batches:  94%| | 67/71 [00:10<00:00,  6.43it/s]\u001b[A\n",
            "Predicting batches:  96%| | 68/71 [00:10<00:00,  6.23it/s]\u001b[A\n",
            "Predicting batches:  97%|| 69/71 [00:10<00:00,  6.30it/s]\u001b[A\n",
            "Predicting batches:  99%|| 70/71 [00:10<00:00,  6.26it/s]\u001b[A\n",
            "Processing subjects:  84%|| 48/57 [06:39<01:47, 11.89s/it, professional account\u001b[A\n",
            "Generating test split: 1534 examples [00:00, 47686.21 examples/s]\n",
            "\n",
            "Generating validation split: 170 examples [00:00, 35195.80 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3244.86 examples/s]\n",
            "Processing subjects:  84%|| 48/57 [06:39<01:47, 11.89s/it, professional law]\n",
            "Formatting batches:   0%|                              | 0/1534 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   2%|                   | 32/1534 [00:00<00:04, 318.22it/s]\u001b[A\n",
            "Formatting batches:   4%|                   | 65/1534 [00:00<00:04, 320.27it/s]\u001b[A\n",
            "Formatting batches:   6%|                  | 98/1534 [00:00<00:04, 320.50it/s]\u001b[A\n",
            "Formatting batches:   9%|                 | 131/1534 [00:00<00:04, 322.20it/s]\u001b[A\n",
            "Formatting batches:  11%|                 | 164/1534 [00:00<00:04, 323.86it/s]\u001b[A\n",
            "Formatting batches:  13%|                | 197/1534 [00:00<00:04, 318.76it/s]\u001b[A\n",
            "Formatting batches:  15%|                | 229/1534 [00:00<00:04, 318.62it/s]\u001b[A\n",
            "Formatting batches:  17%|               | 262/1534 [00:00<00:03, 320.37it/s]\u001b[A\n",
            "Formatting batches:  19%|               | 295/1534 [00:00<00:03, 320.32it/s]\u001b[A\n",
            "Formatting batches:  21%|               | 328/1534 [00:01<00:03, 321.42it/s]\u001b[A\n",
            "Formatting batches:  24%|              | 361/1534 [00:01<00:03, 320.27it/s]\u001b[A\n",
            "Formatting batches:  26%|              | 394/1534 [00:01<00:03, 322.39it/s]\u001b[A\n",
            "Formatting batches:  28%|             | 427/1534 [00:01<00:03, 324.19it/s]\u001b[A\n",
            "Formatting batches:  30%|             | 460/1534 [00:01<00:03, 314.02it/s]\u001b[A\n",
            "Formatting batches:  32%|             | 492/1534 [00:01<00:03, 314.72it/s]\u001b[A\n",
            "Formatting batches:  34%|            | 524/1534 [00:01<00:03, 314.96it/s]\u001b[A\n",
            "Formatting batches:  36%|            | 556/1534 [00:01<00:03, 316.32it/s]\u001b[A\n",
            "Formatting batches:  38%|           | 589/1534 [00:01<00:02, 317.94it/s]\u001b[A\n",
            "Formatting batches:  40%|           | 621/1534 [00:01<00:02, 318.07it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 653/1534 [00:02<00:02, 317.49it/s]\u001b[A\n",
            "Formatting batches:  45%|          | 686/1534 [00:02<00:02, 318.91it/s]\u001b[A\n",
            "Formatting batches:  47%|          | 719/1534 [00:02<00:02, 320.74it/s]\u001b[A\n",
            "Formatting batches:  49%|         | 752/1534 [00:02<00:02, 320.57it/s]\u001b[A\n",
            "Formatting batches:  51%|         | 785/1534 [00:02<00:02, 319.68it/s]\u001b[A\n",
            "Formatting batches:  53%|         | 817/1534 [00:02<00:02, 317.51it/s]\u001b[A\n",
            "Formatting batches:  55%|        | 850/1534 [00:02<00:02, 318.28it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 883/1534 [00:02<00:02, 318.65it/s]\u001b[A\n",
            "Formatting batches:  60%|       | 916/1534 [00:02<00:01, 320.35it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 949/1534 [00:02<00:01, 321.07it/s]\u001b[A\n",
            "Formatting batches:  64%|      | 982/1534 [00:03<00:01, 322.07it/s]\u001b[A\n",
            "Formatting batches:  66%|      | 1015/1534 [00:03<00:01, 313.29it/s]\u001b[A\n",
            "Formatting batches:  68%|     | 1048/1534 [00:03<00:01, 316.07it/s]\u001b[A\n",
            "Formatting batches:  70%|     | 1080/1534 [00:03<00:01, 316.83it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 1113/1534 [00:03<00:01, 318.41it/s]\u001b[A\n",
            "Formatting batches:  75%|    | 1146/1534 [00:03<00:01, 320.01it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 1179/1534 [00:03<00:01, 320.08it/s]\u001b[A\n",
            "Formatting batches:  79%|   | 1212/1534 [00:03<00:01, 318.69it/s]\u001b[A\n",
            "Formatting batches:  81%|   | 1244/1534 [00:03<00:00, 318.79it/s]\u001b[A\n",
            "Formatting batches:  83%|   | 1276/1534 [00:04<00:00, 313.37it/s]\u001b[A\n",
            "Formatting batches:  85%|  | 1308/1534 [00:04<00:00, 314.84it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 1341/1534 [00:04<00:00, 317.63it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 1374/1534 [00:04<00:00, 319.62it/s]\u001b[A\n",
            "Formatting batches:  92%| | 1407/1534 [00:04<00:00, 320.75it/s]\u001b[A\n",
            "Formatting batches:  94%| | 1440/1534 [00:04<00:00, 320.97it/s]\u001b[A\n",
            "Formatting batches:  96%|| 1473/1534 [00:04<00:00, 322.58it/s]\u001b[A\n",
            "Formatting batches:  98%|| 1506/1534 [00:04<00:00, 322.58it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/384 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   0%|                       | 1/384 [00:00<02:44,  2.33it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 2/384 [00:00<02:45,  2.30it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 3/384 [00:01<02:54,  2.18it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 4/384 [00:01<02:51,  2.21it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 5/384 [00:02<02:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 6/384 [00:02<02:49,  2.23it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 7/384 [00:03<02:50,  2.21it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 8/384 [00:03<02:48,  2.23it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 9/384 [00:04<02:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 10/384 [00:04<02:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 11/384 [00:04<02:44,  2.27it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 12/384 [00:05<02:45,  2.25it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 13/384 [00:05<02:49,  2.19it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 14/384 [00:06<02:47,  2.21it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 15/384 [00:06<02:46,  2.21it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 16/384 [00:07<02:47,  2.20it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 17/384 [00:07<02:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 18/384 [00:08<02:55,  2.08it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 19/384 [00:08<02:47,  2.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 20/384 [00:09<02:47,  2.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 21/384 [00:09<02:49,  2.14it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 22/384 [00:09<02:42,  2.22it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 23/384 [00:10<02:48,  2.14it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 24/384 [00:10<02:45,  2.17it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 25/384 [00:11<02:52,  2.08it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 26/384 [00:11<02:46,  2.14it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 27/384 [00:12<02:42,  2.20it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 28/384 [00:12<02:40,  2.21it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 29/384 [00:13<02:39,  2.23it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 30/384 [00:13<02:35,  2.28it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 31/384 [00:14<02:36,  2.25it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 32/384 [00:14<02:38,  2.22it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 33/384 [00:14<02:37,  2.23it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 34/384 [00:15<02:33,  2.28it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 35/384 [00:15<02:36,  2.24it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 36/384 [00:16<02:32,  2.29it/s]\u001b[A\n",
            "Predicting batches:  10%|                    | 37/384 [00:16<02:32,  2.27it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 38/384 [00:17<02:37,  2.20it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 39/384 [00:17<02:35,  2.21it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 40/384 [00:18<02:34,  2.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 41/384 [00:18<02:32,  2.25it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 42/384 [00:19<02:34,  2.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 43/384 [00:19<02:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 44/384 [00:19<02:36,  2.17it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 45/384 [00:20<02:32,  2.22it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 46/384 [00:20<02:29,  2.26it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 47/384 [00:21<02:33,  2.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 48/384 [00:21<02:30,  2.24it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 49/384 [00:22<02:35,  2.15it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 50/384 [00:22<02:31,  2.20it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 51/384 [00:23<02:32,  2.18it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 52/384 [00:23<02:31,  2.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 53/384 [00:23<02:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 54/384 [00:24<02:28,  2.22it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 55/384 [00:24<02:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 56/384 [00:25<02:27,  2.22it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 57/384 [00:25<02:25,  2.25it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 58/384 [00:26<02:23,  2.27it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 59/384 [00:26<02:22,  2.29it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 60/384 [00:27<02:26,  2.21it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 61/384 [00:27<02:24,  2.24it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 62/384 [00:28<02:35,  2.07it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 63/384 [00:28<02:31,  2.12it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 64/384 [00:29<02:29,  2.14it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 65/384 [00:29<02:30,  2.12it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 66/384 [00:29<02:25,  2.18it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 67/384 [00:30<02:22,  2.22it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 68/384 [00:30<02:21,  2.23it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 69/384 [00:31<02:26,  2.15it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 70/384 [00:31<02:23,  2.18it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 71/384 [00:32<02:24,  2.17it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 72/384 [00:32<02:29,  2.08it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 73/384 [00:33<02:27,  2.11it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 74/384 [00:33<02:21,  2.19it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 75/384 [00:34<02:20,  2.20it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 76/384 [00:34<02:16,  2.26it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 77/384 [00:34<02:14,  2.28it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 78/384 [00:35<02:13,  2.29it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 79/384 [00:35<02:17,  2.21it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 80/384 [00:36<02:22,  2.14it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 81/384 [00:36<02:21,  2.15it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 82/384 [00:37<02:23,  2.10it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 83/384 [00:37<02:24,  2.09it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 84/384 [00:38<02:25,  2.06it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 85/384 [00:38<02:21,  2.11it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 86/384 [00:39<02:17,  2.17it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 87/384 [00:39<02:14,  2.22it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 88/384 [00:40<02:15,  2.19it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 89/384 [00:40<02:13,  2.21it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 90/384 [00:41<02:18,  2.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 91/384 [00:41<02:15,  2.16it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 92/384 [00:41<02:13,  2.19it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 93/384 [00:42<02:10,  2.23it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 94/384 [00:42<02:11,  2.20it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 95/384 [00:43<02:09,  2.24it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 96/384 [00:43<02:09,  2.23it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 97/384 [00:44<02:09,  2.21it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 98/384 [00:44<02:07,  2.24it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 99/384 [00:45<02:08,  2.22it/s]\u001b[A\n",
            "Predicting batches:  26%|               | 100/384 [00:45<02:08,  2.21it/s]\u001b[A\n",
            "Predicting batches:  26%|               | 101/384 [00:45<02:07,  2.23it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 102/384 [00:46<02:04,  2.26it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 103/384 [00:46<02:04,  2.26it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 104/384 [00:47<02:05,  2.24it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 105/384 [00:47<02:02,  2.27it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 106/384 [00:48<02:02,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 107/384 [00:48<02:02,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 108/384 [00:49<02:02,  2.25it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 109/384 [00:49<02:02,  2.24it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 110/384 [00:49<02:03,  2.21it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 111/384 [00:50<02:02,  2.23it/s]\u001b[A\n",
            "Predicting batches:  29%|              | 112/384 [00:50<02:06,  2.14it/s]\u001b[A\n",
            "Predicting batches:  29%|              | 113/384 [00:51<02:09,  2.09it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 114/384 [00:51<02:06,  2.14it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 115/384 [00:52<02:05,  2.14it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 116/384 [00:52<02:02,  2.19it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 117/384 [00:53<02:02,  2.17it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 118/384 [00:53<02:01,  2.20it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 119/384 [00:54<01:58,  2.23it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 120/384 [00:54<01:59,  2.21it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 121/384 [00:55<02:00,  2.19it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 122/384 [00:55<01:57,  2.22it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 123/384 [00:55<01:56,  2.25it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 124/384 [00:56<01:57,  2.21it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 125/384 [00:56<01:55,  2.25it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 126/384 [00:57<01:53,  2.27it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 127/384 [00:57<01:56,  2.20it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 128/384 [00:58<01:56,  2.19it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 129/384 [00:58<01:54,  2.22it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 130/384 [00:59<01:53,  2.25it/s]\u001b[A\n",
            "Predicting batches:  34%|             | 131/384 [00:59<01:51,  2.27it/s]\u001b[A\n",
            "Predicting batches:  34%|             | 132/384 [00:59<01:51,  2.26it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 133/384 [01:00<01:50,  2.28it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 134/384 [01:00<01:48,  2.30it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 135/384 [01:01<01:48,  2.30it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 136/384 [01:01<01:48,  2.28it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 137/384 [01:02<01:49,  2.26it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 138/384 [01:02<01:50,  2.23it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 139/384 [01:02<01:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 140/384 [01:03<01:51,  2.19it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 141/384 [01:03<01:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 142/384 [01:04<01:49,  2.22it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 143/384 [01:04<01:52,  2.13it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 144/384 [01:05<01:52,  2.14it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 145/384 [01:05<01:50,  2.16it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 146/384 [01:06<01:50,  2.16it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 147/384 [01:06<01:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 148/384 [01:07<01:48,  2.17it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 149/384 [01:07<01:46,  2.22it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 150/384 [01:08<01:44,  2.25it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 151/384 [01:08<01:46,  2.19it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 152/384 [01:08<01:45,  2.19it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 153/384 [01:09<01:45,  2.18it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 154/384 [01:09<01:47,  2.13it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 155/384 [01:10<01:48,  2.10it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 156/384 [01:10<01:46,  2.13it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 157/384 [01:11<01:47,  2.11it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 158/384 [01:11<01:44,  2.16it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 159/384 [01:12<01:46,  2.11it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 160/384 [01:12<01:43,  2.16it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 161/384 [01:13<01:40,  2.21it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 162/384 [01:13<01:41,  2.19it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 163/384 [01:14<01:41,  2.18it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 164/384 [01:14<01:42,  2.14it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 165/384 [01:15<01:40,  2.17it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 166/384 [01:15<01:40,  2.17it/s]\u001b[A\n",
            "Predicting batches:  43%|           | 167/384 [01:15<01:42,  2.12it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 168/384 [01:16<01:40,  2.15it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 169/384 [01:16<01:37,  2.20it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 170/384 [01:17<01:39,  2.16it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 171/384 [01:17<01:36,  2.21it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 172/384 [01:18<01:36,  2.20it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 173/384 [01:18<01:36,  2.18it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 174/384 [01:19<01:35,  2.20it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 175/384 [01:19<01:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 176/384 [01:20<01:32,  2.25it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 177/384 [01:20<01:34,  2.19it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 178/384 [01:20<01:32,  2.22it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 179/384 [01:21<01:32,  2.21it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 180/384 [01:21<01:31,  2.24it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 181/384 [01:22<01:31,  2.23it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 182/384 [01:22<01:30,  2.23it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 183/384 [01:23<01:30,  2.22it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 184/384 [01:23<01:31,  2.19it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 185/384 [01:24<01:30,  2.21it/s]\u001b[A\n",
            "Predicting batches:  48%|          | 186/384 [01:24<01:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 187/384 [01:24<01:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 188/384 [01:25<01:29,  2.18it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 189/384 [01:25<01:28,  2.20it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 190/384 [01:26<01:30,  2.14it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 191/384 [01:26<01:28,  2.19it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 192/384 [01:27<01:27,  2.20it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 193/384 [01:27<01:28,  2.15it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 194/384 [01:28<01:27,  2.16it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 195/384 [01:28<01:25,  2.20it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 196/384 [01:29<01:23,  2.26it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 197/384 [01:29<01:25,  2.19it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 198/384 [01:30<01:25,  2.18it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 199/384 [01:30<01:23,  2.22it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 200/384 [01:30<01:26,  2.14it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 201/384 [01:31<01:25,  2.14it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 202/384 [01:31<01:22,  2.20it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 203/384 [01:32<01:21,  2.21it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 204/384 [01:32<01:23,  2.16it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 205/384 [01:33<01:21,  2.20it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 206/384 [01:33<01:21,  2.18it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 207/384 [01:34<01:22,  2.14it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 208/384 [01:34<01:20,  2.19it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 209/384 [01:35<01:20,  2.18it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 210/384 [01:35<01:17,  2.24it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 211/384 [01:35<01:17,  2.24it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 212/384 [01:36<01:17,  2.21it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 213/384 [01:36<01:17,  2.20it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 214/384 [01:37<01:16,  2.23it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 215/384 [01:37<01:19,  2.12it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 216/384 [01:38<01:17,  2.18it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 217/384 [01:38<01:16,  2.17it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 218/384 [01:39<01:14,  2.22it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 219/384 [01:39<01:12,  2.27it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 220/384 [01:40<01:11,  2.28it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 221/384 [01:40<01:12,  2.24it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 222/384 [01:40<01:12,  2.23it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 223/384 [01:41<01:11,  2.25it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 224/384 [01:41<01:10,  2.28it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 225/384 [01:42<01:12,  2.21it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 226/384 [01:42<01:11,  2.20it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 227/384 [01:43<01:10,  2.21it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 228/384 [01:43<01:08,  2.27it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 229/384 [01:44<01:07,  2.29it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 230/384 [01:44<01:09,  2.22it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 231/384 [01:44<01:08,  2.22it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 232/384 [01:45<01:07,  2.25it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 233/384 [01:45<01:06,  2.27it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 234/384 [01:46<01:06,  2.26it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 235/384 [01:46<01:06,  2.23it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 236/384 [01:47<01:06,  2.22it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 237/384 [01:47<01:05,  2.26it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 238/384 [01:48<01:09,  2.11it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 239/384 [01:48<01:09,  2.09it/s]\u001b[A\n",
            "Predicting batches:  62%|       | 240/384 [01:49<01:07,  2.12it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 241/384 [01:49<01:07,  2.13it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 242/384 [01:50<01:05,  2.16it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 243/384 [01:50<01:04,  2.20it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 244/384 [01:50<01:05,  2.13it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 245/384 [01:51<01:03,  2.20it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 246/384 [01:51<01:02,  2.21it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 247/384 [01:52<01:01,  2.22it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 248/384 [01:52<01:00,  2.25it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 249/384 [01:53<01:00,  2.23it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 250/384 [01:53<01:03,  2.10it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 251/384 [01:54<01:02,  2.12it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 252/384 [01:54<01:03,  2.08it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 253/384 [01:55<01:01,  2.14it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 254/384 [01:55<01:01,  2.11it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 255/384 [01:56<01:00,  2.15it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 256/384 [01:56<01:00,  2.11it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 257/384 [01:56<00:58,  2.16it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 258/384 [01:57<00:56,  2.22it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 259/384 [01:57<00:57,  2.19it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 260/384 [01:58<00:55,  2.22it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 261/384 [01:58<00:56,  2.19it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 262/384 [01:59<00:54,  2.23it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 263/384 [01:59<00:53,  2.26it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 264/384 [02:00<00:53,  2.24it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 265/384 [02:00<00:53,  2.21it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 266/384 [02:00<00:53,  2.19it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 267/384 [02:01<00:55,  2.11it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 268/384 [02:01<00:54,  2.13it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 269/384 [02:02<00:54,  2.10it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 270/384 [02:02<00:54,  2.10it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 271/384 [02:03<00:53,  2.12it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 272/384 [02:03<00:51,  2.16it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 273/384 [02:04<00:52,  2.11it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 274/384 [02:04<00:51,  2.14it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 275/384 [02:05<00:51,  2.13it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 276/384 [02:05<00:50,  2.15it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 277/384 [02:06<00:49,  2.17it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 278/384 [02:06<00:48,  2.18it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 279/384 [02:07<00:48,  2.18it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 280/384 [02:07<00:48,  2.13it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 281/384 [02:08<00:48,  2.13it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 282/384 [02:08<00:47,  2.15it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 283/384 [02:08<00:46,  2.19it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 284/384 [02:09<00:44,  2.23it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 285/384 [02:09<00:43,  2.27it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 286/384 [02:10<00:45,  2.17it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 287/384 [02:10<00:43,  2.21it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 288/384 [02:11<00:43,  2.19it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 289/384 [02:11<00:43,  2.18it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 290/384 [02:12<00:42,  2.22it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 291/384 [02:12<00:41,  2.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 292/384 [02:12<00:41,  2.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 293/384 [02:13<00:40,  2.26it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 294/384 [02:13<00:39,  2.27it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 295/384 [02:14<00:39,  2.25it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 296/384 [02:14<00:38,  2.26it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 297/384 [02:15<00:38,  2.29it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 298/384 [02:15<00:37,  2.27it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 299/384 [02:16<00:37,  2.28it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 300/384 [02:16<00:36,  2.28it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 301/384 [02:16<00:37,  2.24it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 302/384 [02:17<00:36,  2.25it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 303/384 [02:17<00:38,  2.12it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 304/384 [02:18<00:38,  2.05it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 305/384 [02:18<00:38,  2.04it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 306/384 [02:19<00:36,  2.14it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 307/384 [02:19<00:35,  2.17it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 308/384 [02:20<00:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 309/384 [02:20<00:33,  2.22it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 310/384 [02:21<00:33,  2.19it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 311/384 [02:21<00:33,  2.19it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 312/384 [02:22<00:34,  2.10it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 313/384 [02:22<00:33,  2.14it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 314/384 [02:23<00:32,  2.17it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 315/384 [02:23<00:31,  2.20it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 316/384 [02:23<00:30,  2.24it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 317/384 [02:24<00:29,  2.26it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 318/384 [02:24<00:29,  2.23it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 319/384 [02:25<00:28,  2.25it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 320/384 [02:25<00:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 321/384 [02:26<00:28,  2.21it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 322/384 [02:26<00:28,  2.21it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 323/384 [02:27<00:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 324/384 [02:27<00:29,  2.06it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 325/384 [02:28<00:28,  2.04it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 326/384 [02:28<00:27,  2.08it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 327/384 [02:28<00:26,  2.17it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 328/384 [02:29<00:25,  2.19it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 329/384 [02:29<00:24,  2.20it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 330/384 [02:30<00:24,  2.18it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 331/384 [02:30<00:24,  2.16it/s]\u001b[A\n",
            "Predicting batches:  86%|  | 332/384 [02:31<00:24,  2.15it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 333/384 [02:31<00:24,  2.09it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 334/384 [02:32<00:23,  2.10it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 335/384 [02:32<00:22,  2.18it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 336/384 [02:33<00:22,  2.11it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 337/384 [02:33<00:22,  2.06it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 338/384 [02:34<00:21,  2.15it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 339/384 [02:34<00:20,  2.16it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 340/384 [02:35<00:20,  2.16it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 341/384 [02:35<00:19,  2.18it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 342/384 [02:35<00:19,  2.20it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 343/384 [02:36<00:18,  2.23it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 344/384 [02:36<00:17,  2.25it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 345/384 [02:37<00:17,  2.26it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 346/384 [02:37<00:17,  2.20it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 347/384 [02:38<00:16,  2.19it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 348/384 [02:38<00:16,  2.19it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 349/384 [02:39<00:15,  2.21it/s]\u001b[A\n",
            "Predicting batches:  91%| | 350/384 [02:39<00:15,  2.22it/s]\u001b[A\n",
            "Predicting batches:  91%| | 351/384 [02:40<00:15,  2.19it/s]\u001b[A\n",
            "Predicting batches:  92%| | 352/384 [02:40<00:14,  2.14it/s]\u001b[A\n",
            "Predicting batches:  92%| | 353/384 [02:40<00:14,  2.16it/s]\u001b[A\n",
            "Predicting batches:  92%| | 354/384 [02:41<00:13,  2.17it/s]\u001b[A\n",
            "Predicting batches:  92%| | 355/384 [02:41<00:13,  2.11it/s]\u001b[A\n",
            "Predicting batches:  93%| | 356/384 [02:42<00:13,  2.14it/s]\u001b[A\n",
            "Predicting batches:  93%| | 357/384 [02:42<00:12,  2.16it/s]\u001b[A\n",
            "Predicting batches:  93%| | 358/384 [02:43<00:11,  2.20it/s]\u001b[A\n",
            "Predicting batches:  93%| | 359/384 [02:43<00:11,  2.20it/s]\u001b[A\n",
            "Predicting batches:  94%| | 360/384 [02:44<00:10,  2.20it/s]\u001b[A\n",
            "Predicting batches:  94%| | 361/384 [02:44<00:10,  2.15it/s]\u001b[A\n",
            "Predicting batches:  94%| | 362/384 [02:45<00:10,  2.20it/s]\u001b[A\n",
            "Predicting batches:  95%| | 363/384 [02:45<00:09,  2.23it/s]\u001b[A\n",
            "Predicting batches:  95%| | 364/384 [02:45<00:08,  2.24it/s]\u001b[A\n",
            "Predicting batches:  95%| | 365/384 [02:46<00:08,  2.26it/s]\u001b[A\n",
            "Predicting batches:  95%| | 366/384 [02:46<00:08,  2.24it/s]\u001b[A\n",
            "Predicting batches:  96%| | 367/384 [02:47<00:07,  2.24it/s]\u001b[A\n",
            "Predicting batches:  96%|| 368/384 [02:47<00:07,  2.26it/s]\u001b[A\n",
            "Predicting batches:  96%|| 369/384 [02:48<00:06,  2.23it/s]\u001b[A\n",
            "Predicting batches:  96%|| 370/384 [02:48<00:06,  2.28it/s]\u001b[A\n",
            "Predicting batches:  97%|| 371/384 [02:49<00:05,  2.31it/s]\u001b[A\n",
            "Predicting batches:  97%|| 372/384 [02:49<00:05,  2.10it/s]\u001b[A\n",
            "Predicting batches:  97%|| 373/384 [02:50<00:05,  2.16it/s]\u001b[A\n",
            "Predicting batches:  97%|| 374/384 [02:50<00:04,  2.13it/s]\u001b[A\n",
            "Predicting batches:  98%|| 375/384 [02:50<00:04,  2.13it/s]\u001b[A\n",
            "Predicting batches:  98%|| 376/384 [02:51<00:03,  2.18it/s]\u001b[A\n",
            "Predicting batches:  98%|| 377/384 [02:51<00:03,  2.13it/s]\u001b[A\n",
            "Predicting batches:  98%|| 378/384 [02:52<00:02,  2.15it/s]\u001b[A\n",
            "Predicting batches:  99%|| 379/384 [02:52<00:02,  2.17it/s]\u001b[A\n",
            "Predicting batches:  99%|| 380/384 [02:53<00:01,  2.21it/s]\u001b[A\n",
            "Predicting batches:  99%|| 381/384 [02:53<00:01,  2.16it/s]\u001b[A\n",
            "Predicting batches:  99%|| 382/384 [02:54<00:00,  2.17it/s]\u001b[A\n",
            "Predicting batches: 100%|| 383/384 [02:54<00:00,  2.19it/s]\u001b[A\n",
            "Predicting batches: 100%|| 384/384 [02:54<00:00,  2.60it/s]\u001b[A\n",
            "Processing subjects:  86%|| 49/57 [09:39<08:17, 62.25s/it, professional law]\u001b[A\n",
            "Generating test split: 272 examples [00:00, 34105.13 examples/s]\n",
            "\n",
            "Generating validation split: 31 examples [00:00, 15981.25 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3959.13 examples/s]\n",
            "Processing subjects:  86%|| 49/57 [09:39<08:17, 62.25s/it, professional medicin\n",
            "Formatting batches:   0%|                               | 0/272 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  12%|                  | 33/272 [00:00<00:00, 329.47it/s]\u001b[A\n",
            "Formatting batches:  26%|               | 71/272 [00:00<00:00, 356.00it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 109/272 [00:00<00:00, 365.34it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 147/272 [00:00<00:00, 368.21it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 185/272 [00:00<00:00, 372.02it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 223/272 [00:00<00:00, 373.39it/s]\u001b[A\n",
            "Formatting batches:  96%|| 261/272 [00:00<00:00, 374.74it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/68 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/68 [00:00<00:18,  3.68it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/68 [00:00<00:18,  3.54it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/68 [00:00<00:18,  3.59it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/68 [00:01<00:17,  3.67it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/68 [00:01<00:18,  3.41it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 6/68 [00:01<00:18,  3.33it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/68 [00:02<00:17,  3.43it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/68 [00:02<00:17,  3.47it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 9/68 [00:02<00:17,  3.37it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/68 [00:02<00:16,  3.43it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 11/68 [00:03<00:16,  3.49it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/68 [00:03<00:15,  3.58it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 13/68 [00:03<00:15,  3.58it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/68 [00:04<00:15,  3.46it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 15/68 [00:04<00:15,  3.49it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/68 [00:04<00:14,  3.47it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 17/68 [00:04<00:14,  3.43it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 18/68 [00:05<00:14,  3.49it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 19/68 [00:05<00:13,  3.58it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 20/68 [00:05<00:13,  3.66it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 21/68 [00:05<00:12,  3.71it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 22/68 [00:06<00:12,  3.81it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 23/68 [00:06<00:12,  3.73it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 24/68 [00:06<00:11,  3.67it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 25/68 [00:07<00:12,  3.56it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 26/68 [00:07<00:11,  3.56it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 27/68 [00:07<00:11,  3.51it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 28/68 [00:07<00:11,  3.43it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 29/68 [00:08<00:11,  3.50it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 30/68 [00:08<00:10,  3.47it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 31/68 [00:08<00:10,  3.50it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 32/68 [00:09<00:10,  3.59it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 33/68 [00:09<00:10,  3.47it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 34/68 [00:09<00:09,  3.51it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 35/68 [00:09<00:09,  3.42it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 36/68 [00:10<00:09,  3.52it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 37/68 [00:10<00:08,  3.48it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 38/68 [00:10<00:08,  3.53it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 39/68 [00:11<00:08,  3.49it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 40/68 [00:11<00:08,  3.41it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 41/68 [00:11<00:07,  3.47it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 42/68 [00:11<00:07,  3.53it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 43/68 [00:12<00:07,  3.56it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 44/68 [00:12<00:06,  3.51it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 45/68 [00:12<00:06,  3.39it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 46/68 [00:13<00:06,  3.43it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 47/68 [00:13<00:06,  3.38it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 48/68 [00:13<00:06,  3.31it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 49/68 [00:14<00:05,  3.40it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 50/68 [00:14<00:05,  3.32it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 51/68 [00:14<00:05,  3.31it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 52/68 [00:14<00:04,  3.41it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 53/68 [00:15<00:04,  3.37it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 54/68 [00:15<00:04,  3.35it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 55/68 [00:15<00:03,  3.43it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 56/68 [00:16<00:03,  3.50it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 57/68 [00:16<00:03,  3.51it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 58/68 [00:16<00:02,  3.55it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 59/68 [00:16<00:02,  3.50it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 60/68 [00:17<00:02,  3.44it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 61/68 [00:17<00:02,  3.43it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 62/68 [00:17<00:01,  3.39it/s]\u001b[A\n",
            "Predicting batches:  93%| | 63/68 [00:18<00:01,  3.39it/s]\u001b[A\n",
            "Predicting batches:  94%| | 64/68 [00:18<00:01,  3.46it/s]\u001b[A\n",
            "Predicting batches:  96%| | 65/68 [00:18<00:00,  3.49it/s]\u001b[A\n",
            "Predicting batches:  97%|| 66/68 [00:18<00:00,  3.52it/s]\u001b[A\n",
            "Predicting batches:  99%|| 67/68 [00:19<00:00,  3.59it/s]\u001b[A\n",
            "Predicting batches: 100%|| 68/68 [00:19<00:00,  3.61it/s]\u001b[A\n",
            "Processing subjects:  88%|| 50/57 [09:59<05:47, 49.64s/it, professional medicin\u001b[A\n",
            "Generating test split: 612 examples [00:00, 53624.84 examples/s]\n",
            "\n",
            "Generating validation split: 69 examples [00:00, 29588.69 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3642.15 examples/s]\n",
            "Processing subjects:  88%|| 50/57 [09:59<05:47, 49.64s/it, professional psychol\n",
            "Formatting batches:   0%|                               | 0/612 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   7%|                   | 45/612 [00:00<00:01, 446.32it/s]\u001b[A\n",
            "Formatting batches:  15%|                  | 91/612 [00:00<00:01, 449.61it/s]\u001b[A\n",
            "Formatting batches:  22%|               | 136/612 [00:00<00:01, 448.36it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 181/612 [00:00<00:00, 448.56it/s]\u001b[A\n",
            "Formatting batches:  37%|            | 226/612 [00:00<00:00, 445.08it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 272/612 [00:00<00:00, 446.86it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 317/612 [00:00<00:00, 447.67it/s]\u001b[A\n",
            "Formatting batches:  59%|        | 363/612 [00:00<00:00, 448.63it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 409/612 [00:00<00:00, 449.35it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 454/612 [00:01<00:00, 449.23it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 499/612 [00:01<00:00, 448.73it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 544/612 [00:01<00:00, 448.52it/s]\u001b[A\n",
            "Formatting batches:  96%|| 590/612 [00:01<00:00, 449.93it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/153 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 1/153 [00:00<00:20,  7.58it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/153 [00:00<00:19,  7.71it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 3/153 [00:00<00:21,  7.13it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 4/153 [00:00<00:20,  7.16it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 5/153 [00:00<00:20,  7.05it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 6/153 [00:00<00:20,  7.03it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 7/153 [00:00<00:20,  7.08it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 8/153 [00:01<00:20,  7.15it/s]\u001b[A\n",
            "Predicting batches:   6%|                     | 9/153 [00:01<00:19,  7.39it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 10/153 [00:01<00:19,  7.31it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 11/153 [00:01<00:19,  7.17it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 12/153 [00:01<00:19,  7.08it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 13/153 [00:01<00:19,  7.10it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 14/153 [00:01<00:19,  7.17it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 15/153 [00:02<00:19,  7.10it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 16/153 [00:02<00:19,  6.91it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 17/153 [00:02<00:19,  7.04it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 18/153 [00:02<00:19,  6.88it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 19/153 [00:02<00:19,  6.88it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 20/153 [00:02<00:18,  7.09it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 21/153 [00:02<00:18,  7.24it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 22/153 [00:03<00:18,  7.15it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 23/153 [00:03<00:18,  7.15it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 24/153 [00:03<00:17,  7.30it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 25/153 [00:03<00:17,  7.23it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 26/153 [00:03<00:17,  7.43it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 27/153 [00:03<00:17,  7.27it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 28/153 [00:03<00:16,  7.40it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 29/153 [00:04<00:16,  7.51it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 30/153 [00:04<00:16,  7.57it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 31/153 [00:04<00:16,  7.59it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 32/153 [00:04<00:15,  7.60it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 33/153 [00:04<00:16,  7.21it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 34/153 [00:04<00:16,  7.13it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 35/153 [00:04<00:16,  7.18it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 36/153 [00:04<00:15,  7.32it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 37/153 [00:05<00:15,  7.44it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 38/153 [00:05<00:15,  7.50it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 39/153 [00:05<00:15,  7.39it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 40/153 [00:05<00:15,  7.38it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 41/153 [00:05<00:15,  7.46it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 42/153 [00:05<00:15,  7.35it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 43/153 [00:05<00:15,  7.18it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 44/153 [00:06<00:15,  7.20it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 45/153 [00:06<00:14,  7.31it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 46/153 [00:06<00:14,  7.44it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 47/153 [00:06<00:14,  7.35it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 48/153 [00:06<00:14,  7.18it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 49/153 [00:06<00:14,  7.16it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 50/153 [00:06<00:14,  7.21it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 51/153 [00:07<00:13,  7.34it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 52/153 [00:07<00:14,  7.18it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 53/153 [00:07<00:13,  7.21it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 54/153 [00:07<00:13,  7.23it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 55/153 [00:07<00:13,  7.20it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 56/153 [00:07<00:13,  7.11it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 57/153 [00:07<00:13,  7.16it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 58/153 [00:08<00:13,  7.22it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 59/153 [00:08<00:13,  7.19it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 60/153 [00:08<00:12,  7.39it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 61/153 [00:08<00:12,  7.36it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 62/153 [00:08<00:12,  7.35it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 63/153 [00:08<00:12,  7.35it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 64/153 [00:08<00:11,  7.47it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 65/153 [00:08<00:11,  7.43it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 66/153 [00:09<00:11,  7.47it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 67/153 [00:09<00:11,  7.55it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 68/153 [00:09<00:11,  7.58it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 69/153 [00:09<00:11,  7.35it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 70/153 [00:09<00:11,  7.27it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 71/153 [00:09<00:11,  7.02it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 72/153 [00:09<00:11,  7.18it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 73/153 [00:10<00:11,  7.07it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 74/153 [00:10<00:10,  7.22it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 75/153 [00:10<00:10,  7.36it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 76/153 [00:10<00:10,  7.34it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 77/153 [00:10<00:10,  7.43it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 78/153 [00:10<00:10,  7.27it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 79/153 [00:10<00:10,  7.00it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 80/153 [00:11<00:10,  7.20it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 81/153 [00:11<00:09,  7.23it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 82/153 [00:11<00:09,  7.26it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 83/153 [00:11<00:10,  6.78it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 84/153 [00:11<00:09,  7.04it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 85/153 [00:11<00:09,  7.23it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 86/153 [00:11<00:09,  7.36it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 87/153 [00:11<00:09,  7.28it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 88/153 [00:12<00:08,  7.22it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 89/153 [00:12<00:08,  7.24it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 90/153 [00:12<00:08,  7.26it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 91/153 [00:12<00:08,  7.26it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 92/153 [00:12<00:08,  7.38it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 93/153 [00:12<00:08,  7.44it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 94/153 [00:12<00:08,  7.33it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 95/153 [00:13<00:07,  7.32it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 96/153 [00:13<00:07,  7.42it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 97/153 [00:13<00:07,  7.47it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 98/153 [00:13<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 99/153 [00:13<00:07,  7.38it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 100/153 [00:13<00:07,  7.45it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 101/153 [00:13<00:06,  7.58it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 102/153 [00:14<00:06,  7.61it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 103/153 [00:14<00:06,  7.61it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 104/153 [00:14<00:06,  7.49it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 105/153 [00:14<00:06,  7.44it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 106/153 [00:14<00:06,  6.91it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 107/153 [00:14<00:06,  7.11it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 108/153 [00:14<00:06,  7.16it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 109/153 [00:15<00:06,  7.08it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 110/153 [00:15<00:05,  7.24it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 111/153 [00:15<00:05,  7.13it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 112/153 [00:15<00:05,  7.05it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 113/153 [00:15<00:05,  6.97it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 114/153 [00:15<00:05,  6.95it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 115/153 [00:15<00:05,  7.13it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 116/153 [00:15<00:05,  7.26it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 117/153 [00:16<00:05,  7.15it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 118/153 [00:16<00:04,  7.28it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 119/153 [00:16<00:04,  7.22it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 120/153 [00:16<00:04,  7.36it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 121/153 [00:16<00:04,  7.07it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 122/153 [00:16<00:04,  7.14it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 123/153 [00:16<00:04,  7.28it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 124/153 [00:17<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 125/153 [00:17<00:03,  7.22it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 126/153 [00:17<00:03,  7.18it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 127/153 [00:17<00:03,  7.07it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 128/153 [00:17<00:03,  7.24it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 129/153 [00:17<00:03,  7.26it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 130/153 [00:17<00:03,  7.36it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 131/153 [00:18<00:02,  7.33it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 132/153 [00:18<00:02,  7.19it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 133/153 [00:18<00:02,  7.16it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 134/153 [00:18<00:02,  7.20it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 135/153 [00:18<00:02,  7.22it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 136/153 [00:18<00:02,  7.18it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 137/153 [00:18<00:02,  7.15it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 138/153 [00:19<00:02,  7.19it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 139/153 [00:19<00:01,  7.31it/s]\u001b[A\n",
            "Predicting batches:  92%| | 140/153 [00:19<00:01,  7.39it/s]\u001b[A\n",
            "Predicting batches:  92%| | 141/153 [00:19<00:01,  7.24it/s]\u001b[A\n",
            "Predicting batches:  93%| | 142/153 [00:19<00:01,  6.97it/s]\u001b[A\n",
            "Predicting batches:  93%| | 143/153 [00:19<00:01,  7.17it/s]\u001b[A\n",
            "Predicting batches:  94%| | 144/153 [00:19<00:01,  7.37it/s]\u001b[A\n",
            "Predicting batches:  95%| | 145/153 [00:19<00:01,  7.46it/s]\u001b[A\n",
            "Predicting batches:  95%| | 146/153 [00:20<00:00,  7.53it/s]\u001b[A\n",
            "Predicting batches:  96%|| 147/153 [00:20<00:00,  7.07it/s]\u001b[A\n",
            "Predicting batches:  97%|| 148/153 [00:20<00:00,  7.13it/s]\u001b[A\n",
            "Predicting batches:  97%|| 149/153 [00:20<00:00,  7.05it/s]\u001b[A\n",
            "Predicting batches:  98%|| 150/153 [00:20<00:00,  6.99it/s]\u001b[A\n",
            "Predicting batches:  99%|| 151/153 [00:20<00:00,  7.08it/s]\u001b[A\n",
            "Predicting batches:  99%|| 152/153 [00:20<00:00,  7.23it/s]\u001b[A\n",
            "Predicting batches: 100%|| 153/153 [00:21<00:00,  7.33it/s]\u001b[A\n",
            "Processing subjects:  89%|| 51/57 [10:22<04:08, 41.50s/it, professional psychol\u001b[A\n",
            "Generating test split: 110 examples [00:00, 31668.16 examples/s]\n",
            "\n",
            "Generating validation split: 12 examples [00:00, 7802.15 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3891.54 examples/s]\n",
            "Processing subjects:  89%|| 51/57 [10:22<04:08, 41.50s/it, public relations]\n",
            "Formatting batches:   0%|                               | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  42%|            | 46/110 [00:00<00:00, 458.64it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 92/110 [00:00<00:00, 459.27it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/28 [00:00<00:03,  8.52it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 3/28 [00:00<00:02, 10.00it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/28 [00:00<00:02,  9.96it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 5/28 [00:00<00:02,  9.25it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 6/28 [00:00<00:02,  9.22it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 8/28 [00:00<00:02,  9.83it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 10/28 [00:01<00:01,  9.99it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 11/28 [00:01<00:01,  9.97it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 12/28 [00:01<00:01,  9.94it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 14/28 [00:01<00:01, 10.22it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 16/28 [00:01<00:01, 10.09it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 18/28 [00:01<00:00, 10.24it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 20/28 [00:01<00:00, 10.35it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 22/28 [00:02<00:00, 10.18it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 24/28 [00:02<00:00, 10.30it/s]\u001b[A\n",
            "Predicting batches:  93%| | 26/28 [00:02<00:00, 10.39it/s]\u001b[A\n",
            "Predicting batches: 100%|| 28/28 [00:02<00:00, 11.32it/s]\u001b[A\n",
            "Processing subjects:  91%|| 52/57 [10:25<02:29, 29.94s/it, public relations]\u001b[A\n",
            "Generating test split: 245 examples [00:00, 39212.57 examples/s]\n",
            "\n",
            "Generating validation split: 27 examples [00:00, 14889.06 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3858.61 examples/s]\n",
            "Processing subjects:  91%|| 52/57 [10:25<02:29, 29.94s/it, security studies]\n",
            "Formatting batches:   0%|                               | 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  14%|                  | 35/245 [00:00<00:00, 347.30it/s]\u001b[A\n",
            "Formatting batches:  29%|               | 70/245 [00:00<00:00, 331.69it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 105/245 [00:00<00:00, 337.79it/s]\u001b[A\n",
            "Formatting batches:  57%|        | 140/245 [00:00<00:00, 339.64it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 174/245 [00:00<00:00, 338.63it/s]\u001b[A\n",
            "Formatting batches:  85%|   | 209/245 [00:00<00:00, 341.62it/s]\u001b[A\n",
            "Formatting batches: 100%|| 245/245 [00:00<00:00, 345.65it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/62 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/62 [00:00<00:18,  3.32it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/62 [00:00<00:19,  3.15it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/62 [00:00<00:18,  3.24it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/62 [00:01<00:17,  3.26it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/62 [00:01<00:18,  3.15it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/62 [00:01<00:17,  3.18it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 7/62 [00:02<00:17,  3.22it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 8/62 [00:02<00:16,  3.20it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/62 [00:02<00:16,  3.19it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 10/62 [00:03<00:16,  3.18it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 11/62 [00:03<00:15,  3.19it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 12/62 [00:03<00:15,  3.23it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 13/62 [00:04<00:15,  3.21it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 14/62 [00:04<00:15,  3.08it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 15/62 [00:04<00:15,  3.12it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 16/62 [00:05<00:14,  3.15it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 17/62 [00:05<00:14,  3.11it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 18/62 [00:05<00:13,  3.19it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 19/62 [00:05<00:13,  3.20it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 20/62 [00:06<00:12,  3.31it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 21/62 [00:06<00:12,  3.32it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 22/62 [00:06<00:12,  3.29it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 23/62 [00:07<00:12,  3.21it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 24/62 [00:07<00:11,  3.23it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 25/62 [00:07<00:11,  3.17it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 26/62 [00:08<00:11,  3.22it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 27/62 [00:08<00:10,  3.25it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 28/62 [00:08<00:10,  3.21it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 29/62 [00:09<00:10,  3.20it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 30/62 [00:09<00:09,  3.21it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 31/62 [00:09<00:09,  3.27it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 32/62 [00:09<00:09,  3.25it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 33/62 [00:10<00:09,  3.22it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 34/62 [00:10<00:08,  3.31it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 35/62 [00:10<00:07,  3.39it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 36/62 [00:11<00:07,  3.40it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 37/62 [00:11<00:07,  3.32it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 38/62 [00:11<00:07,  3.22it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 39/62 [00:12<00:07,  3.27it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 40/62 [00:12<00:06,  3.29it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 41/62 [00:12<00:06,  3.20it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 42/62 [00:13<00:06,  3.26it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 43/62 [00:13<00:05,  3.23it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 44/62 [00:13<00:05,  3.22it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 45/62 [00:13<00:05,  3.28it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 46/62 [00:14<00:04,  3.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 47/62 [00:14<00:04,  3.24it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 48/62 [00:14<00:04,  3.26it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 49/62 [00:15<00:03,  3.25it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 50/62 [00:15<00:03,  3.24it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 51/62 [00:15<00:03,  3.26it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 52/62 [00:16<00:03,  3.23it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 53/62 [00:16<00:02,  3.21it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 54/62 [00:16<00:02,  3.22it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 55/62 [00:17<00:02,  3.20it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 56/62 [00:17<00:01,  3.21it/s]\u001b[A\n",
            "Predicting batches:  92%| | 57/62 [00:17<00:01,  3.31it/s]\u001b[A\n",
            "Predicting batches:  94%| | 58/62 [00:17<00:01,  3.31it/s]\u001b[A\n",
            "Predicting batches:  95%| | 59/62 [00:18<00:00,  3.20it/s]\u001b[A\n",
            "Predicting batches:  97%|| 60/62 [00:18<00:00,  3.18it/s]\u001b[A\n",
            "Predicting batches:  98%|| 61/62 [00:18<00:00,  3.19it/s]\u001b[A\n",
            "Processing subjects:  93%|| 53/57 [10:45<01:47, 26.87s/it, security studies]\u001b[A\n",
            "Generating test split: 201 examples [00:00, 40541.24 examples/s]\n",
            "\n",
            "Generating validation split: 22 examples [00:00, 13344.13 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3993.05 examples/s]\n",
            "Processing subjects:  93%|| 53/57 [10:45<01:47, 26.87s/it, sociology]\n",
            "Formatting batches:   0%|                               | 0/201 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  22%|                | 45/201 [00:00<00:00, 449.50it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 91/201 [00:00<00:00, 455.23it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 138/201 [00:00<00:00, 458.18it/s]\u001b[A\n",
            "Formatting batches:  92%| | 185/201 [00:00<00:00, 459.77it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:00<00:04, 10.23it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:00<00:04,  9.64it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:00<00:04,  9.67it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:00<00:04,  9.41it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:00<00:04,  9.50it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:00<00:04,  9.58it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:00<00:04,  9.64it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:01<00:04,  9.66it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:01<00:04,  9.69it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:01<00:04,  9.51it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:01<00:03,  9.59it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:01<00:03,  9.34it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:01<00:03,  9.29it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:01<00:03,  9.27it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:01<00:03,  9.41it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:01<00:03,  9.31it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:02<00:03,  9.26it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:02<00:03,  9.23it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:02<00:03,  9.21it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:02<00:03,  9.19it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:02<00:02,  9.37it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:02<00:02,  9.31it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:02<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:02<00:02,  9.38it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:02<00:02,  9.32it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:02<00:02,  9.44it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:03<00:02,  9.53it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:03<00:02,  9.63it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:03<00:02,  9.70it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:03<00:01,  9.53it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:03<00:01,  9.59it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:03<00:01,  9.62it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:03<00:01,  9.63it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:03<00:01,  9.69it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:03<00:01,  9.69it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:04<00:01,  9.89it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:04<00:01,  9.70it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:04<00:01,  9.72it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:04<00:00,  9.55it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:04<00:00,  9.60it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:04<00:00,  9.48it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:04<00:00,  9.56it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:04<00:00,  9.58it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:04<00:00,  9.46it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:05<00:00,  9.36it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:05<00:00,  9.46it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:05<00:00,  9.53it/s]\u001b[A\n",
            "Processing subjects:  95%|| 54/57 [10:50<01:01, 20.53s/it, sociology]\u001b[A\n",
            "Generating test split: 100 examples [00:00, 29171.68 examples/s]\n",
            "\n",
            "Generating validation split: 11 examples [00:00, 7750.27 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3932.41 examples/s]\n",
            "Processing subjects:  95%|| 54/57 [10:50<01:01, 20.53s/it, us foreign policy]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  46%|           | 46/100 [00:00<00:00, 449.80it/s]\u001b[A\n",
            "Formatting batches:  92%| | 92/100 [00:00<00:00, 455.30it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02, 10.20it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02, 10.21it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 10.06it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01,  9.99it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 10.06it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01, 10.07it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  9.99it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 10.15it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 10.24it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 10.09it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00, 10.09it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:02<00:00, 10.00it/s]\u001b[A\n",
            "Processing subjects:  96%|| 55/57 [10:53<00:30, 15.19s/it, us foreign policy]\u001b[A\n",
            "Generating test split: 166 examples [00:00, 35207.04 examples/s]\n",
            "\n",
            "Generating validation split: 18 examples [00:00, 7703.04 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3148.40 examples/s]\n",
            "Processing subjects:  96%|| 55/57 [10:53<00:30, 15.19s/it, virology]\n",
            "Formatting batches:   0%|                               | 0/166 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  27%|               | 45/166 [00:00<00:00, 448.87it/s]\u001b[A\n",
            "Formatting batches:  56%|         | 93/166 [00:00<00:00, 466.60it/s]\u001b[A\n",
            "Formatting batches:  86%|   | 142/166 [00:00<00:00, 474.41it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/42 [00:00<00:02, 13.50it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/42 [00:00<00:02, 13.27it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/42 [00:00<00:02, 12.86it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/42 [00:00<00:02, 12.75it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/42 [00:00<00:02, 12.78it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/42 [00:00<00:02, 12.32it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/42 [00:01<00:02, 12.20it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 16/42 [00:01<00:02, 12.39it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 18/42 [00:01<00:01, 12.38it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 20/42 [00:01<00:01, 12.33it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 22/42 [00:01<00:01, 12.50it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 24/42 [00:01<00:01, 12.58it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 26/42 [00:02<00:01, 12.57it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 28/42 [00:02<00:01, 12.25it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 30/42 [00:02<00:00, 12.42it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 32/42 [00:02<00:00, 12.38it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 34/42 [00:02<00:00, 12.14it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 36/42 [00:02<00:00, 12.27it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 38/42 [00:03<00:00, 12.41it/s]\u001b[A\n",
            "Predicting batches:  95%| | 40/42 [00:03<00:00, 10.90it/s]\u001b[A\n",
            "Predicting batches: 100%|| 42/42 [00:03<00:00, 11.93it/s]\u001b[A\n",
            "Processing subjects:  98%|| 56/57 [10:57<00:11, 11.77s/it, virology]\u001b[A\n",
            "Generating test split: 171 examples [00:00, 39952.43 examples/s]\n",
            "\n",
            "Generating validation split: 19 examples [00:00, 11549.53 examples/s]\n",
            "\n",
            "Generating train split: 5 examples [00:00, 3784.79 examples/s]\n",
            "Processing subjects:  98%|| 56/57 [10:57<00:11, 11.77s/it, world religions]\n",
            "Formatting batches:   0%|                               | 0/171 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  29%|               | 49/171 [00:00<00:00, 483.29it/s]\u001b[A\n",
            "Formatting batches:  57%|         | 98/171 [00:00<00:00, 484.62it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 148/171 [00:00<00:00, 489.72it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 2/43 [00:00<00:02, 17.14it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 4/43 [00:00<00:02, 16.64it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/43 [00:00<00:02, 16.38it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/43 [00:00<00:02, 15.81it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 10/43 [00:00<00:02, 16.35it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 12/43 [00:00<00:01, 16.24it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/43 [00:00<00:01, 16.22it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 16/43 [00:00<00:01, 15.80it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 18/43 [00:01<00:01, 15.90it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 20/43 [00:01<00:01, 15.58it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 22/43 [00:01<00:01, 15.39it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 24/43 [00:01<00:01, 15.27it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 26/43 [00:01<00:01, 15.51it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 28/43 [00:01<00:00, 16.05it/s]\u001b[A\n",
            "Predicting batches:  70%|       | 30/43 [00:01<00:00, 16.07it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 32/43 [00:02<00:00, 15.72it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 34/43 [00:02<00:00, 16.18it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 36/43 [00:02<00:00, 16.14it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 38/43 [00:02<00:00, 15.79it/s]\u001b[A\n",
            "Predicting batches:  93%| | 40/43 [00:02<00:00, 15.90it/s]\u001b[A\n",
            "Predicting batches:  98%|| 42/43 [00:02<00:00, 15.95it/s]\u001b[A\n",
            "Processing subjects: 100%|| 57/57 [11:00<00:00, 11.59s/it, world religions]\u001b[A\n",
            "        Average: 59.68\n",
            "           STEM: 49.87\n",
            "Social Sciences: 68.57\n",
            "     Humanities: 56.30\n",
            "          Other: 65.27\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli eval CPT_freeze_eval.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwHhAHQwmyf6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# --- NLG Quality Prediction (BLEU / ROUGE) for FrozenBottom Model ---\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    model_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged\",\n",
        "    trust_remote_code=True,\n",
        "    template=\"llama3\",\n",
        "\n",
        "    # Method settings\n",
        "    stage=\"sft\",\n",
        "    do_predict=True,\n",
        "    finetuning_type=\"freeze\",\n",
        "\n",
        "    # Dataset settings\n",
        "    eval_dataset=\"wiki_1percent,markdown_docs\",\n",
        "    cutoff_len=2048,\n",
        "    # max_samples=50,\n",
        "    overwrite_cache=True,\n",
        "    preprocessing_num_workers=16,\n",
        "\n",
        "    # Output and evaluation\n",
        "    output_dir=\"CPT_freeze/predict\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    ddp_timeout=180000000,\n",
        ")\n",
        "\n",
        "# write out your predict config\n",
        "json.dump(args, open(\"CPT_freeze_predict.json\", \"w\"), indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreB8Rk0myf6",
        "outputId": "8cab2bcd-2c1b-48fd-bcdb-2e4bfbbfe5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|2025-06-20 11:49:39] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,016 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,016 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,016 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,016 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,016 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,016 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 11:49:39,240 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 11:49:39,241 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 11:49:39,242 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,243 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,243 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,243 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,243 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,243 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:49:39,243 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 11:49:39,479 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-06-20 11:49:39] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-06-20 11:49:39] llamafactory.data.loader:143 >> Loading dataset wiki_1percent.json...\n",
            "Converting format of dataset (num_proc=16): 100%|| 64078/64078 [00:00<00:00, 18\n",
            "[INFO|2025-06-20 11:49:40] llamafactory.data.loader:143 >> Loading dataset markdown_docs.jsonl...\n",
            "Converting format of dataset (num_proc=16): 100%|| 37/37 [00:00<00:00, 347.93 e\n",
            "Running tokenizer on dataset (num_proc=16): 100%|| 64115/64115 [00:19<00:00, 33\n",
            "eval example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 2127, 1132, 2191, 374, 264, 5054, 19675, 323, 7351, 430, 374, 44929, 315, 682, 1120, 7174, 369, 11447, 323, 26737, 311, 90376, 279, 14673, 433, 8349, 10519, 26225, 78242, 323, 30022, 11, 11383, 2737, 7140, 90160, 11, 323, 32682, 13, 1556, 1132, 2191, 28424, 369, 279, 14039, 315, 279, 1614, 449, 1614, 1752, 34775, 323, 37079, 1949, 30257, 13, 1666, 264, 35901, 2163, 29480, 7351, 11, 420, 5403, 315, 44565, 2191, 374, 9277, 389, 279, 3117, 61943, 2163, 315, 279, 5054, 20326, 11, 6118, 7633, 439, 279, 57125, 20611, 315, 279, 41289, 7351, 320, 2808, 531, 8997, 51618, 3677, 95668, 617, 12439, 304, 34775, 2085, 16287, 12694, 1132, 552, 1317, 1603, 279, 21967, 315, 5415, 11, 77563, 11, 477, 991, 19505, 13, 3161, 279, 10205, 315, 39433, 70994, 13162, 11, 67451, 42914, 9017, 11447, 1101, 16392, 13, 10541, 35483, 315, 78431, 6848, 527, 1766, 682, 6957, 3925, 11, 6617, 44565, 2191, 22763, 505, 279, 92931, 13, 12220, 279, 15629, 4376, 315, 279, 220, 777, 339, 323, 279, 1176, 11026, 315, 279, 220, 508, 339, 9478, 11, 279, 78431, 7351, 20415, 3384, 304, 1455, 5596, 315, 279, 1917, 323, 1047, 264, 5199, 3560, 304, 7487, 6, 28970, 369, 91225, 49686, 13, 40741, 78431, 8853, 315, 3463, 14454, 2391, 420, 4261, 13, 1556, 1132, 1705, 617, 4529, 961, 304, 3892, 93574, 11, 1455, 35146, 304, 279, 12366, 6947, 2957, 11, 279, 8690, 16803, 5111, 323, 279, 15506, 16803, 5111, 11, 6832, 842, 13160, 279, 842, 315, 279, 29924, 11639, 315, 44565, 2191, 13, 763, 279, 1566, 11026, 315, 279, 220, 508, 339, 323, 1139, 279, 220, 1691, 267, 9478, 11, 279, 78431, 7351, 706, 1027, 594, 86153, 3131, 810, 11, 7982, 304, 23354, 323, 10383, 2949, 7294, 98231, 380, 11, 7294, 48260, 323, 7294, 74419, 8082, 19567, 382, 2127, 1132, 1705, 3539, 17226, 20414, 11, 902, 1253, 387, 8965, 18255, 1139, 30191, 323, 41993, 15174, 26, 1070, 374, 5199, 28347, 1990, 279, 1403, 13, 38321, 661, 5528, 1456, 311, 38553, 1148, 459, 78431, 8396, 2643, 387, 1093, 11, 719, 30191, 26411, 11, 902, 617, 35901, 4529, 264, 16806, 2543, 11, 9395, 311, 63331, 11447, 323, 279, 1614, 13, 9176, 62814, 315, 3823, 36017, 617, 1027, 28160, 555, 78431, 10334, 11, 43665, 11, 323, 550, 7332, 382, 32960, 99174, 11, 57726, 11, 323, 7419, 4815, 791, 1880, 1631, 5848, 6371, 315, 44565, 2191, 374, 505, 279, 38050, 18341, 459, 847, 71, 689, 11, 7438, 330, 30096, 264, 49080, 498, 24306, 315, 279, 9436, 459, 12, 3573, 30096, 909, 323, 279, 3492, 802, 31764, 437, 3573, 38491, 1, 477, 330, 81, 8646, 1865, 578, 21166, 482, 2191, 72214, 279, 42933, 1510, 430, 9428, 2530, 459, 15630, 13, 1556, 1132, 2191, 8111, 304, 6498, 505, 220, 10513, 17, 439, 44565, 44618, 323, 459, 15630, 505, 220, 9800, 24, 26, 4216, 6498, 603, 1154, 20654, 4147, 264, 5647, 315, 19823, 13, 40741, 48752, 2949, 279, 8753, 22910, 61336, 872, 19949, 439, 93134, 11, 8051, 2478, 1778, 13487, 6222, 1690, 6325, 449, 3010, 93134, 13, 9176, 14110, 5548, 315, 279, 220, 777, 339, 9478, 1778, 439, 12656, 4359, 7678, 320, 10005, 21, 4235, 10750, 21, 8, 323, 93537, 1226, 275, 2785, 320, 5245, 23, 4235, 9674, 16, 8, 1053, 17210, 311, 279, 78431, 83258, 315, 279, 1828, 9659, 719, 1550, 539, 1005, 78431, 477, 44565, 2191, 304, 23524, 5694, 477, 872, 21463, 382, 791, 1176, 5054, 55475, 311, 1650, 5678, 459, 78431, 1754, 574, 38077, 12278, 974, 764, 393, 583, 31721, 263, 320, 5245, 24, 4235, 9714, 20, 705, 36024, 279, 16287, 7342, 315, 44565, 2191, 304, 279, 5209, 12, 777, 339, 9478, 13, 8876, 279, 220, 9378, 15, 82, 323, 7314, 304, 9822, 11, 57125, 2191, 706, 3629, 1027, 1511, 439, 264, 74450, 369, 44565, 2191, 323, 1202, 1005, 439, 264, 74450, 374, 2103, 4279, 4994, 279, 3723, 4273, 13, 4427, 603, 1154, 315, 57125, 2191, 8464, 311, 3927, 4633, 1949, 48831, 19675, 1193, 11, 323, 1949, 48831, 44565, 2191, 304, 4040, 374, 61937, 57125, 44565, 2191, 382, 8142, 279, 4751, 57125, 706, 1027, 14090, 69593, 449, 44565, 2191, 11, 1202, 7438, 706, 810, 6051, 1027, 80703, 555, 22622, 25375, 505, 2679, 30450, 85129, 5315, 11, 2737, 2225, 279, 1561, 14043, 323, 57125, 28187, 1705, 11, 889, 656, 539, 22712, 5694, 449, 59021, 3674, 1705, 477, 264, 348, 53290, 4717, 11, 323, 14560, 13042, 45750, 11, 889, 527, 15871, 11920, 449, 8431, 58455, 13, 23212, 11, 1063, 93134, 1005, 57125, 41289, 311, 5766, 44565, 2191, 596, 8389, 390, 14632, 323, 20654, 1082, 1202, 13537, 449, 51618, 13, 1556, 1132, 2191, 374, 44029, 1511, 311, 7664, 279, 7294, 43802, 20631, 20611, 315, 279, 41289, 7351, 13, 1556, 1132, 2191, 374, 13168, 291, 311, 41289, 7739, 902, 527, 1614, 36185, 477, 505, 3485, 13, 99394, 315, 44565, 2191, 8965, 11415, 44565, 2191, 596, 41289, 16792, 323, 9940, 1082, 13865, 520, 6968, 29953, 354, 316, 552, 1990, 279, 1403, 13, 4427, 31839, 7664, 44565, 2191, 439, 3515, 1690, 34453, 505, 84581, 11, 323, 1694, 2225, 18250, 323, 41289, 719, 810, 779, 13, 9176, 31839, 8007, 459, 277, 971, 98231, 2191, 439, 264, 70847, 315, 78431, 16565, 382, 8142, 14076, 311, 279, 1614, 374, 8792, 311, 78431, 3463, 11, 27409, 44565, 2191, 374, 539, 459, 4228, 3465, 369, 31839, 11, 439, 1070, 374, 264, 2763, 315, 10430, 4315, 31839, 323, 93134, 389, 279, 5030, 11, 323, 5370, 60701, 45493, 44565, 2191, 10284, 22009, 13, 17559, 36222, 3079, 5540, 2997, 279, 690, 369, 264, 2536, 23283, 3035, 535, 8396, 11, 279, 38001, 315, 279, 1614, 41705, 11, 279, 16801, 430, 3823, 7138, 6276, 12966, 311, 3073, 304, 477, 5208, 9017, 1778, 264, 2536, 23283, 3035, 535, 8396, 11, 323, 264, 24710, 389, 1268, 311, 1180, 311, 23564, 279, 10728, 315, 459, 15630, 382, 13730, 271, 4808, 17515, 944, 11639, 4815, 10438, 279, 9886, 315, 25861, 323, 9919, 11, 9749, 11447, 1550, 539, 3073, 13, 1102, 574, 1306, 279, 15244, 315, 11447, 430, 44565, 4633, 6848, 1051, 16948, 37588, 439, 264, 13010, 13, 578, 1455, 28289, 5956, 34291, 311, 44565, 2191, 304, 279, 14154, 1917, 1051, 304, 5734, 323, 25431, 13, 763, 5734, 11, 41903, 44565, 2191, 320, 1820, 10430, 389, 279, 57008, 315, 279, 1614, 8, 574, 91784, 660, 555, 60608, 380, 61787, 68844, 526, 67927, 323, 445, 3524, 8510, 13, 32944, 3002, 71883, 42914, 11, 60608, 2191, 706, 1027, 1071, 311, 617, 1047, 330, 91645, 16961, 811, 1, 315, 44565, 2191, 382, 2127, 1132, 292, 33726, 1051, 1101, 83280, 555, 28375, 5493, 323, 61787, 304, 25431, 13, 362, 60478, 4010, 355, 323, 34940, 511, 645, 1511, 279, 21849, 315, 6898, 343, 606, 311, 41468, 279, 12324, 1990, 7016, 27070, 555, 279, 1614, 323, 4443, 51360, 13, 328, 78046, 29440, 59652, 1122, 11527, 15320, 323, 29676, 389, 279, 1314, 315, 3927, 11542, 315, 42563, 13, 356, 1910, 1233, 27292, 3823, 2383, 320, 17101, 437, 8, 323, 5938, 11527, 1418, 4560, 311, 3974, 4184, 311, 7138, 320, 764, 4548, 570, 71883, 1233, 1051, 33445, 315, 264, 8396, 3196, 389, 57751, 323, 11919, 4398, 4315, 1202, 10495, 2085, 279, 9546, 315, 264, 1614, 382, 644, 42108, 4606, 11, 1070, 574, 912, 44565, 4633, 5820, 3734, 1063, 14943, 5411, 10597, 19567, 13, 4314, 11, 323, 1023, 10451, 19567, 11, 3010, 6688, 7342, 311, 10597, 44565, 2191, 13, 763, 279, 328, 46488, 1122, 21080, 11, 40091, 67, 587, 2663, 369, 459, 77271, 20631, 8396, 323, 279, 76445, 315, 87149, 11, 1193, 311, 387, 5246, 16070, 555, 35414, 735, 38155, 358, 382, 644, 15004, 969, 11, 10597, 31237, 82, 89194, 2403, 279, 1614, 13, 763, 4606, 11, 5370, 31237, 82, 8040, 7294, 21395, 323, 57125, 61555, 13, 50086, 291, 2802, 304, 61386, 488, 2391, 279, 55383, 323, 304, 879, 19971, 2391, 279, 1050, 1659, 28101, 5540, 315, 7294, 43802, 20631, 37019, 2191, 11, 8104, 304, 9822, 13, 92931, 11774, 311, 20207, 11447, 320, 5132, 1299, 323, 10597, 8, 323, 279, 93574, 315, 279, 220, 11128, 15, 82, 323, 220, 10336, 23, 682, 85747, 279, 42933, 4500, 315, 1148, 6244, 279, 11639, 315, 29924, 44565, 2191, 382, 49552, 11639, 720, 16397, 279, 8753, 22910, 11, 49638, 5315, 1778, 439, 279, 2998, 4193, 5512, 323, 279, 220, 5602, 264, 13353, 1486, 304, 279, 74454, 315, 7294, 21395, 323, 6918, 380, 58214, 13, 578, 1176, 78431, 60701, 8040, 6957, 279, 220, 972, 339, 9478, 439, 12656, 4359, 7678, 16948, 37588, 41903, 44565, 2191, 304, 9635, 11, 57323, 20445, 275, 318, 3876, 279, 1614, 11, 7639, 65292, 1215, 596, 7422, 63675, 279, 1648, 311, 3927, 2191, 323, 38077, 12278, 974, 764, 393, 583, 31721, 263, 596, 10334, 315, 27848, 2191, 1766, 70225, 17614, 304, 9822, 13, 3296, 279, 3389, 220, 9674, 15, 82, 11, 5370, 78431, 8853, 315, 3463, 1047, 3719, 1664, 39817, 323, 264, 12330, 315, 1243, 31069, 3728, 8082, 10222, 505, 220, 9367, 15, 311, 220, 7529, 19, 13, 1115, 11639, 315, 29924, 44565, 2191, 36513, 3156, 279, 842, 315, 279, 15506, 16803, 5111, 323, 374, 6646, 279, 21411, 4325, 315, 44565, 2191, 382, 38537, 505, 27848, 2191, 11, 92551, 36769, 359, 258, 18538, 6667, 80244, 44565, 2191, 323, 10862, 279, 7327, 22938, 5794, 596, 10229, 11, 264, 538, 12128, 11552, 3010, 3967, 439, 279, 5629, 7327, 430, 14454, 304, 220, 9714, 19, 311, 52696, 17226, 30191, 60701, 13, 578, 7327, 6244, 264, 5199, 5054, 5457, 11, 449, 35131, 28187, 1694, 264, 6522, 7216, 323, 264, 4562, 315, 1202, 3331, 9251, 13, 36769, 359, 258, 596, 37480, 320, 1820, 622, 5808, 28331, 8, 323, 393, 583, 31721, 263, 596, 20723, 320, 1820, 27848, 1705, 8, 16475, 1614, 51618, 11, 59416, 5054, 63944, 3012, 2191, 323, 2678, 3424, 58348, 13, 4740, 26242, 42254, 11, 279, 36769, 359, 258, 1705, 1051, 67331, 505, 279, 7327, 555, 279, 28187, 1705, 520, 279, 220, 9674, 17, 86026, 8151, 13, 1556, 1132, 1705, 1051, 12020, 30293, 304, 279, 10657, 7327, 11, 1694, 13967, 67331, 304, 220, 9378, 21, 13, 36769, 359, 258, 51287, 19698, 430, 422, 14110, 5548, 18661, 2410, 555, 28187, 596, 3878, 11, 814, 1053, 842, 709, 279, 502, 43049, 1821, 315, 7487, 13, 763, 2077, 311, 872, 95989, 505, 279, 5629, 7327, 11, 93134, 14454, 279, 800, 13, 2417, 1291, 7327, 13, 9636, 279, 10383, 315, 11291, 735, 897, 354, 8148, 11, 264, 8690, 55475, 323, 28568, 11, 459, 277, 971, 88389, 359, 2191, 29204, 5795, 449, 6667, 74050, 13, 1556, 277, 971, 88389, 359, 1705, 11, 889, 24465, 20343, 505, 279, 220, 9674, 16, 12366, 6947, 2957, 11, 64854, 369, 1949, 80375, 323, 369, 279, 8141, 315, 11822, 4184, 311, 832, 596, 3966, 382, 1383, 279, 2543, 315, 279, 220, 508, 339, 9478, 11, 44565, 2191, 1047, 9041, 682, 927, 279, 1917, 13, 1102, 574, 264, 28289, 4668, 315, 279, 6625, 22013, 950, 380, 7351, 13, 763, 5734, 11, 2678, 5315, 315, 4236, 25973, 279, 3823, 4633, 463, 31419, 1873, 2373, 315, 459, 277, 971, 88389, 359, 2191, 13, 27286, 574, 264, 80310, 369, 42301, 1245, 12822, 505, 6460, 14875, 5961, 11, 889, 7882, 311, 279, 11002, 6864, 311, 4007, 13, 763, 20023, 5270, 11, 32164, 574, 264, 86568, 369, 459, 277, 971, 1355, 88, 303, 950, 2191, 11, 1405, 433, 6244, 279, 1455, 21102, 2163, 29480, 34649, 13, 12220, 420, 892, 11, 264, 23413, 315, 93134, 18306, 26411, 315, 30191, 5054, 9349, 11, 3967, 439, 30617, 315, 279, 56408, 13, 578, 834, 9792, 479, 315, 279, 8753, 41289, 7351, 1139, 1690, 5315, 323, 279, 11572, 323, 61087, 315, 1690, 57298, 2402, 311, 47426, 49028, 2768, 279, 46735, 315, 279, 12366, 6947, 2957, 92867, 3927, 380, 5054, 7645, 323, 14385, 13, 7570, 3582, 1690, 93134, 1612, 4979, 5694, 505, 1521, 20320, 14385, 11, 4225, 27322, 3782, 5304, 279, 7351, 323, 13865, 1051, 1903, 311, 5471, 93134, 15644, 1113, 311, 279, 2326, 11, 2737, 279, 40782, 3298, 315, 220, 7028, 18, 11, 1101, 2663, 279, 1556, 1132, 380, 1398, 9134, 3298, 13, 15388, 2191, 574, 2500, 8446, 902, 1063, 93134, 18306, 2391, 420, 4261, 382, 20397, 10742, 11, 93134, 99325, 31408, 304, 279, 8690, 22910, 304, 14076, 311, 279, 5929, 7351, 11, 5423, 304, 279, 386, 22506, 39142, 939, 81236, 26, 4869, 11, 814, 2322, 25984, 46735, 1306, 279, 92501, 3109, 1047, 27276, 4147, 11, 2737, 2391, 279, 97660, 45378, 53848, 13, 26778, 93134, 505, 62579, 6902, 323, 23223, 30010, 311, 19278, 11, 1603, 279, 92501, 82, 33745, 279, 78431, 7351, 1070]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. As a historically left-wing movement, this reading of anarchism is placed on the farthest left of the political spectrum, usually described as the libertarian wing of the socialist movement (libertarian socialism).\n",
            "\n",
            "Humans have lived in societies without formal hierarchies long before the establishment of states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more, growing in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements.\n",
            "\n",
            "Anarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. Evolutionary methods try to simulate what an anarchist society might be like, but revolutionary tactics, which have historically taken a violent turn, aim to overthrow authority and the state. Many facets of human civilization have been influenced by anarchist theory, critique, and praxis.\n",
            "\n",
            "Etymology, terminology, and definition \n",
            "\n",
            "The etymological origin of anarchism is from the Ancient Greek anarkhia, meaning \"without a ruler\", composed of the prefix an- (\"without\") and the word arkhos (\"leader\" or \"ruler\"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (17561836) and Wilhelm Weitling (18081871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.\n",
            "\n",
            "The first political philosopher to call himself an anarchist () was Pierre-Joseph Proudhon (18091865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.\n",
            "\n",
            "While the term libertarian has been largely synonymous with anarchism, its meaning has more recently been diluted by wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism's negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism's socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberal and socialist but more so. Many scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.\n",
            "\n",
            "While opposition to the state is central to anarchist thought, defining anarchism is not an easy task for scholars, as there is a lot of discussion among scholars and anarchists on the matter, and various currents perceive anarchism slightly differently. Major definitional elements include the will for a non-coercive society, the rejection of the state apparatus, the belief that human nature allows humans to exist in or progress toward such a non-coercive society, and a suggestion on how to act to pursue the ideal of anarchy.\n",
            "\n",
            "History\n",
            "\n",
            "Pre-modern era \n",
            "\n",
            "Before the creation of towns and cities, established authority did not exist. It was after the institution of authority that anarchistic ideas were espoused as a reaction. The most notable precursors to anarchism in the ancient world were in China and Greece. In China, philosophical anarchism (the discussion on the legitimacy of the state) was delineated by Taoist philosophers Zhuang Zhou and Laozi. Alongside Stoicism, Taoism has been said to have had \"significant anticipations\" of anarchism.\n",
            "\n",
            "Anarchic attitudes were also articulated by tragedians and philosophers in Greece. Aeschylus and Sophocles used the myth of Antigone to illustrate the conflict between laws imposed by the state and personal autonomy. Socrates questioned Athenian authorities constantly and insisted on the right of individual freedom of conscience. Cynics dismissed human law (nomos) and associated authorities while trying to live according to nature (physis). Stoics were supportive of a society based on unofficial and friendly relations among its citizens without the presence of a state.\n",
            "\n",
            "In medieval Europe, there was no anarchistic activity except some ascetic religious movements. These, and other Muslim movements, later gave birth to religious anarchism. In the Sasanian Empire, Mazdak called for an egalitarian society and the abolition of monarchy, only to be soon executed by Emperor Kavad I.\n",
            "\n",
            "In Basra, religious sects preached against the state. In Europe, various sects developed anti-state and libertarian tendencies. Renewed interest in antiquity during the Renaissance and in private judgment during the Reformation restored elements of anti-authoritarian secularism, particularly in France. Enlightenment challenges to intellectual authority (secular and religious) and the revolutions of the 1790s and 1848 all spurred the ideological development of what became the era of classical anarchism.\n",
            "\n",
            "Modern era \n",
            "During the French Revolution, partisan groups such as the Enrags and the  saw a turning point in the fermentation of anti-state and federalist sentiments. The first anarchist currents developed throughout the 18th century as William Godwin espoused philosophical anarchism in England, morally delegitimising the state, Max Stirner's thinking paved the way to individualism and Pierre-Joseph Proudhon's theory of mutualism found fertile soil in France. By the late 1870s, various anarchist schools of thought had become well-defined and a wave of then unprecedented globalisation occurred from 1880 to 1914. This era of classical anarchism lasted until the end of the Spanish Civil War and is considered the golden age of anarchism.\n",
            "\n",
            "Drawing from mutualism, Mikhail Bakunin founded collectivist anarchism and entered the International Workingmen's Association, a class worker union later known as the First International that formed in 1864 to unite diverse revolutionary currents. The International became a significant political force, with Karl Marx being a leading figure and a member of its General Council. Bakunin's faction (the Jura Federation) and Proudhon's followers (the mutualists) opposed state socialism, advocating political abstentionism and small property holdings. After bitter disputes, the Bakuninists were expelled from the International by the Marxists at the 1872 Hague Congress. Anarchists were treated similarly in the Second International, being ultimately expelled in 1896. Bakunin famously predicted that if revolutionaries gained power by Marx's terms, they would end up the new tyrants of workers. In response to their expulsion from the First International, anarchists formed the St. Imier International. Under the influence of Peter Kropotkin, a Russian philosopher and scientist, anarcho-communism overlapped with collectivism. Anarcho-communists, who drew inspiration from the 1871 Paris Commune, advocated for free federation and for the distribution of goods according to one's needs.\n",
            "\n",
            "By the turn of the 20th century, anarchism had spread all over the world. It was a notable feature of the international syndicalist movement. In China, small groups of students imported the humanistic pro-science version of anarcho-communism. Tokyo was a hotspot for rebellious youth from East Asian countries, who moved to the Japanese capital to study. In Latin America, Argentina was a stronghold for anarcho-syndicalism, where it became the most prominent left-wing ideology. During this time, a minority of anarchists adopted tactics of revolutionary political violence, known as propaganda of the deed. The dismemberment of the French socialist movement into many groups and the execution and exile of many Communards to penal colonies following the suppression of the Paris Commune favoured individualist political expression and acts. Even though many anarchists distanced themselves from these terrorist acts, infamy came upon the movement and attempts were made to prevent anarchists immigrating to the US, including the Immigration Act of 1903, also called the Anarchist Exclusion Act. Illegalism was another strategy which some anarchists adopted during this period.\n",
            "\n",
            "Despite concerns, anarchists enthusiastically participated in the Russian Revolution in opposition to the White movement, especially in the Makhnovshchina; however, they met harsh suppression after the Bolshevik government had stabilised, including during the Kronstadt rebellion. Several anarchists from Petrograd and Moscow fled to Ukraine, before the Bolsheviks crushed the anarchist movement there\n",
            "label_ids:\n",
            "[128009]\n",
            "labels:\n",
            "<|eot_id|>\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 11:50:00,706 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 11:50:00,707 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 11:50:00] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-20 11:50:00,798 >> loading weights file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-20 11:50:00,798 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 11:50:00,799 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.06s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-20 11:50:05,035 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-20 11:50:05,035 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-20 11:50:05,036 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 11:50:05,036 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 11:50:05] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-20 11:50:05] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
            "[WARNING|2025-06-20 11:50:05] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.\n",
            "[WARNING|2025-06-20 11:50:05] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n",
            "[INFO|trainer.py:4327] 2025-06-20 11:50:05,046 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4329] 2025-06-20 11:50:05,046 >>   Num examples = 64115\n",
            "[INFO|trainer.py:4332] 2025-06-20 11:50:05,046 >>   Batch size = 16\n",
            "  0%|                                       | 4/4008 [03:09<57:26:08, 51.64s/it]Traceback (most recent call last):\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/src/llamafactory/cli.py\", line 151, in main\n",
            "    COMMAND_MAP[command]()\n",
            "  File \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 129, in run_sft\n",
            "    predict_results = trainer.predict(dataset_module[\"eval_dataset\"], metric_key_prefix=\"predict\", **gen_kwargs)\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\", line 255, in predict\n",
            "    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py\", line 4251, in predict\n",
            "    output = eval_loop(\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py\", line 4368, in evaluation_loop\n",
            "    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/src/llamafactory/train/sft/trainer.py\", line 123, in prediction_step\n",
            "    loss, generated_tokens, _ = super().prediction_step(\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer_seq2seq.py\", line 327, in prediction_step\n",
            "    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2597, in generate\n",
            "    result = self._sample(\n",
            "  File \"/home/ai-research-lab/miniconda3/envs/myenv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3616, in _sample\n",
            "    this_peer_finished = unfinished_sequences.max() == 0\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# then run\n",
        "!llamafactory-cli train CPT_freeze_predict.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOdtkZZ3myf7"
      },
      "source": [
        "### Evalution check for instruction tuning on layer freezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xik4LiaEmyf7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# --- GeneralCapability Evaluation ---\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    model_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged_fintuned\",\n",
        "    # adapter_name_or_path=\"InstructionTuning_LoRA_Alpaca\",\n",
        "    trust_remote_code=True,\n",
        "    template=\"fewshot\",         # fewshot prompt template\n",
        "\n",
        "    # Method settings\n",
        "    finetuning_type=\"freeze\",\n",
        "\n",
        "    # Dataset settings\n",
        "    task=\"mmlu_test\",           # or ceval_validation, cmmlu_test\n",
        "    lang=\"en\",\n",
        "    n_shot=5,\n",
        "\n",
        "    # Output and evaluation\n",
        "    save_dir=\"InstructionTuning_Freeze/eval\",\n",
        "    batch_size=4,\n",
        "    seed=42,\n",
        "    # download_mode=\"reuse\",      # reuse existing data if present\n",
        ")\n",
        "\n",
        "# write out your eval config\n",
        "json.dump(args, open(\"InstructionTuning_Freeze_eval.json\", \"w\"), indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh52Yd9mmyf7",
        "outputId": "8c66f2e4-6cbf-4b9d-e303-beb1bf8ae0f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:02,814 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:02,814 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:02,814 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:02,814 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:02,814 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:02,814 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 11:55:03,040 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 11:55:03,040 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged_fintuned/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 11:55:03,042 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:03,042 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:03,042 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:03,042 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:03,042 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:03,042 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 11:55:03,042 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 11:55:03,277 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 11:55:03,293 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged_fintuned/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 11:55:03,294 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 11:55:03] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-20 11:55:03,382 >> loading weights file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged_fintuned/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-20 11:55:03,382 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 11:55:03,383 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.04s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-20 11:55:07,645 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-20 11:55:07,645 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged_fintuned.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-20 11:55:07,646 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_freeze_merged_fintuned/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 11:55:07,646 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 11:55:07] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-20 11:55:07] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
            "Processing subjects:   0%|             | 0/57 [00:00<?, ?it/s, abstract algebra]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  57%|         | 57/100 [00:00<00:00, 566.85it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A[WARNING|logging.py:313] 2025-06-20 11:55:07,833 >> You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:05,  4.50it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.62it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:01, 10.09it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:01, 11.04it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:00<00:01, 11.58it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01, 12.01it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:00, 12.17it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:00, 12.29it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:01<00:00, 12.37it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:01<00:00, 12.42it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:01<00:00, 12.45it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:01<00:00, 12.49it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:02<00:00, 12.60it/s]\u001b[A\n",
            "Processing subjects:   2%|             | 1/57 [00:02<02:09,  2.31s/it, anatomy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/135 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 60/135 [00:00<00:00, 598.62it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 120/135 [00:00<00:00, 598.03it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/34 [00:00<00:02, 13.48it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/34 [00:00<00:02, 12.63it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 6/34 [00:00<00:02, 13.07it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 8/34 [00:00<00:02, 12.86it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 10/34 [00:00<00:01, 13.12it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 12/34 [00:00<00:01, 12.93it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 14/34 [00:01<00:01, 12.80it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 16/34 [00:01<00:01, 12.78it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 18/34 [00:01<00:01, 12.72it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 20/34 [00:01<00:01, 12.47it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 22/34 [00:01<00:00, 12.60it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 24/34 [00:01<00:00, 12.43it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 26/34 [00:02<00:00, 12.56it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 28/34 [00:02<00:00, 12.59it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 30/34 [00:02<00:00, 12.82it/s]\u001b[A\n",
            "Predicting batches:  94%| | 32/34 [00:02<00:00, 12.94it/s]\u001b[A\n",
            "Predicting batches: 100%|| 34/34 [00:02<00:00, 13.58it/s]\u001b[A\n",
            "Processing subjects:   4%|           | 2/57 [00:05<02:25,  2.64s/it, astronomy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/152 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  36%|             | 54/152 [00:00<00:00, 535.97it/s]\u001b[A\n",
            "Formatting batches:  72%|     | 109/152 [00:00<00:00, 539.98it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/38 [00:00<00:04,  8.15it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/38 [00:00<00:04,  7.77it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/38 [00:00<00:04,  7.37it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/38 [00:00<00:04,  7.67it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 5/38 [00:00<00:04,  7.73it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/38 [00:00<00:04,  7.78it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 7/38 [00:00<00:03,  7.95it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 8/38 [00:01<00:03,  7.75it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/38 [00:01<00:03,  7.87it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 10/38 [00:01<00:03,  7.80it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 11/38 [00:01<00:03,  7.81it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/38 [00:01<00:03,  7.91it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 13/38 [00:01<00:03,  7.83it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 14/38 [00:01<00:03,  7.76it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 15/38 [00:01<00:02,  7.87it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 16/38 [00:02<00:02,  7.96it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 17/38 [00:02<00:02,  8.03it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 18/38 [00:02<00:02,  7.96it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 19/38 [00:02<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 20/38 [00:02<00:02,  7.99it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 21/38 [00:02<00:02,  7.81it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 22/38 [00:02<00:02,  7.76it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 23/38 [00:02<00:01,  7.78it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 24/38 [00:03<00:01,  7.79it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 25/38 [00:03<00:01,  7.92it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 26/38 [00:03<00:01,  8.00it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 27/38 [00:03<00:01,  8.03it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 28/38 [00:03<00:01,  7.92it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 29/38 [00:03<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 30/38 [00:03<00:01,  7.57it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 31/38 [00:03<00:00,  7.58it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 32/38 [00:04<00:00,  7.59it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 33/38 [00:04<00:00,  7.60it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 34/38 [00:04<00:00,  7.61it/s]\u001b[A\n",
            "Predicting batches:  92%| | 35/38 [00:04<00:00,  7.80it/s]\u001b[A\n",
            "Predicting batches:  95%| | 36/38 [00:04<00:00,  7.81it/s]\u001b[A\n",
            "Predicting batches:  97%|| 37/38 [00:04<00:00,  7.94it/s]\u001b[A\n",
            "Predicting batches: 100%|| 38/38 [00:04<00:00,  8.03it/s]\u001b[A\n",
            "Processing subjects:   5%|     | 3/57 [00:10<03:24,  3.78s/it, business ethics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  56%|         | 56/100 [00:00<00:00, 552.42it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  8.11it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  8.16it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.16it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  8.19it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  8.17it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  8.07it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:02,  7.99it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.93it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.95it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.80it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:01,  7.68it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.84it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.83it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  7.83it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  7.77it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.79it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  7.92it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.76it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00,  7.98it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  8.06it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  7.93it/s]\u001b[A\n",
            "Processing subjects:   7%|  | 4/57 [00:13<03:11,  3.61s/it, clinical knowledge]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/265 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  23%|                | 61/265 [00:00<00:00, 601.65it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 122/265 [00:00<00:00, 605.02it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 183/265 [00:00<00:00, 605.89it/s]\u001b[A\n",
            "Formatting batches:  92%| | 244/265 [00:00<00:00, 605.74it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/67 [00:00<00:05, 11.67it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/67 [00:00<00:05, 11.17it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 6/67 [00:00<00:05, 11.17it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/67 [00:00<00:05, 11.37it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/67 [00:00<00:05, 11.35it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/67 [00:01<00:04, 11.32it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/67 [00:01<00:04, 11.61it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/67 [00:01<00:04, 11.35it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 18/67 [00:01<00:04, 11.48it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 20/67 [00:01<00:04, 11.31it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 22/67 [00:01<00:03, 11.44it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 24/67 [00:02<00:03, 11.42it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 26/67 [00:02<00:03, 11.66it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 28/67 [00:02<00:03, 11.55it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 30/67 [00:02<00:03, 11.62it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 32/67 [00:02<00:02, 11.81it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 34/67 [00:02<00:02, 11.67it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 36/67 [00:03<00:02, 11.54it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 38/67 [00:03<00:02, 11.46it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 40/67 [00:03<00:02, 11.17it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 42/67 [00:03<00:02, 11.09it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 44/67 [00:03<00:02, 10.99it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 46/67 [00:04<00:01, 11.21it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 48/67 [00:04<00:01, 11.23it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 50/67 [00:04<00:01, 11.25it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 52/67 [00:04<00:01, 11.25it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 54/67 [00:04<00:01, 11.15it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 56/67 [00:04<00:00, 11.18it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 58/67 [00:05<00:00, 11.31it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 60/67 [00:05<00:00, 11.45it/s]\u001b[A\n",
            "Predicting batches:  93%| | 62/67 [00:05<00:00, 11.37it/s]\u001b[A\n",
            "Predicting batches:  96%| | 64/67 [00:05<00:00, 11.48it/s]\u001b[A\n",
            "Predicting batches:  99%|| 66/67 [00:05<00:00, 11.40it/s]\u001b[A\n",
            "Processing subjects:   9%|     | 5/57 [00:19<03:57,  4.57s/it, college biology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/144 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 57/144 [00:00<00:00, 568.47it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 115/144 [00:00<00:00, 569.55it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/36 [00:00<00:03, 10.26it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/36 [00:00<00:03,  9.45it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 5/36 [00:00<00:03,  9.54it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 6/36 [00:00<00:03,  9.59it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 7/36 [00:00<00:03,  9.10it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 8/36 [00:00<00:03,  9.28it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 9/36 [00:00<00:02,  9.41it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 10/36 [00:01<00:02,  9.40it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 11/36 [00:01<00:02,  8.83it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 12/36 [00:01<00:02,  8.99it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 13/36 [00:01<00:02,  9.21it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 14/36 [00:01<00:02,  8.97it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 15/36 [00:01<00:02,  8.68it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 16/36 [00:01<00:02,  8.96it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 17/36 [00:01<00:02,  9.07it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 18/36 [00:01<00:02,  8.91it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 20/36 [00:02<00:01,  9.13it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 22/36 [00:02<00:01,  9.58it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 23/36 [00:02<00:01,  9.29it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 24/36 [00:02<00:01,  8.96it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 25/36 [00:02<00:01,  9.14it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 26/36 [00:02<00:01,  9.20it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 27/36 [00:02<00:00,  9.34it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 28/36 [00:03<00:00,  9.11it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 29/36 [00:03<00:00,  8.95it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 30/36 [00:03<00:00,  9.05it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 31/36 [00:03<00:00,  9.13it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 32/36 [00:03<00:00,  9.29it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 33/36 [00:03<00:00,  9.39it/s]\u001b[A\n",
            "Predicting batches:  94%| | 34/36 [00:03<00:00,  9.33it/s]\u001b[A\n",
            "Predicting batches:  97%|| 35/36 [00:03<00:00,  9.31it/s]\u001b[A\n",
            "Predicting batches: 100%|| 36/36 [00:03<00:00,  9.31it/s]\u001b[A\n",
            "Processing subjects:  11%|   | 6/57 [00:24<03:46,  4.43s/it, college chemistry]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  57%|         | 57/100 [00:00<00:00, 569.01it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  8.39it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  8.43it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.26it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  8.16it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  8.15it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  8.12it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  8.10it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:02,  7.90it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.83it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:01,  7.99it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  8.04it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  8.07it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.78it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:01,  7.86it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:01,  7.94it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  8.00it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  8.32it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  8.00it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  8.11it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.68it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00,  7.57it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.78it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  8.15it/s]\u001b[A\n",
            "Processing subjects:  12%| | 7/57 [00:27<03:23,  4.06s/it, college computer scie\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  51%|          | 51/100 [00:00<00:00, 501.35it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:04,  4.83it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:04,  4.98it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:04,  4.98it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:04,  5.03it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:03,  5.17it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:01<00:03,  5.33it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:01<00:03,  5.25it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:03,  5.05it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:03,  5.21it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:02,  5.28it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:02<00:02,  5.11it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:02<00:02,  4.98it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:02<00:02,  4.97it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:02<00:02,  5.02it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:02<00:01,  5.12it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:03<00:01,  4.98it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:03<00:01,  5.11it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:03<00:01,  5.05it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:03<00:01,  5.14it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:03<00:00,  5.01it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:04<00:00,  5.13it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:04<00:00,  5.11it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:04<00:00,  5.06it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:04<00:00,  5.03it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:04<00:00,  4.94it/s]\u001b[A\n",
            "Processing subjects:  14%| | 8/57 [00:32<03:35,  4.40s/it, college mathematics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  56%|         | 56/100 [00:00<00:00, 556.59it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:03,  7.77it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:03,  7.43it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  7.44it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  7.44it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  7.32it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  7.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  7.24it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:02,  7.24it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.08it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:02,  7.18it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.30it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.49it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.47it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.53it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:02<00:01,  7.56it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.47it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.46it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  7.23it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  7.24it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.44it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  7.59it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.48it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:03<00:00,  7.53it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.56it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  7.58it/s]\u001b[A\n",
            "Processing subjects:  16%|    | 9/57 [00:36<03:18,  4.14s/it, college medicine]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/173 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  32%|              | 56/173 [00:00<00:00, 550.66it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 113/173 [00:00<00:00, 559.07it/s]\u001b[A\n",
            "Formatting batches:  99%|| 171/173 [00:00<00:00, 565.33it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/44 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/44 [00:00<00:05,  8.45it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 2/44 [00:00<00:10,  4.17it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/44 [00:00<00:07,  5.55it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 4/44 [00:00<00:06,  6.41it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 5/44 [00:00<00:05,  7.27it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/44 [00:00<00:04,  7.91it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 7/44 [00:00<00:04,  8.27it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 8/44 [00:01<00:04,  8.04it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 10/44 [00:01<00:03,  8.81it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 11/44 [00:01<00:03,  8.58it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 12/44 [00:01<00:03,  8.73it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 13/44 [00:01<00:05,  5.84it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 15/44 [00:02<00:04,  6.95it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 16/44 [00:02<00:03,  7.43it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 17/44 [00:02<00:04,  5.41it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 18/44 [00:02<00:04,  6.07it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 19/44 [00:02<00:03,  6.58it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 20/44 [00:02<00:03,  7.13it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 21/44 [00:03<00:03,  7.33it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 22/44 [00:03<00:02,  7.87it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 23/44 [00:03<00:02,  7.92it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 24/44 [00:03<00:03,  5.38it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 25/44 [00:03<00:03,  6.12it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 27/44 [00:03<00:02,  7.41it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 28/44 [00:03<00:02,  7.84it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 29/44 [00:04<00:01,  7.97it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 30/44 [00:04<00:01,  8.26it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 31/44 [00:04<00:01,  8.50it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 32/44 [00:04<00:01,  8.67it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 33/44 [00:04<00:01,  8.54it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 34/44 [00:04<00:01,  8.80it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 35/44 [00:04<00:01,  9.00it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 36/44 [00:04<00:00,  9.01it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 38/44 [00:05<00:00,  9.27it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 39/44 [00:05<00:00,  9.24it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 40/44 [00:05<00:00,  9.20it/s]\u001b[A\n",
            "Predicting batches:  93%| | 41/44 [00:05<00:00,  5.99it/s]\u001b[A\n",
            "Predicting batches:  95%| | 42/44 [00:05<00:00,  6.62it/s]\u001b[A\n",
            "Predicting batches:  98%|| 43/44 [00:05<00:00,  7.24it/s]\u001b[A\n",
            "Processing subjects:  18%|    | 10/57 [00:42<03:44,  4.77s/it, college physics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  57%|         | 58/102 [00:00<00:00, 571.24it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/26 [00:00<00:02,  8.73it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/26 [00:00<00:02,  8.99it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/26 [00:00<00:02,  9.08it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/26 [00:00<00:02,  8.81it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 5/26 [00:00<00:02,  8.62it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 6/26 [00:00<00:02,  8.77it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 7/26 [00:00<00:02,  8.90it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 8/26 [00:00<00:02,  8.72it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 9/26 [00:01<00:01,  8.82it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 10/26 [00:01<00:01,  8.92it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 11/26 [00:01<00:01,  8.96it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 12/26 [00:01<00:01,  8.99it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 13/26 [00:01<00:01,  9.02it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 14/26 [00:01<00:01,  9.03it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 15/26 [00:01<00:01,  9.03it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 16/26 [00:01<00:01,  9.04it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 17/26 [00:01<00:01,  8.83it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 18/26 [00:02<00:00,  8.89it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 19/26 [00:02<00:00,  8.73it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 20/26 [00:02<00:00,  8.82it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 21/26 [00:02<00:00,  8.88it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 22/26 [00:02<00:00,  8.96it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 23/26 [00:02<00:00,  9.02it/s]\u001b[A\n",
            "Predicting batches:  92%| | 24/26 [00:02<00:00,  9.04it/s]\u001b[A\n",
            "Predicting batches:  96%| | 25/26 [00:02<00:00,  8.82it/s]\u001b[A\n",
            "Processing subjects:  19%|  | 11/57 [00:45<03:15,  4.25s/it, computer security]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 60/100 [00:00<00:00, 590.35it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  9.60it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  9.58it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  9.00it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:01, 10.24it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:01, 10.55it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:00<00:01, 10.56it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01, 10.92it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01, 10.93it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:00, 11.38it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:01<00:00, 11.00it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:01<00:00, 10.98it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:01<00:00, 11.13it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00, 11.39it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:02<00:00, 11.44it/s]\u001b[A\n",
            "Processing subjects:  21%| | 12/57 [00:47<02:46,  3.70s/it, conceptual physics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  27%|               | 63/235 [00:00<00:00, 623.42it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 126/235 [00:00<00:00, 625.81it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 189/235 [00:00<00:00, 623.90it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/59 [00:00<00:03, 15.35it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/59 [00:00<00:03, 14.99it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/59 [00:00<00:03, 15.07it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/59 [00:00<00:03, 14.59it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/59 [00:00<00:03, 14.60it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/59 [00:00<00:03, 14.64it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 14/59 [00:00<00:03, 14.74it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/59 [00:01<00:02, 14.68it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 18/59 [00:01<00:02, 14.63it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 20/59 [00:01<00:02, 14.78it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/59 [00:01<00:02, 14.53it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 24/59 [00:01<00:02, 14.35it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 26/59 [00:01<00:02, 14.41it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/59 [00:01<00:02, 14.62it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 30/59 [00:02<00:01, 14.52it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 32/59 [00:02<00:01, 14.56it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 34/59 [00:02<00:01, 14.85it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 36/59 [00:02<00:01, 14.92it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 38/59 [00:02<00:01, 14.80it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 40/59 [00:02<00:01, 14.54it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 42/59 [00:02<00:01, 14.58it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 44/59 [00:03<00:01, 14.62it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 46/59 [00:03<00:00, 14.65it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 48/59 [00:03<00:00, 14.43it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 50/59 [00:03<00:00, 14.46it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 52/59 [00:03<00:00, 14.61it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 54/59 [00:03<00:00, 14.59it/s]\u001b[A\n",
            "Predicting batches:  95%| | 56/59 [00:03<00:00, 14.74it/s]\u001b[A\n",
            "Predicting batches:  98%|| 58/59 [00:03<00:00, 14.76it/s]\u001b[A\n",
            "Processing subjects:  23%|      | 13/57 [00:52<02:52,  3.91s/it, econometrics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/114 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 54/114 [00:00<00:00, 533.95it/s]\u001b[A\n",
            "Formatting batches:  96%| | 109/114 [00:00<00:00, 540.12it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/29 [00:00<00:03,  7.49it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/29 [00:00<00:03,  7.33it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 3/29 [00:00<00:03,  7.03it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/29 [00:00<00:03,  6.88it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 5/29 [00:00<00:03,  6.82it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 6/29 [00:00<00:03,  6.92it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 7/29 [00:00<00:03,  7.22it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 8/29 [00:01<00:03,  6.91it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 9/29 [00:01<00:02,  6.99it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 10/29 [00:01<00:02,  7.00it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 11/29 [00:01<00:02,  6.90it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 12/29 [00:01<00:02,  6.95it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 13/29 [00:01<00:02,  6.98it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 14/29 [00:02<00:02,  6.89it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 15/29 [00:02<00:02,  6.84it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 16/29 [00:02<00:01,  6.80it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 17/29 [00:02<00:01,  6.75it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 18/29 [00:02<00:01,  6.64it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 19/29 [00:02<00:01,  6.84it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 20/29 [00:02<00:01,  6.78it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 21/29 [00:03<00:01,  6.75it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 22/29 [00:03<00:01,  6.93it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 23/29 [00:03<00:00,  6.96it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 24/29 [00:03<00:00,  6.99it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 25/29 [00:03<00:00,  6.89it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 26/29 [00:03<00:00,  7.03it/s]\u001b[A\n",
            "Predicting batches:  93%| | 27/29 [00:03<00:00,  6.82it/s]\u001b[A\n",
            "Predicting batches:  97%|| 28/29 [00:04<00:00,  6.79it/s]\u001b[A\n",
            "Processing subjects:  25%|| 14/57 [00:56<02:54,  4.05s/it, electrical engineeri\u001b[A\n",
            "Formatting batches:   0%|                               | 0/145 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  42%|            | 61/145 [00:00<00:00, 600.23it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 122/145 [00:00<00:00, 602.40it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/37 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/37 [00:00<00:03,  9.91it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/37 [00:00<00:03, 10.14it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 5/37 [00:00<00:03, 10.12it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 7/37 [00:00<00:02, 10.10it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/37 [00:00<00:02, 10.12it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 11/37 [00:01<00:02, 10.12it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 13/37 [00:01<00:02, 10.13it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 15/37 [00:01<00:02, 10.33it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 17/37 [00:01<00:01, 10.37it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 19/37 [00:01<00:01, 10.52it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 21/37 [00:02<00:01, 10.40it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 23/37 [00:02<00:01, 10.33it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 25/37 [00:02<00:01, 10.27it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 27/37 [00:02<00:00, 10.26it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 29/37 [00:02<00:00, 10.24it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 31/37 [00:03<00:00, 10.22it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 33/37 [00:03<00:00, 10.08it/s]\u001b[A\n",
            "Predicting batches:  95%| | 35/37 [00:03<00:00, 10.21it/s]\u001b[A\n",
            "Predicting batches: 100%|| 37/37 [00:03<00:00, 11.26it/s]\u001b[A\n",
            "Processing subjects:  26%|| 15/57 [01:00<02:46,  3.97s/it, elementary mathemati\u001b[A\n",
            "Formatting batches:   0%|                               | 0/378 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 58/378 [00:00<00:00, 572.53it/s]\u001b[A\n",
            "Formatting batches:  31%|             | 116/378 [00:00<00:00, 574.55it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 174/378 [00:00<00:00, 575.16it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 232/378 [00:00<00:00, 575.70it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 290/378 [00:00<00:00, 574.75it/s]\u001b[A\n",
            "Formatting batches:  92%| | 348/378 [00:00<00:00, 574.72it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/95 [00:00<00:09,  9.46it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/95 [00:00<00:10,  8.75it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 3/95 [00:00<00:11,  8.34it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 4/95 [00:00<00:10,  8.32it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 5/95 [00:00<00:10,  8.19it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 6/95 [00:00<00:10,  8.24it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 7/95 [00:00<00:10,  8.49it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 8/95 [00:00<00:10,  8.40it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 9/95 [00:01<00:10,  8.33it/s]\u001b[A\n",
            "Predicting batches:  11%|                    | 10/95 [00:01<00:10,  8.18it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 11/95 [00:01<00:10,  8.12it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 12/95 [00:01<00:10,  8.04it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 13/95 [00:01<00:10,  8.12it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 14/95 [00:01<00:10,  7.94it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 15/95 [00:01<00:10,  7.95it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 16/95 [00:01<00:09,  8.02it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 17/95 [00:02<00:09,  8.08it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 18/95 [00:02<00:09,  8.14it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 19/95 [00:02<00:09,  7.95it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 20/95 [00:02<00:09,  8.03it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 21/95 [00:02<00:08,  8.31it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 22/95 [00:02<00:08,  8.27it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 23/95 [00:02<00:08,  8.16it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 24/95 [00:02<00:08,  8.16it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 25/95 [00:03<00:08,  8.20it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 26/95 [00:03<00:08,  8.13it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 27/95 [00:03<00:08,  8.05it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 28/95 [00:03<00:08,  8.01it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 29/95 [00:03<00:08,  8.09it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 30/95 [00:03<00:07,  8.15it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 31/95 [00:03<00:07,  8.09it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 32/95 [00:03<00:07,  8.05it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 33/95 [00:04<00:07,  7.76it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 34/95 [00:04<00:07,  7.82it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 35/95 [00:04<00:07,  7.92it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 36/95 [00:04<00:07,  8.01it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 37/95 [00:04<00:07,  8.07it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 38/95 [00:04<00:07,  8.09it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 39/95 [00:04<00:06,  8.04it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 40/95 [00:04<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 41/95 [00:05<00:06,  8.10it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 42/95 [00:05<00:06,  8.16it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 43/95 [00:05<00:06,  8.21it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 44/95 [00:05<00:06,  8.45it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 45/95 [00:05<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 46/95 [00:05<00:05,  8.22it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 47/95 [00:05<00:05,  8.11it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 48/95 [00:05<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 49/95 [00:06<00:05,  8.17it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 50/95 [00:06<00:05,  8.09it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 51/95 [00:06<00:05,  8.15it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 52/95 [00:06<00:05,  8.05it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 53/95 [00:06<00:05,  8.11it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 54/95 [00:06<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 55/95 [00:06<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 56/95 [00:06<00:04,  8.21it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 57/95 [00:07<00:04,  7.99it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 58/95 [00:07<00:04,  7.94it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 59/95 [00:07<00:04,  8.03it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 60/95 [00:07<00:04,  8.11it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 61/95 [00:07<00:04,  8.13it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 62/95 [00:07<00:04,  8.15it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 63/95 [00:07<00:04,  7.95it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 64/95 [00:07<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 65/95 [00:08<00:03,  8.02it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 66/95 [00:08<00:03,  8.07it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 67/95 [00:08<00:03,  8.01it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 68/95 [00:08<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 69/95 [00:08<00:03,  8.11it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 70/95 [00:08<00:03,  8.14it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 71/95 [00:08<00:02,  8.15it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 72/95 [00:08<00:02,  8.09it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 73/95 [00:08<00:02,  8.14it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 74/95 [00:09<00:02,  8.19it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 75/95 [00:09<00:02,  8.22it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 76/95 [00:09<00:02,  8.11it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 77/95 [00:09<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 78/95 [00:09<00:02,  8.10it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 79/95 [00:09<00:01,  8.34it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 80/95 [00:09<00:01,  8.20it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 81/95 [00:09<00:01,  7.84it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 82/95 [00:10<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 83/95 [00:10<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 84/95 [00:10<00:01,  7.98it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 85/95 [00:10<00:01,  8.06it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 86/95 [00:10<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 87/95 [00:10<00:00,  8.03it/s]\u001b[A\n",
            "Predicting batches:  93%| | 88/95 [00:10<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  94%| | 89/95 [00:10<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  95%| | 90/95 [00:11<00:00,  8.25it/s]\u001b[A\n",
            "Predicting batches:  96%| | 91/95 [00:11<00:00,  8.23it/s]\u001b[A\n",
            "Predicting batches:  97%|| 92/95 [00:11<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  98%|| 93/95 [00:11<00:00,  8.15it/s]\u001b[A\n",
            "Predicting batches:  99%|| 94/95 [00:11<00:00,  8.16it/s]\u001b[A\n",
            "Processing subjects:  28%|     | 16/57 [01:12<04:25,  6.48s/it, formal logic]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 56/126 [00:00<00:00, 555.89it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 112/126 [00:00<00:00, 558.00it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/32 [00:00<00:04,  6.59it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/32 [00:00<00:04,  6.87it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 3/32 [00:00<00:04,  7.07it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/32 [00:00<00:03,  7.06it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 5/32 [00:00<00:03,  7.33it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 6/32 [00:00<00:03,  7.32it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 7/32 [00:00<00:03,  7.37it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 8/32 [00:01<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 9/32 [00:01<00:03,  7.29it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 10/32 [00:01<00:03,  7.29it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 11/32 [00:01<00:02,  7.09it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 12/32 [00:01<00:02,  6.85it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 13/32 [00:01<00:02,  6.59it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 14/32 [00:01<00:02,  6.91it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 15/32 [00:02<00:02,  6.96it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 16/32 [00:02<00:02,  6.84it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 17/32 [00:02<00:02,  7.02it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 18/32 [00:02<00:02,  6.78it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 19/32 [00:02<00:01,  6.97it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 20/32 [00:02<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 21/32 [00:02<00:01,  6.88it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 22/32 [00:03<00:01,  6.79it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 23/32 [00:03<00:01,  6.85it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 24/32 [00:03<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 25/32 [00:03<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 26/32 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 27/32 [00:03<00:00,  6.97it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 28/32 [00:04<00:00,  7.06it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 29/32 [00:04<00:00,  7.05it/s]\u001b[A\n",
            "Predicting batches:  94%| | 30/32 [00:04<00:00,  7.26it/s]\u001b[A\n",
            "Predicting batches:  97%|| 31/32 [00:04<00:00,  7.27it/s]\u001b[A\n",
            "Processing subjects:  30%|     | 17/57 [01:17<03:58,  5.95s/it, global facts]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 61/100 [00:00<00:00, 600.58it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:01, 11.54it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:01, 11.46it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 11.39it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 11.15it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 11.04it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01, 10.98it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01, 10.94it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 10.63it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 10.68it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 10.87it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00, 10.63it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:02<00:00, 10.83it/s]\u001b[A\n",
            "Processing subjects:  32%|| 18/57 [01:19<03:11,  4.90s/it, high school biology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/310 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  18%|                 | 57/310 [00:00<00:00, 564.46it/s]\u001b[A\n",
            "Formatting batches:  37%|            | 115/310 [00:00<00:00, 569.24it/s]\u001b[A\n",
            "Formatting batches:  56%|        | 173/310 [00:00<00:00, 570.48it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 231/310 [00:00<00:00, 568.89it/s]\u001b[A\n",
            "Formatting batches:  93%| | 288/310 [00:00<00:00, 569.11it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/78 [00:00<00:08,  9.42it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/78 [00:00<00:08,  9.23it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/78 [00:00<00:08,  8.55it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/78 [00:00<00:08,  8.71it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/78 [00:00<00:08,  8.40it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/78 [00:00<00:08,  8.36it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 7/78 [00:00<00:08,  8.32it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/78 [00:00<00:08,  8.31it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 9/78 [00:01<00:08,  8.51it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/78 [00:01<00:07,  8.63it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 11/78 [00:01<00:08,  8.36it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/78 [00:01<00:08,  8.21it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 13/78 [00:01<00:07,  8.42it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/78 [00:01<00:07,  8.56it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/78 [00:01<00:07,  8.34it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/78 [00:01<00:07,  8.30it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 17/78 [00:02<00:07,  8.49it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/78 [00:02<00:07,  8.16it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 19/78 [00:02<00:07,  8.16it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/78 [00:02<00:06,  8.38it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 21/78 [00:02<00:06,  8.30it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 22/78 [00:02<00:06,  8.27it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 23/78 [00:02<00:06,  8.46it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 24/78 [00:02<00:06,  8.59it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 25/78 [00:02<00:06,  8.67it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 26/78 [00:03<00:06,  8.26it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 27/78 [00:03<00:06,  8.11it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/78 [00:03<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 29/78 [00:03<00:06,  7.96it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 30/78 [00:03<00:05,  8.22it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 31/78 [00:03<00:05,  8.20it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 32/78 [00:03<00:05,  8.39it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 33/78 [00:03<00:05,  8.33it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 34/78 [00:04<00:05,  8.28it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 35/78 [00:04<00:05,  8.23it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 36/78 [00:04<00:04,  8.43it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 37/78 [00:04<00:05,  8.04it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/78 [00:04<00:04,  8.07it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 39/78 [00:04<00:04,  8.31it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 40/78 [00:04<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 41/78 [00:04<00:04,  8.14it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 42/78 [00:05<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 43/78 [00:05<00:04,  8.19it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 44/78 [00:05<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 45/78 [00:05<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 46/78 [00:05<00:04,  7.94it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 47/78 [00:05<00:03,  7.99it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/78 [00:05<00:03,  8.04it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 49/78 [00:05<00:03,  8.31it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 50/78 [00:06<00:03,  8.52it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 51/78 [00:06<00:03,  8.29it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 52/78 [00:06<00:03,  8.17it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 53/78 [00:06<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 54/78 [00:06<00:03,  7.97it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 55/78 [00:06<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 56/78 [00:06<00:02,  8.09it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 57/78 [00:06<00:02,  8.12it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 58/78 [00:07<00:02,  8.35it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 59/78 [00:07<00:02,  8.33it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 60/78 [00:07<00:02,  8.51it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 61/78 [00:07<00:02,  8.37it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 62/78 [00:07<00:01,  8.21it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 63/78 [00:07<00:01,  8.10it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 64/78 [00:07<00:01,  8.14it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 65/78 [00:07<00:01,  8.02it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 66/78 [00:07<00:01,  8.27it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 67/78 [00:08<00:01,  8.14it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 68/78 [00:08<00:01,  8.07it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 69/78 [00:08<00:01,  8.11it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 70/78 [00:08<00:01,  7.83it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 71/78 [00:08<00:00,  8.13it/s]\u001b[A\n",
            "Predicting batches:  92%| | 72/78 [00:08<00:00,  8.35it/s]\u001b[A\n",
            "Predicting batches:  94%| | 73/78 [00:08<00:00,  8.30it/s]\u001b[A\n",
            "Predicting batches:  95%| | 74/78 [00:08<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  96%| | 75/78 [00:09<00:00,  8.54it/s]\u001b[A\n",
            "Predicting batches:  97%|| 76/78 [00:09<00:00,  8.45it/s]\u001b[A\n",
            "Predicting batches:  99%|| 77/78 [00:09<00:00,  8.38it/s]\u001b[A\n",
            "Processing subjects:  33%|| 19/57 [01:29<04:03,  6.41s/it, high school chemistr\u001b[A\n",
            "Formatting batches:   0%|                               | 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 60/203 [00:00<00:00, 589.71it/s]\u001b[A\n",
            "Formatting batches:  59%|        | 120/203 [00:00<00:00, 590.07it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 180/203 [00:00<00:00, 590.33it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/51 [00:00<00:05,  9.39it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:00<00:05,  9.22it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/51 [00:00<00:05,  9.12it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:00<00:05,  8.70it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:00<00:05,  8.93it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:00<00:04,  9.06it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:00<00:04,  9.33it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:00<00:04,  9.19it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:01<00:04,  8.73it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:01<00:04,  8.79it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:01<00:04,  8.94it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:01<00:04,  8.93it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:01<00:04,  8.95it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:01<00:04,  9.07it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:01<00:04,  8.68it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:01<00:04,  8.53it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:01<00:03,  8.77it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:02<00:03,  8.44it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:02<00:03,  8.37it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:02<00:03,  8.21it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:02<00:03,  7.97it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:02<00:03,  7.96it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:02<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:02<00:03,  8.08it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:02<00:03,  8.41it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:03<00:02,  8.36it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:03<00:02,  8.62it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:03<00:02,  8.38it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:03<00:02,  8.19it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:03<00:02,  8.40it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:03<00:02,  8.65it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:03<00:02,  8.85it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:03<00:02,  8.48it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:03<00:02,  8.25it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:04<00:01,  8.54it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:04<00:01,  8.44it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:04<00:01,  8.22it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 38/51 [00:04<00:01,  8.17it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:04<00:01,  8.07it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:04<00:01,  8.01it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:04<00:01,  8.28it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:04<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:05<00:00,  8.34it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:05<00:00,  8.27it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:05<00:00,  8.45it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:05<00:00,  8.60it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:05<00:00,  8.72it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:05<00:00,  8.77it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:05<00:00,  8.80it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:05<00:00,  8.62it/s]\u001b[A\n",
            "Processing subjects:  35%|| 20/57 [01:36<03:55,  6.38s/it, high school computer\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 47/100 [00:00<00:00, 469.73it/s]\u001b[A\n",
            "Formatting batches:  97%|| 97/100 [00:00<00:00, 484.77it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:05,  4.44it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:05,  4.55it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:04,  4.73it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:04,  4.55it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:01<00:04,  4.58it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:01<00:04,  4.65it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:01<00:03,  4.70it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:03,  4.29it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:02<00:03,  4.34it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:02<00:03,  4.51it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:02<00:03,  4.54it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:02<00:02,  4.72it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:02<00:02,  4.76it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:03<00:02,  4.65it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:03<00:02,  4.51it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:03<00:01,  4.64it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:03<00:01,  4.53it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:03<00:01,  4.55it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:04<00:01,  4.50it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:04<00:01,  4.52it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:04<00:00,  4.49it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:04<00:00,  4.69it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:05<00:00,  4.73it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:05<00:00,  4.79it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:05<00:00,  4.81it/s]\u001b[A\n",
            "Processing subjects:  37%|| 21/57 [01:41<03:41,  6.15s/it, high school european\u001b[A\n",
            "Formatting batches:   0%|                               | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 28/165 [00:00<00:00, 276.86it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 57/165 [00:00<00:00, 279.06it/s]\u001b[A\n",
            "Formatting batches:  52%|          | 86/165 [00:00<00:00, 281.24it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 115/165 [00:00<00:00, 281.14it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 144/165 [00:00<00:00, 280.21it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/42 [00:00<00:34,  1.18it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/42 [00:01<00:34,  1.17it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/42 [00:02<00:32,  1.20it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/42 [00:03<00:31,  1.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 5/42 [00:04<00:31,  1.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/42 [00:05<00:30,  1.20it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 7/42 [00:05<00:28,  1.22it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/42 [00:06<00:28,  1.19it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 9/42 [00:07<00:28,  1.17it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/42 [00:08<00:27,  1.17it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 11/42 [00:09<00:26,  1.19it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/42 [00:10<00:25,  1.18it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 13/42 [00:10<00:24,  1.21it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/42 [00:11<00:22,  1.23it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 15/42 [00:12<00:22,  1.22it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 16/42 [00:13<00:21,  1.24it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 17/42 [00:14<00:20,  1.21it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 18/42 [00:15<00:20,  1.20it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 19/42 [00:15<00:19,  1.19it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 20/42 [00:16<00:18,  1.19it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 21/42 [00:17<00:17,  1.19it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 22/42 [00:18<00:16,  1.21it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 23/42 [00:19<00:15,  1.23it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 24/42 [00:19<00:14,  1.20it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 25/42 [00:20<00:14,  1.19it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 26/42 [00:21<00:13,  1.19it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 27/42 [00:22<00:12,  1.17it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 28/42 [00:23<00:11,  1.17it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 29/42 [00:24<00:10,  1.18it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 30/42 [00:25<00:10,  1.20it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 31/42 [00:25<00:09,  1.20it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 32/42 [00:26<00:08,  1.19it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 33/42 [00:27<00:07,  1.21it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 34/42 [00:28<00:06,  1.21it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 35/42 [00:29<00:05,  1.18it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 36/42 [00:30<00:05,  1.17it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 37/42 [00:30<00:04,  1.18it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 38/42 [00:31<00:03,  1.19it/s]\u001b[A\n",
            "Predicting batches:  93%| | 39/42 [00:32<00:02,  1.20it/s]\u001b[A\n",
            "Predicting batches:  95%| | 40/42 [00:33<00:01,  1.20it/s]\u001b[A\n",
            "Predicting batches:  98%|| 41/42 [00:34<00:00,  1.22it/s]\u001b[A\n",
            "Predicting batches: 100%|| 42/42 [00:34<00:00,  1.62it/s]\u001b[A\n",
            "Processing subjects:  39%|| 22/57 [02:16<08:38, 14.81s/it, high school geograph\u001b[A\n",
            "Formatting batches:   0%|                               | 0/198 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 60/198 [00:00<00:00, 599.56it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 121/198 [00:00<00:00, 603.08it/s]\u001b[A\n",
            "Formatting batches:  92%| | 182/198 [00:00<00:00, 602.00it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/50 [00:00<00:03, 12.58it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/50 [00:00<00:03, 12.27it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/50 [00:00<00:03, 12.05it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/50 [00:00<00:03, 12.09it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/50 [00:00<00:03, 11.77it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/50 [00:01<00:03, 11.84it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 14/50 [00:01<00:03, 11.92it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 16/50 [00:01<00:02, 11.90it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 18/50 [00:01<00:02, 11.96it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 20/50 [00:01<00:02, 11.93it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 22/50 [00:01<00:02, 11.88it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 24/50 [00:02<00:02, 11.52it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 26/50 [00:02<00:02, 11.60it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 28/50 [00:02<00:01, 11.69it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 30/50 [00:02<00:01, 11.77it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 32/50 [00:02<00:01, 11.79it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 34/50 [00:02<00:01, 11.81it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 36/50 [00:03<00:01, 11.89it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 38/50 [00:03<00:01, 11.97it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 40/50 [00:03<00:00, 11.76it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 42/50 [00:03<00:00, 11.76it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 44/50 [00:03<00:00, 11.87it/s]\u001b[A\n",
            "Predicting batches:  92%| | 46/50 [00:03<00:00, 11.81it/s]\u001b[A\n",
            "Predicting batches:  96%| | 48/50 [00:04<00:00, 11.62it/s]\u001b[A\n",
            "Predicting batches: 100%|| 50/50 [00:04<00:00, 12.36it/s]\u001b[A\n",
            "Processing subjects:  40%|| 23/57 [02:21<06:38, 11.72s/it, high school governme\u001b[A\n",
            "Formatting batches:   0%|                               | 0/193 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  28%|               | 55/193 [00:00<00:00, 542.01it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 112/193 [00:00<00:00, 553.10it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 168/193 [00:00<00:00, 549.74it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/49 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/49 [00:00<00:05,  9.52it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/49 [00:00<00:04,  9.44it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/49 [00:00<00:04,  9.38it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/49 [00:00<00:04,  9.35it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/49 [00:00<00:04,  9.32it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/49 [00:00<00:04,  9.29it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/49 [00:00<00:04,  9.46it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/49 [00:00<00:04,  9.41it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/49 [00:00<00:04,  9.37it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/49 [00:01<00:04,  9.22it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 11/49 [00:01<00:04,  9.12it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/49 [00:01<00:04,  9.18it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 13/49 [00:01<00:03,  9.20it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 14/49 [00:01<00:03,  9.23it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 15/49 [00:01<00:03,  9.14it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 16/49 [00:01<00:03,  9.18it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 17/49 [00:01<00:03,  9.40it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 18/49 [00:01<00:03,  9.37it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 19/49 [00:02<00:03,  9.35it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 20/49 [00:02<00:03,  9.33it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 21/49 [00:02<00:03,  9.31it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 22/49 [00:02<00:02,  9.28it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 23/49 [00:02<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 24/49 [00:02<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 25/49 [00:02<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 26/49 [00:02<00:02,  9.16it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 27/49 [00:02<00:02,  9.19it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 28/49 [00:03<00:02,  9.21it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 29/49 [00:03<00:02,  9.22it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 30/49 [00:03<00:02,  9.24it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 31/49 [00:03<00:01,  9.25it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 32/49 [00:03<00:01,  9.27it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 33/49 [00:03<00:01,  9.43it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 34/49 [00:03<00:01,  9.37it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 35/49 [00:03<00:01,  9.34it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 36/49 [00:03<00:01,  9.32it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 37/49 [00:03<00:01,  9.32it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 38/49 [00:04<00:01,  9.32it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 39/49 [00:04<00:01,  9.29it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 40/49 [00:04<00:01,  8.93it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 41/49 [00:04<00:00,  8.93it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 42/49 [00:04<00:00,  8.62it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 43/49 [00:04<00:00,  8.80it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 44/49 [00:04<00:00,  8.93it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 45/49 [00:04<00:00,  9.03it/s]\u001b[A\n",
            "Predicting batches:  94%| | 46/49 [00:04<00:00,  9.10it/s]\u001b[A\n",
            "Predicting batches:  96%| | 47/49 [00:05<00:00,  9.04it/s]\u001b[A\n",
            "Predicting batches:  98%|| 48/49 [00:05<00:00,  9.02it/s]\u001b[A\n",
            "Processing subjects:  42%|| 24/57 [02:26<05:26,  9.89s/it, high school macroeco\u001b[A\n",
            "Formatting batches:   0%|                               | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 60/390 [00:00<00:00, 594.15it/s]\u001b[A\n",
            "Formatting batches:  31%|             | 120/390 [00:00<00:00, 596.24it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 181/390 [00:00<00:00, 598.13it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 242/390 [00:00<00:00, 602.06it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 303/390 [00:00<00:00, 603.44it/s]\u001b[A\n",
            "Formatting batches:  93%| | 364/390 [00:00<00:00, 599.06it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/98 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/98 [00:00<00:08, 11.93it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 4/98 [00:00<00:07, 12.03it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 6/98 [00:00<00:08, 11.41it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 8/98 [00:00<00:07, 11.58it/s]\u001b[A\n",
            "Predicting batches:  10%|                    | 10/98 [00:00<00:07, 11.65it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 12/98 [00:01<00:07, 11.71it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 14/98 [00:01<00:07, 11.40it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 16/98 [00:01<00:07, 11.55it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 18/98 [00:01<00:06, 11.65it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 20/98 [00:01<00:06, 11.55it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 22/98 [00:01<00:06, 11.59it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 24/98 [00:02<00:06, 11.66it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 26/98 [00:02<00:06, 11.38it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 28/98 [00:02<00:06, 11.61it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 30/98 [00:02<00:05, 11.76it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 32/98 [00:02<00:05, 11.70it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 34/98 [00:02<00:05, 11.41it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 36/98 [00:03<00:05, 11.49it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 38/98 [00:03<00:05, 11.25it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 40/98 [00:03<00:05, 11.21it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 42/98 [00:03<00:04, 11.37it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 44/98 [00:03<00:04, 11.67it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 46/98 [00:03<00:04, 11.51it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 48/98 [00:04<00:04, 11.41it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 50/98 [00:04<00:04, 11.54it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 52/98 [00:04<00:04, 11.41it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 54/98 [00:04<00:03, 11.53it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 56/98 [00:04<00:03, 11.61it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 58/98 [00:05<00:03, 11.78it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 60/98 [00:05<00:03, 11.80it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 62/98 [00:05<00:03, 11.61it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 64/98 [00:05<00:02, 11.71it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 66/98 [00:05<00:02, 11.66it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 68/98 [00:05<00:02, 11.69it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 70/98 [00:06<00:02, 11.39it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 72/98 [00:06<00:02, 11.36it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 74/98 [00:06<00:02, 11.57it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 76/98 [00:06<00:01, 11.84it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 78/98 [00:06<00:01, 11.83it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 80/98 [00:06<00:01, 11.83it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 82/98 [00:07<00:01, 11.65it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 84/98 [00:07<00:01, 11.66it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 86/98 [00:07<00:01, 11.39it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 88/98 [00:07<00:00, 11.44it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 90/98 [00:07<00:00, 11.54it/s]\u001b[A\n",
            "Predicting batches:  94%| | 92/98 [00:07<00:00, 11.66it/s]\u001b[A\n",
            "Predicting batches:  96%| | 94/98 [00:08<00:00, 11.70it/s]\u001b[A\n",
            "Predicting batches:  98%|| 96/98 [00:08<00:00, 11.70it/s]\u001b[A\n",
            "Predicting batches: 100%|| 98/98 [00:08<00:00, 12.64it/s]\u001b[A\n",
            "Processing subjects:  44%|| 25/57 [02:35<05:08,  9.64s/it, high school mathemat\u001b[A\n",
            "Formatting batches:   0%|                               | 0/270 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  21%|                | 57/270 [00:00<00:00, 563.97it/s]\u001b[A\n",
            "Formatting batches:  42%|           | 114/270 [00:00<00:00, 543.63it/s]\u001b[A\n",
            "Formatting batches:  64%|       | 172/270 [00:00<00:00, 558.19it/s]\u001b[A\n",
            "Formatting batches:  85%|   | 230/270 [00:00<00:00, 564.86it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/68 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/68 [00:00<00:08,  8.28it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/68 [00:00<00:08,  7.66it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/68 [00:00<00:08,  7.91it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/68 [00:00<00:08,  7.89it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/68 [00:00<00:07,  7.99it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 6/68 [00:00<00:07,  8.28it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/68 [00:00<00:07,  8.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/68 [00:00<00:07,  8.18it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 9/68 [00:01<00:07,  8.12it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/68 [00:01<00:07,  8.11it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 11/68 [00:01<00:07,  7.88it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/68 [00:01<00:07,  7.87it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 13/68 [00:01<00:07,  7.73it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/68 [00:01<00:06,  7.75it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 15/68 [00:01<00:06,  8.05it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/68 [00:02<00:06,  7.94it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 17/68 [00:02<00:06,  7.96it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 18/68 [00:02<00:06,  7.99it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 19/68 [00:02<00:06,  8.03it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 20/68 [00:02<00:06,  7.94it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 21/68 [00:02<00:05,  8.18it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 22/68 [00:02<00:05,  8.05it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 23/68 [00:02<00:05,  7.97it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 24/68 [00:03<00:05,  7.92it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 25/68 [00:03<00:05,  8.17it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 26/68 [00:03<00:05,  8.34it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 27/68 [00:03<00:04,  8.28it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 28/68 [00:03<00:04,  8.22it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 29/68 [00:03<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 30/68 [00:03<00:04,  8.09it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 31/68 [00:03<00:04,  8.08it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 32/68 [00:03<00:04,  8.00it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 33/68 [00:04<00:04,  8.03it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 34/68 [00:04<00:04,  7.92it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 35/68 [00:04<00:04,  7.97it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 36/68 [00:04<00:04,  7.65it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 37/68 [00:04<00:03,  7.78it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 38/68 [00:04<00:03,  7.88it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 39/68 [00:04<00:03,  8.15it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 40/68 [00:04<00:03,  8.12it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 41/68 [00:05<00:03,  8.32it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 42/68 [00:05<00:03,  8.13it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 43/68 [00:05<00:03,  8.13it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 44/68 [00:05<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 45/68 [00:05<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 46/68 [00:05<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 47/68 [00:05<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 48/68 [00:05<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 49/68 [00:06<00:02,  7.98it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 50/68 [00:06<00:02,  8.02it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 51/68 [00:06<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 52/68 [00:06<00:01,  8.06it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 53/68 [00:06<00:01,  7.99it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 54/68 [00:06<00:01,  8.03it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 55/68 [00:06<00:01,  8.02it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 56/68 [00:06<00:01,  7.95it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 57/68 [00:07<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 58/68 [00:07<00:01,  7.95it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 59/68 [00:07<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 60/68 [00:07<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 61/68 [00:07<00:00,  7.94it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 62/68 [00:07<00:00,  7.78it/s]\u001b[A\n",
            "Predicting batches:  93%| | 63/68 [00:07<00:00,  7.88it/s]\u001b[A\n",
            "Predicting batches:  94%| | 64/68 [00:07<00:00,  7.86it/s]\u001b[A\n",
            "Predicting batches:  96%| | 65/68 [00:08<00:00,  7.96it/s]\u001b[A\n",
            "Predicting batches:  97%|| 66/68 [00:08<00:00,  8.03it/s]\u001b[A\n",
            "Predicting batches:  99%|| 67/68 [00:08<00:00,  7.68it/s]\u001b[A\n",
            "Processing subjects:  46%|| 26/57 [02:44<04:52,  9.43s/it, high school microeco\u001b[A\n",
            "Formatting batches:   0%|                               | 0/238 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  24%|                | 56/238 [00:00<00:00, 553.72it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 116/238 [00:00<00:00, 578.50it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 177/238 [00:00<00:00, 588.75it/s]\u001b[A\n",
            "Formatting batches: 100%|| 237/238 [00:00<00:00, 589.70it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/60 [00:00<00:06,  9.83it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/60 [00:00<00:05, 10.53it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/60 [00:00<00:05, 10.21it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 7/60 [00:00<00:05, 10.46it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/60 [00:00<00:04, 10.25it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 11/60 [00:01<00:04, 10.43it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 13/60 [00:01<00:04, 10.68it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 15/60 [00:01<00:04, 10.43it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 17/60 [00:01<00:03, 10.79it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 19/60 [00:01<00:03, 10.91it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 21/60 [00:01<00:03, 10.71it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 23/60 [00:02<00:03, 10.97it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 25/60 [00:02<00:03, 10.88it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 27/60 [00:02<00:03, 10.98it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 29/60 [00:02<00:02, 10.87it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 31/60 [00:02<00:02, 10.95it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 33/60 [00:03<00:02, 10.75it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 35/60 [00:03<00:02, 10.73it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 37/60 [00:03<00:02, 10.59it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 39/60 [00:03<00:01, 10.65it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 41/60 [00:03<00:01, 10.66it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 43/60 [00:04<00:01, 10.82it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 45/60 [00:04<00:01, 10.79it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 47/60 [00:04<00:01, 10.54it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 49/60 [00:04<00:01, 10.59it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 51/60 [00:04<00:00, 10.65it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 53/60 [00:04<00:00, 10.81it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 55/60 [00:05<00:00, 10.91it/s]\u001b[A\n",
            "Predicting batches:  95%| | 57/60 [00:05<00:00, 10.60it/s]\u001b[A\n",
            "Predicting batches:  98%|| 59/60 [00:05<00:00, 10.35it/s]\u001b[A\n",
            "Processing subjects:  47%|| 27/57 [02:50<04:12,  8.41s/it, high school physics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/151 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  36%|             | 55/151 [00:00<00:00, 548.93it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 112/151 [00:00<00:00, 555.89it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/38 [00:00<00:04,  7.45it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/38 [00:00<00:05,  6.68it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/38 [00:00<00:05,  6.99it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/38 [00:00<00:04,  7.00it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 5/38 [00:00<00:04,  7.13it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/38 [00:00<00:04,  7.35it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 7/38 [00:00<00:04,  7.36it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 8/38 [00:01<00:04,  7.37it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/38 [00:01<00:03,  7.50it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 10/38 [00:01<00:03,  7.57it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 11/38 [00:01<00:03,  7.51it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/38 [00:01<00:03,  7.67it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 13/38 [00:01<00:03,  7.18it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 14/38 [00:01<00:03,  7.41it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 15/38 [00:02<00:03,  7.40it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 16/38 [00:02<00:02,  7.57it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 17/38 [00:02<00:02,  7.60it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 18/38 [00:02<00:02,  7.42it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 19/38 [00:02<00:02,  7.54it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 20/38 [00:02<00:02,  7.37it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 21/38 [00:02<00:02,  7.50it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 22/38 [00:02<00:02,  7.60it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 23/38 [00:03<00:01,  7.62it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 24/38 [00:03<00:01,  7.67it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 25/38 [00:03<00:01,  7.68it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 26/38 [00:03<00:01,  7.60it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 27/38 [00:03<00:01,  7.63it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 28/38 [00:03<00:01,  7.57it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 29/38 [00:03<00:01,  7.39it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 30/38 [00:04<00:01,  7.11it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 31/38 [00:04<00:00,  7.36it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 32/38 [00:04<00:00,  7.21it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 33/38 [00:04<00:00,  7.37it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 34/38 [00:04<00:00,  7.58it/s]\u001b[A\n",
            "Predicting batches:  92%| | 35/38 [00:04<00:00,  7.64it/s]\u001b[A\n",
            "Predicting batches:  95%| | 36/38 [00:04<00:00,  7.57it/s]\u001b[A\n",
            "Predicting batches:  97%|| 37/38 [00:04<00:00,  7.62it/s]\u001b[A\n",
            "Predicting batches: 100%|| 38/38 [00:05<00:00,  8.15it/s]\u001b[A\n",
            "Processing subjects:  49%|| 28/57 [02:56<03:37,  7.49s/it, high school psycholo\u001b[A\n",
            "Formatting batches:   0%|                               | 0/545 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  10%|                  | 57/545 [00:00<00:00, 563.36it/s]\u001b[A\n",
            "Formatting batches:  21%|               | 114/545 [00:00<00:00, 565.06it/s]\u001b[A\n",
            "Formatting batches:  31%|             | 171/545 [00:00<00:00, 564.23it/s]\u001b[A\n",
            "Formatting batches:  42%|           | 228/545 [00:00<00:00, 564.79it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 285/545 [00:00<00:00, 563.11it/s]\u001b[A\n",
            "Formatting batches:  63%|       | 342/545 [00:00<00:00, 562.04it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 399/545 [00:00<00:00, 562.56it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 456/545 [00:00<00:00, 564.54it/s]\u001b[A\n",
            "Formatting batches:  94%| | 513/545 [00:00<00:00, 563.67it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/137 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 1/137 [00:00<00:13,  9.82it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/137 [00:00<00:15,  8.80it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 3/137 [00:00<00:15,  8.92it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 4/137 [00:00<00:14,  8.92it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 5/137 [00:00<00:14,  8.90it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 6/137 [00:00<00:14,  8.94it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 7/137 [00:00<00:14,  9.07it/s]\u001b[A\n",
            "Predicting batches:   6%|                     | 8/137 [00:00<00:14,  8.62it/s]\u001b[A\n",
            "Predicting batches:   7%|                     | 9/137 [00:01<00:15,  8.46it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 10/137 [00:01<00:16,  7.76it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 11/137 [00:01<00:15,  8.16it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 12/137 [00:01<00:14,  8.37it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 13/137 [00:01<00:14,  8.30it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 14/137 [00:01<00:14,  8.48it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 15/137 [00:01<00:15,  7.96it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 16/137 [00:01<00:14,  8.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 17/137 [00:02<00:14,  8.50it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 18/137 [00:02<00:13,  8.59it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 19/137 [00:02<00:13,  8.79it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 20/137 [00:02<00:13,  8.56it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 21/137 [00:02<00:13,  8.65it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 22/137 [00:02<00:13,  8.71it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 23/137 [00:02<00:12,  8.78it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 24/137 [00:02<00:12,  8.81it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 25/137 [00:02<00:12,  8.94it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 26/137 [00:03<00:12,  8.91it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 27/137 [00:03<00:12,  9.02it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 28/137 [00:03<00:12,  8.69it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 29/137 [00:03<00:12,  8.85it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 30/137 [00:03<00:12,  8.86it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 31/137 [00:03<00:12,  8.51it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 32/137 [00:03<00:12,  8.62it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 33/137 [00:03<00:12,  8.44it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 34/137 [00:03<00:12,  8.55it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 35/137 [00:04<00:11,  8.63it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 36/137 [00:04<00:11,  8.72it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 37/137 [00:04<00:11,  8.75it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 38/137 [00:04<00:11,  8.89it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 39/137 [00:04<00:10,  9.00it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 40/137 [00:04<00:10,  8.95it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 41/137 [00:04<00:10,  9.05it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 42/137 [00:04<00:11,  8.64it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 43/137 [00:04<00:10,  8.72it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 44/137 [00:05<00:10,  8.75it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 45/137 [00:05<00:10,  8.78it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 46/137 [00:05<00:10,  8.91it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 47/137 [00:05<00:10,  8.67it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 48/137 [00:05<00:10,  8.71it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 49/137 [00:05<00:10,  8.75it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 50/137 [00:05<00:09,  8.88it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 51/137 [00:05<00:09,  8.98it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 52/137 [00:05<00:09,  8.70it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 53/137 [00:06<00:09,  8.48it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 54/137 [00:06<00:09,  8.58it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 55/137 [00:06<00:09,  8.43it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 56/137 [00:06<00:09,  8.54it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 57/137 [00:06<00:09,  8.72it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 58/137 [00:06<00:09,  8.74it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 59/137 [00:06<00:08,  8.79it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 60/137 [00:06<00:09,  8.23it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 61/137 [00:07<00:08,  8.50it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 62/137 [00:07<00:09,  7.99it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 63/137 [00:07<00:09,  8.22it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 64/137 [00:07<00:08,  8.42it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 65/137 [00:07<00:08,  8.57it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 66/137 [00:07<00:08,  8.67it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 67/137 [00:07<00:08,  8.72it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 68/137 [00:07<00:08,  8.50it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 69/137 [00:07<00:07,  8.70it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 70/137 [00:08<00:07,  8.77it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 71/137 [00:08<00:07,  8.52it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 72/137 [00:08<00:07,  8.63it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 73/137 [00:08<00:07,  8.80it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 74/137 [00:08<00:07,  8.58it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 75/137 [00:08<00:07,  8.41it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 76/137 [00:08<00:07,  8.53it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 77/137 [00:08<00:06,  8.73it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 78/137 [00:09<00:06,  8.87it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 79/137 [00:09<00:06,  8.96it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 80/137 [00:09<00:06,  8.69it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 81/137 [00:09<00:06,  8.85it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 82/137 [00:09<00:06,  8.83it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 83/137 [00:09<00:06,  8.33it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 84/137 [00:09<00:06,  8.58it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 85/137 [00:09<00:06,  8.40it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 86/137 [00:09<00:06,  8.32it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 87/137 [00:10<00:05,  8.56it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 88/137 [00:10<00:05,  8.42it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 89/137 [00:10<00:05,  8.33it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 90/137 [00:10<00:05,  8.50it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 91/137 [00:10<00:05,  8.38it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 92/137 [00:10<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 93/137 [00:10<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 94/137 [00:10<00:05,  8.50it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 95/137 [00:11<00:04,  8.70it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 96/137 [00:11<00:04,  8.73it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 97/137 [00:11<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 98/137 [00:11<00:04,  8.39it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 99/137 [00:11<00:04,  8.55it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 100/137 [00:11<00:04,  8.37it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 101/137 [00:11<00:04,  8.60it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 102/137 [00:11<00:03,  8.78it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 103/137 [00:11<00:03,  8.58it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 104/137 [00:12<00:03,  8.76it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 105/137 [00:12<00:03,  8.24it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 106/137 [00:12<00:03,  8.43it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 107/137 [00:12<00:03,  8.29it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 108/137 [00:12<00:03,  8.12it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 109/137 [00:12<00:03,  8.42it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 110/137 [00:12<00:03,  8.53it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 111/137 [00:12<00:02,  8.72it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 112/137 [00:13<00:02,  8.86it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 113/137 [00:13<00:02,  8.97it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 114/137 [00:13<00:02,  8.95it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 115/137 [00:13<00:02,  8.91it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 116/137 [00:13<00:02,  8.65it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 117/137 [00:13<00:02,  8.48it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 118/137 [00:13<00:02,  8.68it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 119/137 [00:13<00:02,  8.51it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 120/137 [00:13<00:01,  8.63it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 121/137 [00:14<00:01,  8.79it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 122/137 [00:14<00:01,  8.46it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 123/137 [00:14<00:01,  8.58it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 124/137 [00:14<00:01,  8.65it/s]\u001b[A\n",
            "Predicting batches:  91%| | 125/137 [00:14<00:01,  8.81it/s]\u001b[A\n",
            "Predicting batches:  92%| | 126/137 [00:14<00:01,  8.17it/s]\u001b[A\n",
            "Predicting batches:  93%| | 127/137 [00:14<00:01,  8.35it/s]\u001b[A\n",
            "Predicting batches:  93%| | 128/137 [00:14<00:01,  8.50it/s]\u001b[A\n",
            "Predicting batches:  94%| | 129/137 [00:15<00:00,  8.39it/s]\u001b[A\n",
            "Predicting batches:  95%| | 130/137 [00:15<00:00,  8.18it/s]\u001b[A\n",
            "Predicting batches:  96%| | 131/137 [00:15<00:00,  8.38it/s]\u001b[A\n",
            "Predicting batches:  96%|| 132/137 [00:15<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  97%|| 133/137 [00:15<00:00,  8.42it/s]\u001b[A\n",
            "Predicting batches:  98%|| 134/137 [00:15<00:00,  8.52it/s]\u001b[A\n",
            "Predicting batches:  99%|| 135/137 [00:15<00:00,  8.60it/s]\u001b[A\n",
            "Predicting batches:  99%|| 136/137 [00:15<00:00,  8.34it/s]\u001b[A\n",
            "Processing subjects:  51%|| 29/57 [03:13<04:48, 10.30s/it, high school statisti\u001b[A\n",
            "Formatting batches:   0%|                               | 0/216 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  23%|                | 49/216 [00:00<00:00, 481.54it/s]\u001b[A\n",
            "Formatting batches:  46%|           | 99/216 [00:00<00:00, 490.33it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 150/216 [00:00<00:00, 497.36it/s]\u001b[A\n",
            "Formatting batches:  93%| | 201/216 [00:00<00:00, 501.19it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/54 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/54 [00:00<00:09,  5.44it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/54 [00:00<00:10,  5.07it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/54 [00:00<00:09,  5.24it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/54 [00:00<00:09,  5.44it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 5/54 [00:00<00:09,  5.40it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 6/54 [00:01<00:08,  5.42it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 7/54 [00:01<00:08,  5.37it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 8/54 [00:01<00:08,  5.18it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 9/54 [00:01<00:08,  5.12it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 10/54 [00:01<00:08,  5.01it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 11/54 [00:02<00:08,  5.10it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 12/54 [00:02<00:08,  5.06it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 13/54 [00:02<00:08,  4.97it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 14/54 [00:02<00:07,  5.07it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 15/54 [00:02<00:07,  5.17it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 16/54 [00:03<00:07,  5.12it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 17/54 [00:03<00:07,  5.19it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 18/54 [00:03<00:06,  5.20it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 19/54 [00:03<00:06,  5.27it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 20/54 [00:03<00:06,  5.32it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 21/54 [00:04<00:06,  5.27it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 22/54 [00:04<00:06,  5.11it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 23/54 [00:04<00:06,  5.07it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 24/54 [00:04<00:05,  5.04it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 25/54 [00:04<00:05,  5.08it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 26/54 [00:05<00:05,  5.18it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 27/54 [00:05<00:05,  5.22it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 28/54 [00:05<00:04,  5.27it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 29/54 [00:05<00:04,  5.24it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 30/54 [00:05<00:04,  5.25it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 31/54 [00:05<00:04,  5.16it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 32/54 [00:06<00:04,  5.22it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 33/54 [00:06<00:04,  5.15it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 34/54 [00:06<00:03,  5.17it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 35/54 [00:06<00:03,  4.80it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 36/54 [00:06<00:03,  4.91it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 37/54 [00:07<00:03,  4.91it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 38/54 [00:07<00:03,  4.90it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 39/54 [00:07<00:03,  4.85it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 40/54 [00:07<00:02,  5.08it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 41/54 [00:07<00:02,  5.14it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 42/54 [00:08<00:02,  5.21it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 43/54 [00:08<00:02,  5.12it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 44/54 [00:08<00:01,  5.29it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 45/54 [00:08<00:01,  5.26it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 46/54 [00:08<00:01,  5.15it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 47/54 [00:09<00:01,  5.17it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 48/54 [00:09<00:01,  5.18it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 49/54 [00:09<00:00,  5.23it/s]\u001b[A\n",
            "Predicting batches:  93%| | 50/54 [00:09<00:00,  5.39it/s]\u001b[A\n",
            "Predicting batches:  94%| | 51/54 [00:09<00:00,  5.33it/s]\u001b[A\n",
            "Predicting batches:  96%|| 52/54 [00:10<00:00,  5.29it/s]\u001b[A\n",
            "Predicting batches:  98%|| 53/54 [00:10<00:00,  5.42it/s]\u001b[A\n",
            "Predicting batches: 100%|| 54/54 [00:10<00:00,  5.28it/s]\u001b[A\n",
            "Processing subjects:  53%|| 30/57 [03:23<04:42, 10.47s/it, high school us histo\u001b[A\n",
            "Formatting batches:   0%|                               | 0/204 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 32/204 [00:00<00:00, 318.96it/s]\u001b[A\n",
            "Formatting batches:  32%|              | 65/204 [00:00<00:00, 322.76it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 99/204 [00:00<00:00, 326.14it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 132/204 [00:00<00:00, 324.88it/s]\u001b[A\n",
            "Formatting batches:  81%|   | 165/204 [00:00<00:00, 325.48it/s]\u001b[A\n",
            "Formatting batches:  97%|| 198/204 [00:00<00:00, 325.59it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/51 [00:00<00:30,  1.63it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:01<00:29,  1.63it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/51 [00:01<00:29,  1.64it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:02<00:28,  1.63it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:03<00:28,  1.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:03<00:27,  1.62it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:04<00:27,  1.63it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:04<00:26,  1.60it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:05<00:26,  1.58it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:06<00:26,  1.56it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:06<00:25,  1.56it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:07<00:24,  1.58it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:09<00:22,  1.59it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:10<00:22,  1.58it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:10<00:21,  1.57it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:11<00:21,  1.56it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:11<00:20,  1.58it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:12<00:19,  1.59it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:13<00:18,  1.59it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:13<00:18,  1.58it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:14<00:17,  1.59it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:15<00:16,  1.60it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:15<00:16,  1.61it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:16<00:15,  1.61it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:16<00:15,  1.58it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:17<00:14,  1.59it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:18<00:13,  1.57it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:18<00:13,  1.56it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:19<00:12,  1.57it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:20<00:11,  1.59it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:20<00:11,  1.60it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:21<00:10,  1.61it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:22<00:10,  1.58it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:22<00:09,  1.59it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:23<00:08,  1.61it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 38/51 [00:23<00:08,  1.61it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:24<00:07,  1.61it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:25<00:06,  1.59it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:25<00:06,  1.58it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:26<00:05,  1.58it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:27<00:05,  1.59it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:27<00:04,  1.56it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:28<00:03,  1.55it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:28<00:03,  1.55it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:29<00:02,  1.55it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:30<00:01,  1.57it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:30<00:01,  1.58it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:31<00:00,  1.59it/s]\u001b[A\n",
            "Predicting batches: 100%|| 51/51 [00:32<00:00,  1.59it/s]\u001b[A\n",
            "Processing subjects:  54%|| 31/57 [03:56<07:26, 17.16s/it, high school world hi\u001b[A\n",
            "Formatting batches:   0%|                               | 0/237 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 39/237 [00:00<00:00, 388.44it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 78/237 [00:00<00:00, 387.42it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 118/237 [00:00<00:00, 390.14it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 158/237 [00:00<00:00, 391.87it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 198/237 [00:00<00:00, 391.47it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/60 [00:00<00:23,  2.51it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/60 [00:00<00:21,  2.69it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/60 [00:01<00:22,  2.56it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/60 [00:01<00:21,  2.58it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/60 [00:01<00:21,  2.58it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/60 [00:02<00:22,  2.41it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 7/60 [00:02<00:21,  2.41it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 8/60 [00:03<00:20,  2.53it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/60 [00:03<00:21,  2.40it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/60 [00:04<00:20,  2.41it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 11/60 [00:04<00:19,  2.47it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/60 [00:04<00:19,  2.51it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 13/60 [00:05<00:18,  2.49it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 14/60 [00:05<00:19,  2.40it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 15/60 [00:06<00:18,  2.49it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/60 [00:06<00:17,  2.52it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 17/60 [00:06<00:16,  2.62it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 18/60 [00:07<00:16,  2.61it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 19/60 [00:07<00:16,  2.51it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 20/60 [00:07<00:15,  2.55it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 21/60 [00:08<00:15,  2.51it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/60 [00:08<00:16,  2.35it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 23/60 [00:09<00:15,  2.42it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 24/60 [00:09<00:14,  2.43it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 25/60 [00:10<00:14,  2.47it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 26/60 [00:10<00:13,  2.52it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 27/60 [00:10<00:13,  2.42it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/60 [00:11<00:13,  2.36it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 29/60 [00:11<00:13,  2.33it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 30/60 [00:12<00:12,  2.43it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 31/60 [00:12<00:11,  2.50it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 32/60 [00:12<00:11,  2.44it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 33/60 [00:13<00:10,  2.48it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 34/60 [00:13<00:10,  2.50it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 35/60 [00:14<00:10,  2.46it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 36/60 [00:14<00:09,  2.50it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 37/60 [00:14<00:09,  2.55it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 38/60 [00:15<00:08,  2.60it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 39/60 [00:15<00:08,  2.54it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 40/60 [00:16<00:07,  2.57it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 41/60 [00:16<00:07,  2.58it/s]\u001b[A\n",
            "Predicting batches:  70%|       | 42/60 [00:16<00:07,  2.45it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 43/60 [00:17<00:07,  2.41it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 44/60 [00:17<00:06,  2.41it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 45/60 [00:18<00:06,  2.38it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 46/60 [00:18<00:05,  2.48it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 47/60 [00:18<00:05,  2.50it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 48/60 [00:19<00:04,  2.42it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 49/60 [00:19<00:04,  2.53it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 50/60 [00:20<00:03,  2.57it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 51/60 [00:20<00:03,  2.58it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 52/60 [00:20<00:03,  2.47it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 53/60 [00:21<00:02,  2.46it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 54/60 [00:21<00:02,  2.41it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 55/60 [00:22<00:02,  2.48it/s]\u001b[A\n",
            "Predicting batches:  93%| | 56/60 [00:22<00:01,  2.55it/s]\u001b[A\n",
            "Predicting batches:  95%| | 57/60 [00:22<00:01,  2.45it/s]\u001b[A\n",
            "Predicting batches:  97%|| 58/60 [00:23<00:00,  2.52it/s]\u001b[A\n",
            "Predicting batches:  98%|| 59/60 [00:23<00:00,  2.64it/s]\u001b[A\n",
            "Processing subjects:  56%|    | 32/57 [04:21<08:03, 19.33s/it, human aging]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/223 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  26%|               | 59/223 [00:00<00:00, 584.61it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 120/223 [00:00<00:00, 596.48it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 183/223 [00:00<00:00, 608.66it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/56 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/56 [00:00<00:03, 13.94it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/56 [00:00<00:03, 13.51it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 6/56 [00:00<00:03, 13.48it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/56 [00:00<00:03, 13.55it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 10/56 [00:00<00:03, 13.75it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 12/56 [00:00<00:03, 13.67it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 14/56 [00:01<00:03, 13.81it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 16/56 [00:01<00:02, 13.70it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 18/56 [00:01<00:02, 13.61it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 20/56 [00:01<00:02, 13.81it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 22/56 [00:01<00:02, 13.76it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 24/56 [00:01<00:02, 13.84it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 26/56 [00:01<00:02, 13.86it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 28/56 [00:02<00:02, 13.94it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 30/56 [00:02<00:01, 13.96it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 32/56 [00:02<00:01, 13.85it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 34/56 [00:02<00:01, 13.86it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 36/56 [00:02<00:01, 13.74it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 38/56 [00:02<00:01, 13.63it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 40/56 [00:02<00:01, 13.54it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 42/56 [00:03<00:01, 13.55it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 44/56 [00:03<00:00, 13.65it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 46/56 [00:03<00:00, 13.80it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 48/56 [00:03<00:00, 13.71it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 50/56 [00:03<00:00, 13.48it/s]\u001b[A\n",
            "Predicting batches:  93%| | 52/56 [00:03<00:00, 13.63it/s]\u001b[A\n",
            "Predicting batches:  96%|| 54/56 [00:03<00:00, 13.76it/s]\u001b[A\n",
            "Predicting batches: 100%|| 56/56 [00:04<00:00, 14.26it/s]\u001b[A\n",
            "Processing subjects:  58%|  | 33/57 [04:25<05:56, 14.87s/it, human sexuality]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  45%|           | 59/131 [00:00<00:00, 588.28it/s]\u001b[A\n",
            "Formatting batches:  92%| | 120/131 [00:00<00:00, 596.68it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/33 [00:00<00:02, 12.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/33 [00:00<00:02, 12.38it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 6/33 [00:00<00:02, 12.64it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 8/33 [00:00<00:02, 12.28it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 10/33 [00:00<00:01, 12.43it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 12/33 [00:00<00:01, 12.42it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 14/33 [00:01<00:01, 12.72it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 16/33 [00:01<00:01, 12.66it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 18/33 [00:01<00:01, 12.69it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 20/33 [00:01<00:01, 12.47it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 22/33 [00:01<00:00, 12.67it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 24/33 [00:01<00:00, 12.59it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 26/33 [00:02<00:00, 11.84it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 28/33 [00:02<00:00, 11.92it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 30/33 [00:02<00:00, 11.99it/s]\u001b[A\n",
            "Predicting batches:  97%|| 32/33 [00:02<00:00, 12.10it/s]\u001b[A\n",
            "Processing subjects:  60%| | 34/57 [04:28<04:19, 11.27s/it, international law]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/121 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  42%|            | 51/121 [00:00<00:00, 502.12it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 105/121 [00:00<00:00, 518.69it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/31 [00:00<00:04,  6.86it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/31 [00:00<00:04,  6.69it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 3/31 [00:00<00:04,  6.81it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 4/31 [00:00<00:04,  6.70it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 5/31 [00:00<00:03,  6.53it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 6/31 [00:00<00:03,  6.54it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 7/31 [00:01<00:03,  6.44it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 8/31 [00:01<00:03,  6.47it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 9/31 [00:01<00:03,  6.50it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 10/31 [00:01<00:03,  6.64it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 11/31 [00:01<00:03,  6.62it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 12/31 [00:01<00:02,  6.60it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 13/31 [00:01<00:02,  6.68it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 14/31 [00:02<00:02,  6.76it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 15/31 [00:02<00:02,  6.69it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 16/31 [00:02<00:02,  6.65it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 17/31 [00:02<00:02,  6.62it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 18/31 [00:02<00:01,  6.70it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 19/31 [00:02<00:01,  6.63it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 20/31 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 21/31 [00:03<00:01,  6.46it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 22/31 [00:03<00:01,  6.61it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 23/31 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 24/31 [00:03<00:01,  6.67it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 25/31 [00:03<00:00,  6.72it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 26/31 [00:03<00:00,  6.64it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 27/31 [00:04<00:00,  6.71it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 28/31 [00:04<00:00,  6.76it/s]\u001b[A\n",
            "Predicting batches:  94%| | 29/31 [00:04<00:00,  6.68it/s]\u001b[A\n",
            "Predicting batches:  97%|| 30/31 [00:04<00:00,  6.64it/s]\u001b[A\n",
            "Processing subjects:  61%|  | 35/57 [04:33<03:25,  9.33s/it, jurisprudence]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/108 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  55%|         | 59/108 [00:00<00:00, 587.28it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/27 [00:00<00:02, 11.15it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/27 [00:00<00:02, 10.42it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 6/27 [00:00<00:01, 10.52it/s]\u001b[A\n",
            "Predicting batches:  30%|                 | 8/27 [00:00<00:01, 10.73it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 10/27 [00:00<00:01, 10.70it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 12/27 [00:01<00:01, 10.83it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 14/27 [00:01<00:01, 10.93it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 16/27 [00:01<00:01, 10.58it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 18/27 [00:01<00:00, 10.62it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 20/27 [00:01<00:00, 10.40it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 22/27 [00:02<00:00, 10.74it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 24/27 [00:02<00:00, 11.01it/s]\u001b[A\n",
            "Predicting batches:  96%|| 26/27 [00:02<00:00, 10.92it/s]\u001b[A\n",
            "Processing subjects:  63%| | 36/57 [04:35<02:34,  7.34s/it, logical fallacies]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/163 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  34%|              | 55/163 [00:00<00:00, 547.46it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 113/163 [00:00<00:00, 562.10it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/41 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/41 [00:00<00:04,  9.95it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/41 [00:00<00:04,  9.58it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/41 [00:00<00:03,  9.75it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/41 [00:00<00:03,  9.81it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 5/41 [00:00<00:03,  9.62it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 6/41 [00:00<00:03,  9.50it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 7/41 [00:00<00:03,  9.61it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 8/41 [00:00<00:03,  9.38it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 9/41 [00:00<00:03,  9.33it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/41 [00:01<00:03,  9.29it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 11/41 [00:01<00:03,  9.17it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/41 [00:01<00:03,  9.20it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 13/41 [00:01<00:03,  9.22it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 14/41 [00:01<00:02,  9.23it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 15/41 [00:01<00:02,  9.24it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 16/41 [00:01<00:02,  9.15it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 17/41 [00:01<00:02,  9.38it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 18/41 [00:01<00:02,  9.36it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 19/41 [00:02<00:02,  9.23it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 20/41 [00:02<00:02,  9.24it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 21/41 [00:02<00:02,  9.44it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 22/41 [00:02<00:02,  9.38it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 23/41 [00:02<00:01,  9.23it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 24/41 [00:02<00:01,  9.22it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 25/41 [00:02<00:01,  9.37it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 26/41 [00:02<00:01,  9.33it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 27/41 [00:02<00:01,  9.31it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 28/41 [00:02<00:01,  9.27it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 29/41 [00:03<00:01,  9.44it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 30/41 [00:03<00:01,  9.54it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 31/41 [00:03<00:01,  9.45it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 32/41 [00:03<00:00,  9.00it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 33/41 [00:03<00:00,  9.27it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 34/41 [00:03<00:00,  9.26it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 35/41 [00:03<00:00,  9.24it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 36/41 [00:03<00:00,  9.40it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 37/41 [00:03<00:00,  9.21it/s]\u001b[A\n",
            "Predicting batches:  93%| | 38/41 [00:04<00:00,  9.11it/s]\u001b[A\n",
            "Predicting batches:  95%| | 39/41 [00:04<00:00,  9.15it/s]\u001b[A\n",
            "Predicting batches:  98%|| 40/41 [00:04<00:00,  9.17it/s]\u001b[A\n",
            "Processing subjects:  65%| | 37/57 [04:40<02:10,  6.54s/it, machine learning]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 53/112 [00:00<00:00, 526.54it/s]\u001b[A\n",
            "Formatting batches:  95%| | 106/112 [00:00<00:00, 527.56it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/28 [00:00<00:03,  6.82it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/28 [00:00<00:03,  6.67it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 3/28 [00:00<00:03,  6.62it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/28 [00:00<00:03,  6.59it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 5/28 [00:00<00:03,  6.54it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 6/28 [00:00<00:03,  6.52it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 7/28 [00:01<00:03,  6.53it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 8/28 [00:01<00:03,  6.32it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 9/28 [00:01<00:02,  6.36it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 10/28 [00:01<00:02,  6.40it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 11/28 [00:01<00:02,  6.24it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 12/28 [00:01<00:02,  6.32it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 13/28 [00:02<00:02,  6.30it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 14/28 [00:02<00:02,  6.46it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 15/28 [00:02<00:02,  6.39it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 16/28 [00:02<00:01,  6.42it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 17/28 [00:02<00:01,  6.27it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 18/28 [00:02<00:01,  6.34it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 19/28 [00:02<00:01,  6.39it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 20/28 [00:03<00:01,  6.35it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 21/28 [00:03<00:01,  6.39it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 22/28 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 23/28 [00:03<00:00,  6.45it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 24/28 [00:03<00:00,  6.39it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 25/28 [00:03<00:00,  6.34it/s]\u001b[A\n",
            "Predicting batches:  93%| | 26/28 [00:04<00:00,  5.96it/s]\u001b[A\n",
            "Predicting batches:  96%|| 27/28 [00:04<00:00,  5.76it/s]\u001b[A\n",
            "Predicting batches: 100%|| 28/28 [00:04<00:00,  5.88it/s]\u001b[A\n",
            "Processing subjects:  67%|   | 38/57 [04:45<01:53,  5.98s/it, management]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/103 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 63/103 [00:00<00:00, 625.80it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/26 [00:00<00:01, 15.96it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/26 [00:00<00:01, 15.25it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 6/26 [00:00<00:01, 15.26it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 8/26 [00:00<00:01, 15.19it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 10/26 [00:00<00:01, 15.19it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 12/26 [00:00<00:00, 15.19it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 14/26 [00:00<00:00, 15.13it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 16/26 [00:01<00:00, 15.12it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 18/26 [00:01<00:00, 15.13it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 20/26 [00:01<00:00, 14.99it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 22/26 [00:01<00:00, 14.89it/s]\u001b[A\n",
            "Predicting batches:  92%| | 24/26 [00:01<00:00, 14.95it/s]\u001b[A\n",
            "Predicting batches: 100%|| 26/26 [00:01<00:00, 15.48it/s]\u001b[A\n",
            "Processing subjects:  68%|   | 39/57 [04:47<01:25,  4.75s/it, marketing]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/234 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  24%|                | 57/234 [00:00<00:00, 562.01it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 114/234 [00:00<00:00, 565.01it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 171/234 [00:00<00:00, 565.67it/s]\u001b[A\n",
            "Formatting batches:  98%|| 229/234 [00:00<00:00, 569.16it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/59 [00:00<00:05, 10.64it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/59 [00:00<00:05, 10.76it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/59 [00:00<00:04, 10.79it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/59 [00:00<00:04, 10.79it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/59 [00:00<00:04, 10.79it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/59 [00:01<00:04, 10.79it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 14/59 [00:01<00:04, 10.90it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/59 [00:01<00:03, 10.86it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 18/59 [00:01<00:03, 10.80it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 20/59 [00:01<00:03, 10.88it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/59 [00:02<00:03, 10.66it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 24/59 [00:02<00:03, 10.57it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 26/59 [00:02<00:03, 10.63it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/59 [00:02<00:02, 10.52it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 30/59 [00:02<00:02, 10.70it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 32/59 [00:02<00:02, 10.83it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 34/59 [00:03<00:02, 10.66it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 36/59 [00:03<00:02, 10.67it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 38/59 [00:03<00:01, 10.95it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 40/59 [00:03<00:01, 10.73it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 42/59 [00:03<00:01, 10.71it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 44/59 [00:04<00:01, 10.61it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 46/59 [00:04<00:01, 10.54it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 48/59 [00:04<00:01, 10.73it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 50/59 [00:04<00:00, 10.70it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 52/59 [00:04<00:00, 10.70it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 54/59 [00:05<00:00, 10.57it/s]\u001b[A\n",
            "Predicting batches:  95%| | 56/59 [00:05<00:00, 10.63it/s]\u001b[A\n",
            "Predicting batches:  98%|| 58/59 [00:05<00:00, 10.75it/s]\u001b[A\n",
            "Processing subjects:  70%| | 40/57 [04:53<01:26,  5.09s/it, medical genetics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 61/100 [00:00<00:00, 604.57it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:01, 13.14it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:01, 13.11it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 12.96it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 12.75it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 12.64it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:00<00:01, 12.57it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:00, 12.44it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 12.34it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 12.64it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 12.75it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:01<00:00, 12.83it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:01<00:00, 12.87it/s]\u001b[A\n",
            "Processing subjects:  72%|  | 41/57 [04:55<01:07,  4.20s/it, miscellaneous]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/783 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   8%|                   | 63/783 [00:00<00:01, 629.46it/s]\u001b[A\n",
            "Formatting batches:  16%|                | 127/783 [00:00<00:01, 632.24it/s]\u001b[A\n",
            "Formatting batches:  24%|               | 191/783 [00:00<00:00, 633.86it/s]\u001b[A\n",
            "Formatting batches:  33%|             | 255/783 [00:00<00:00, 630.29it/s]\u001b[A\n",
            "Formatting batches:  41%|           | 319/783 [00:00<00:00, 630.70it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 383/783 [00:00<00:00, 632.43it/s]\u001b[A\n",
            "Formatting batches:  57%|        | 447/783 [00:00<00:00, 633.36it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 511/783 [00:00<00:00, 634.41it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 575/783 [00:00<00:00, 634.15it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 639/783 [00:01<00:00, 632.93it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 703/783 [00:01<00:00, 633.10it/s]\u001b[A\n",
            "Formatting batches:  98%|| 767/783 [00:01<00:00, 633.88it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/196 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/196 [00:00<00:13, 14.75it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 4/196 [00:00<00:13, 14.43it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 6/196 [00:00<00:13, 14.42it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 8/196 [00:00<00:12, 14.76it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 10/196 [00:00<00:12, 14.92it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 12/196 [00:00<00:12, 14.73it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 14/196 [00:00<00:12, 14.76it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 16/196 [00:01<00:12, 14.53it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 18/196 [00:01<00:12, 14.48it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 20/196 [00:01<00:12, 13.60it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 22/196 [00:01<00:12, 14.09it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 24/196 [00:01<00:12, 14.17it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 26/196 [00:01<00:11, 14.48it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 28/196 [00:01<00:11, 14.32it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 30/196 [00:02<00:12, 13.39it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 32/196 [00:02<00:11, 13.89it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 34/196 [00:02<00:11, 13.92it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 36/196 [00:02<00:11, 13.98it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 38/196 [00:02<00:11, 14.17it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 40/196 [00:02<00:10, 14.49it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 42/196 [00:02<00:10, 14.38it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 44/196 [00:03<00:10, 14.31it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 46/196 [00:03<00:10, 13.96it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 48/196 [00:03<00:10, 13.88it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 50/196 [00:03<00:10, 14.07it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 52/196 [00:03<00:10, 14.24it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 54/196 [00:03<00:10, 13.76it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 56/196 [00:03<00:10, 13.48it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 58/196 [00:04<00:09, 13.83it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 60/196 [00:04<00:09, 14.22it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 62/196 [00:04<00:09, 14.38it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 64/196 [00:04<00:09, 14.17it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 66/196 [00:04<00:09, 13.49it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 68/196 [00:04<00:09, 13.67it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 70/196 [00:04<00:08, 14.11it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 72/196 [00:05<00:08, 14.24it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 74/196 [00:05<00:08, 14.52it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 76/196 [00:05<00:08, 14.72it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 78/196 [00:05<00:08, 14.56it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 80/196 [00:05<00:08, 14.44it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 82/196 [00:05<00:07, 14.68it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 84/196 [00:05<00:07, 14.69it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 86/196 [00:06<00:07, 14.69it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 88/196 [00:06<00:07, 14.51it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 90/196 [00:06<00:07, 14.35it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 92/196 [00:06<00:07, 14.15it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 94/196 [00:06<00:07, 14.43it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 96/196 [00:06<00:06, 14.46it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 98/196 [00:06<00:06, 14.66it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 100/196 [00:07<00:06, 14.46it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 102/196 [00:07<00:06, 14.49it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 104/196 [00:07<00:06, 14.14it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 106/196 [00:07<00:06, 14.25it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 108/196 [00:07<00:06, 13.99it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 110/196 [00:07<00:06, 14.02it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 112/196 [00:07<00:06, 14.00it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 114/196 [00:08<00:05, 14.34it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 116/196 [00:08<00:05, 14.59it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 118/196 [00:08<00:05, 13.75it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 120/196 [00:08<00:05, 13.73it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 122/196 [00:08<00:05, 13.45it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 124/196 [00:08<00:05, 13.82it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 126/196 [00:08<00:05, 13.44it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 128/196 [00:09<00:05, 13.53it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 130/196 [00:09<00:05, 13.07it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 132/196 [00:09<00:04, 13.46it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 134/196 [00:09<00:04, 13.92it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 136/196 [00:09<00:04, 13.93it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 138/196 [00:09<00:04, 13.00it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 140/196 [00:09<00:04, 13.42it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 142/196 [00:10<00:03, 13.50it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 144/196 [00:10<00:03, 13.24it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 146/196 [00:10<00:03, 13.49it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 148/196 [00:10<00:03, 12.94it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 150/196 [00:10<00:03, 12.84it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 152/196 [00:10<00:03, 12.40it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 154/196 [00:11<00:03, 12.98it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 156/196 [00:11<00:02, 13.41it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 158/196 [00:11<00:02, 13.89it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 160/196 [00:11<00:02, 14.07it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 162/196 [00:11<00:02, 12.91it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 164/196 [00:11<00:02, 13.12it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 166/196 [00:11<00:02, 13.51it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 168/196 [00:12<00:02, 13.66it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 170/196 [00:12<00:01, 14.09it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 172/196 [00:12<00:01, 14.09it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 174/196 [00:12<00:01, 13.86it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 176/196 [00:12<00:01, 14.06it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 178/196 [00:12<00:01, 14.38it/s]\u001b[A\n",
            "Predicting batches:  92%| | 180/196 [00:12<00:01, 14.00it/s]\u001b[A\n",
            "Predicting batches:  93%| | 182/196 [00:13<00:00, 14.34it/s]\u001b[A\n",
            "Predicting batches:  94%| | 184/196 [00:13<00:00, 14.57it/s]\u001b[A\n",
            "Predicting batches:  95%| | 186/196 [00:13<00:00, 14.52it/s]\u001b[A\n",
            "Predicting batches:  96%|| 188/196 [00:13<00:00, 14.72it/s]\u001b[A\n",
            "Predicting batches:  97%|| 190/196 [00:13<00:00, 14.49it/s]\u001b[A\n",
            "Predicting batches:  98%|| 192/196 [00:13<00:00, 14.29it/s]\u001b[A\n",
            "Predicting batches:  99%|| 194/196 [00:13<00:00, 13.98it/s]\u001b[A\n",
            "Predicting batches: 100%|| 196/196 [00:13<00:00, 14.59it/s]\u001b[A\n",
            "Processing subjects:  74%| | 42/57 [05:10<01:52,  7.51s/it, moral disputes]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/346 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 56/346 [00:00<00:00, 556.27it/s]\u001b[A\n",
            "Formatting batches:  33%|             | 113/346 [00:00<00:00, 563.66it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 170/346 [00:00<00:00, 565.08it/s]\u001b[A\n",
            "Formatting batches:  66%|       | 227/346 [00:00<00:00, 566.56it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 284/346 [00:00<00:00, 566.77it/s]\u001b[A\n",
            "Formatting batches:  99%|| 341/346 [00:00<00:00, 551.62it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/87 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/87 [00:00<00:09,  9.41it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/87 [00:00<00:09,  9.44it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 3/87 [00:00<00:09,  9.23it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 4/87 [00:00<00:08,  9.28it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/87 [00:00<00:08,  9.14it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 6/87 [00:00<00:08,  9.18it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 7/87 [00:00<00:08,  9.09it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 8/87 [00:00<00:08,  9.04it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 9/87 [00:00<00:08,  8.72it/s]\u001b[A\n",
            "Predicting batches:  11%|                    | 10/87 [00:01<00:08,  8.88it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 11/87 [00:01<00:08,  8.90it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 12/87 [00:01<00:08,  8.99it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 13/87 [00:01<00:08,  8.71it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 14/87 [00:01<00:08,  8.77it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 15/87 [00:01<00:08,  8.91it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 16/87 [00:01<00:07,  9.01it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 17/87 [00:01<00:07,  9.07it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 18/87 [00:02<00:07,  8.99it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 19/87 [00:02<00:07,  9.07it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 20/87 [00:02<00:07,  9.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 21/87 [00:02<00:07,  9.15it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 22/87 [00:02<00:07,  9.05it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 23/87 [00:02<00:07,  9.00it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 24/87 [00:02<00:06,  9.07it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 25/87 [00:02<00:06,  9.11it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 26/87 [00:02<00:06,  9.05it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 27/87 [00:02<00:06,  9.00it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 28/87 [00:03<00:06,  8.98it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 29/87 [00:03<00:06,  9.06it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 30/87 [00:03<00:06,  9.12it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 31/87 [00:03<00:06,  9.03it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 32/87 [00:03<00:06,  8.99it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 33/87 [00:03<00:06,  8.96it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 34/87 [00:03<00:05,  8.92it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 35/87 [00:03<00:05,  9.01it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 36/87 [00:03<00:05,  8.95it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 37/87 [00:04<00:05,  9.02it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 38/87 [00:04<00:05,  8.71it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 39/87 [00:04<00:05,  8.85it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 40/87 [00:04<00:05,  8.97it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 41/87 [00:04<00:05,  9.03it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 42/87 [00:04<00:04,  9.09it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 43/87 [00:04<00:04,  9.03it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 44/87 [00:04<00:04,  8.98it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 45/87 [00:04<00:04,  9.06it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 46/87 [00:05<00:04,  9.13it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 47/87 [00:05<00:04,  8.77it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 48/87 [00:05<00:04,  8.91it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 49/87 [00:05<00:04,  9.00it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 50/87 [00:05<00:04,  8.97it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 51/87 [00:05<00:03,  9.04it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 52/87 [00:05<00:03,  9.08it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 53/87 [00:05<00:03,  9.01it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 54/87 [00:05<00:03,  9.06it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 55/87 [00:06<00:03,  9.09it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 56/87 [00:06<00:03,  9.03it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 57/87 [00:06<00:03,  9.09it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 58/87 [00:06<00:03,  9.00it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 59/87 [00:06<00:03,  8.94it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 60/87 [00:06<00:03,  8.94it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 61/87 [00:06<00:02,  8.92it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 62/87 [00:06<00:02,  9.01it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 63/87 [00:06<00:02,  9.08it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 64/87 [00:07<00:02,  9.03it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 65/87 [00:07<00:02,  8.69it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 66/87 [00:07<00:02,  8.85it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 67/87 [00:07<00:02,  8.86it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 68/87 [00:07<00:02,  8.96it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 69/87 [00:07<00:02,  8.91it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 70/87 [00:07<00:01,  8.64it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 71/87 [00:07<00:01,  8.71it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 72/87 [00:08<00:01,  8.77it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 73/87 [00:08<00:01,  8.91it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 74/87 [00:08<00:01,  8.91it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 75/87 [00:08<00:01,  8.99it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 76/87 [00:08<00:01,  8.95it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 77/87 [00:08<00:01,  9.03it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 78/87 [00:08<00:00,  9.09it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 79/87 [00:08<00:00,  9.13it/s]\u001b[A\n",
            "Predicting batches:  92%| | 80/87 [00:08<00:00,  9.03it/s]\u001b[A\n",
            "Predicting batches:  93%| | 81/87 [00:09<00:00,  9.08it/s]\u001b[A\n",
            "Predicting batches:  94%| | 82/87 [00:09<00:00,  9.11it/s]\u001b[A\n",
            "Predicting batches:  95%| | 83/87 [00:09<00:00,  9.13it/s]\u001b[A\n",
            "Predicting batches:  97%|| 84/87 [00:09<00:00,  9.16it/s]\u001b[A\n",
            "Predicting batches:  98%|| 85/87 [00:09<00:00,  9.17it/s]\u001b[A\n",
            "Predicting batches:  99%|| 86/87 [00:09<00:00,  9.06it/s]\u001b[A\n",
            "Processing subjects:  75%| | 43/57 [05:20<01:56,  8.33s/it, moral scenarios]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/895 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   6%|                   | 54/895 [00:00<00:01, 535.06it/s]\u001b[A\n",
            "Formatting batches:  12%|                 | 109/895 [00:00<00:01, 540.14it/s]\u001b[A\n",
            "Formatting batches:  18%|                | 164/895 [00:00<00:01, 540.09it/s]\u001b[A\n",
            "Formatting batches:  24%|               | 219/895 [00:00<00:01, 542.02it/s]\u001b[A\n",
            "Formatting batches:  31%|              | 274/895 [00:00<00:01, 542.96it/s]\u001b[A\n",
            "Formatting batches:  37%|            | 329/895 [00:00<00:01, 543.31it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 384/895 [00:00<00:00, 534.86it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 439/895 [00:00<00:00, 539.21it/s]\u001b[A\n",
            "Formatting batches:  55%|         | 494/895 [00:00<00:00, 542.03it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 549/895 [00:01<00:00, 542.59it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 604/895 [00:01<00:00, 543.70it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 659/895 [00:01<00:00, 542.29it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 714/895 [00:01<00:00, 540.94it/s]\u001b[A\n",
            "Formatting batches:  86%|  | 769/895 [00:01<00:00, 540.99it/s]\u001b[A\n",
            "Formatting batches:  92%| | 824/895 [00:01<00:00, 540.04it/s]\u001b[A\n",
            "Formatting batches:  98%|| 879/895 [00:01<00:00, 538.85it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/224 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   0%|                       | 1/224 [00:00<00:30,  7.24it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/224 [00:00<00:32,  6.85it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 3/224 [00:00<00:32,  6.89it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 4/224 [00:00<00:32,  6.76it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 5/224 [00:00<00:32,  6.82it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 6/224 [00:00<00:32,  6.71it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 7/224 [00:01<00:32,  6.67it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 8/224 [00:01<00:32,  6.65it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 9/224 [00:01<00:32,  6.63it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 10/224 [00:01<00:32,  6.61it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 11/224 [00:01<00:32,  6.61it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 12/224 [00:01<00:32,  6.60it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 13/224 [00:01<00:32,  6.59it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 14/224 [00:02<00:31,  6.68it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 15/224 [00:02<00:30,  6.75it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 16/224 [00:02<00:31,  6.69it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 17/224 [00:02<00:31,  6.66it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 18/224 [00:02<00:31,  6.64it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 19/224 [00:02<00:30,  6.71it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 20/224 [00:02<00:30,  6.66it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 21/224 [00:03<00:30,  6.73it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 22/224 [00:03<00:29,  6.77it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 23/224 [00:03<00:30,  6.70it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 24/224 [00:03<00:30,  6.66it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 25/224 [00:03<00:30,  6.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 26/224 [00:03<00:29,  6.61it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 27/224 [00:04<00:29,  6.61it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 28/224 [00:04<00:29,  6.60it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 29/224 [00:04<00:29,  6.59it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 30/224 [00:04<00:29,  6.58it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 31/224 [00:04<00:28,  6.67it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 32/224 [00:04<00:28,  6.63it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 33/224 [00:04<00:28,  6.61it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 34/224 [00:05<00:28,  6.60it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 35/224 [00:05<00:28,  6.59it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 36/224 [00:05<00:28,  6.58it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 37/224 [00:05<00:28,  6.57it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 38/224 [00:05<00:28,  6.56it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 39/224 [00:05<00:28,  6.54it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 40/224 [00:06<00:27,  6.64it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 41/224 [00:06<00:27,  6.62it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 42/224 [00:06<00:27,  6.60it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 43/224 [00:06<00:27,  6.59it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 44/224 [00:06<00:27,  6.59it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 45/224 [00:06<00:27,  6.58it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 46/224 [00:06<00:27,  6.57it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 47/224 [00:07<00:27,  6.55it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 48/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 49/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 50/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 51/224 [00:07<00:26,  6.55it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 52/224 [00:07<00:26,  6.56it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 53/224 [00:07<00:25,  6.65it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 54/224 [00:08<00:25,  6.62it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 55/224 [00:08<00:25,  6.70it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 56/224 [00:08<00:24,  6.75it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 57/224 [00:08<00:24,  6.69it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 58/224 [00:08<00:24,  6.65it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 59/224 [00:08<00:24,  6.62it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 60/224 [00:09<00:24,  6.60it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 61/224 [00:09<00:24,  6.59it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 62/224 [00:09<00:24,  6.67it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 63/224 [00:09<00:23,  6.73it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 64/224 [00:09<00:23,  6.67it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 65/224 [00:09<00:23,  6.64it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 66/224 [00:09<00:23,  6.62it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 67/224 [00:10<00:23,  6.69it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 68/224 [00:10<00:23,  6.74it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 69/224 [00:10<00:23,  6.66it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 70/224 [00:10<00:23,  6.62it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 71/224 [00:10<00:23,  6.60it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 72/224 [00:10<00:22,  6.68it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 73/224 [00:10<00:22,  6.63it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 74/224 [00:11<00:22,  6.60it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 75/224 [00:11<00:22,  6.59it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 76/224 [00:11<00:22,  6.67it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 77/224 [00:11<00:22,  6.61it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 78/224 [00:11<00:22,  6.58it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 79/224 [00:11<00:22,  6.57it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 80/224 [00:12<00:21,  6.56it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 81/224 [00:12<00:21,  6.66it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 82/224 [00:12<00:21,  6.62it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 83/224 [00:12<00:21,  6.60it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 84/224 [00:12<00:21,  6.58it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 85/224 [00:12<00:21,  6.58it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 86/224 [00:12<00:21,  6.56it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 87/224 [00:13<00:20,  6.55it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 88/224 [00:13<00:20,  6.54it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 89/224 [00:13<00:20,  6.54it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 90/224 [00:13<00:20,  6.54it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 91/224 [00:13<00:20,  6.63it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 92/224 [00:13<00:19,  6.61it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 93/224 [00:14<00:19,  6.59it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 94/224 [00:14<00:19,  6.58it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 95/224 [00:14<00:19,  6.57it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 96/224 [00:14<00:19,  6.57it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 97/224 [00:14<00:19,  6.56it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 98/224 [00:14<00:18,  6.65it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 99/224 [00:14<00:18,  6.62it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 100/224 [00:15<00:18,  6.69it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 101/224 [00:15<00:18,  6.64it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 102/224 [00:15<00:18,  6.60it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 103/224 [00:15<00:18,  6.58it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 104/224 [00:15<00:18,  6.57it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 105/224 [00:15<00:18,  6.56it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 106/224 [00:16<00:18,  6.55it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 107/224 [00:16<00:17,  6.55it/s]\u001b[A\n",
            "Predicting batches:  48%|          | 108/224 [00:16<00:17,  6.54it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 109/224 [00:16<00:17,  6.54it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 110/224 [00:16<00:17,  6.64it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 111/224 [00:16<00:17,  6.60it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 112/224 [00:16<00:17,  6.58it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 113/224 [00:17<00:16,  6.66it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 114/224 [00:17<00:16,  6.62it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 115/224 [00:17<00:16,  6.70it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 116/224 [00:17<00:16,  6.65it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 117/224 [00:17<00:16,  6.62it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 118/224 [00:17<00:15,  6.69it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 119/224 [00:17<00:15,  6.74it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 120/224 [00:18<00:15,  6.68it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 121/224 [00:18<00:15,  6.74it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 122/224 [00:18<00:15,  6.65it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 123/224 [00:18<00:15,  6.62it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 124/224 [00:18<00:15,  6.58it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 125/224 [00:18<00:15,  6.56it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 126/224 [00:19<00:14,  6.56it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 127/224 [00:19<00:14,  6.55it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 128/224 [00:19<00:14,  6.65it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 129/224 [00:19<00:14,  6.61it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 130/224 [00:19<00:14,  6.59it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 131/224 [00:19<00:14,  6.57it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 132/224 [00:19<00:13,  6.66it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 133/224 [00:20<00:13,  6.72it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 134/224 [00:20<00:13,  6.65it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 135/224 [00:20<00:13,  6.61it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 136/224 [00:20<00:13,  6.68it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 137/224 [00:20<00:12,  6.74it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 138/224 [00:20<00:12,  6.66it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 139/224 [00:20<00:12,  6.60it/s]\u001b[A\n",
            "Predicting batches:  62%|       | 140/224 [00:21<00:12,  6.58it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 141/224 [00:21<00:12,  6.57it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 142/224 [00:21<00:12,  6.56it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 143/224 [00:21<00:12,  6.54it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 144/224 [00:21<00:12,  6.54it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 145/224 [00:21<00:12,  6.54it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 146/224 [00:22<00:11,  6.54it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 147/224 [00:22<00:11,  6.54it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 148/224 [00:22<00:11,  6.54it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 149/224 [00:22<00:11,  6.63it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 150/224 [00:22<00:11,  6.70it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 151/224 [00:22<00:10,  6.75it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 152/224 [00:22<00:10,  6.67it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 153/224 [00:23<00:10,  6.73it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 154/224 [00:23<00:10,  6.67it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 155/224 [00:23<00:10,  6.63it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 156/224 [00:23<00:10,  6.60it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 157/224 [00:23<00:10,  6.59it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 158/224 [00:23<00:10,  6.57it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 159/224 [00:24<00:09,  6.66it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 160/224 [00:24<00:09,  6.72it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 161/224 [00:24<00:09,  6.64it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 162/224 [00:24<00:09,  6.61it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 163/224 [00:24<00:09,  6.59it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 164/224 [00:24<00:08,  6.67it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 165/224 [00:24<00:08,  6.72it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 166/224 [00:25<00:08,  6.75it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 167/224 [00:25<00:08,  6.78it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 168/224 [00:25<00:08,  6.70it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 169/224 [00:25<00:08,  6.75it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 170/224 [00:25<00:08,  6.67it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 171/224 [00:25<00:07,  6.63it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 172/224 [00:25<00:07,  6.60it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 173/224 [00:26<00:07,  6.59it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 174/224 [00:26<00:07,  6.67it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 175/224 [00:26<00:07,  6.62it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 176/224 [00:26<00:07,  6.59it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 177/224 [00:26<00:07,  6.56it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 178/224 [00:26<00:07,  6.55it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 179/224 [00:27<00:06,  6.65it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 180/224 [00:27<00:06,  6.61it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 181/224 [00:27<00:06,  6.59it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 182/224 [00:27<00:06,  6.58it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 183/224 [00:27<00:06,  6.57it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 184/224 [00:27<00:06,  6.56it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 185/224 [00:27<00:05,  6.55it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 186/224 [00:28<00:05,  6.55it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 187/224 [00:28<00:05,  6.64it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 188/224 [00:28<00:05,  6.61it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 189/224 [00:28<00:05,  6.59it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 190/224 [00:28<00:05,  6.67it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 191/224 [00:28<00:05,  6.60it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 192/224 [00:28<00:04,  6.57it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 193/224 [00:29<00:04,  6.66it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 194/224 [00:29<00:04,  6.61it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 195/224 [00:29<00:04,  6.59it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 196/224 [00:29<00:04,  6.57it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 197/224 [00:29<00:04,  6.65it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 198/224 [00:29<00:03,  6.61it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 199/224 [00:30<00:03,  6.58it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 200/224 [00:30<00:03,  6.57it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 201/224 [00:30<00:03,  6.65it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 202/224 [00:30<00:03,  6.61it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 203/224 [00:30<00:03,  6.59it/s]\u001b[A\n",
            "Predicting batches:  91%| | 204/224 [00:30<00:03,  6.57it/s]\u001b[A\n",
            "Predicting batches:  92%| | 205/224 [00:30<00:02,  6.65it/s]\u001b[A\n",
            "Predicting batches:  92%| | 206/224 [00:31<00:02,  6.60it/s]\u001b[A\n",
            "Predicting batches:  92%| | 207/224 [00:31<00:02,  6.57it/s]\u001b[A\n",
            "Predicting batches:  93%| | 208/224 [00:31<00:02,  6.57it/s]\u001b[A\n",
            "Predicting batches:  93%| | 209/224 [00:31<00:02,  6.56it/s]\u001b[A\n",
            "Predicting batches:  94%| | 210/224 [00:31<00:02,  6.64it/s]\u001b[A\n",
            "Predicting batches:  94%| | 211/224 [00:31<00:01,  6.60it/s]\u001b[A\n",
            "Predicting batches:  95%| | 212/224 [00:32<00:01,  6.67it/s]\u001b[A\n",
            "Predicting batches:  95%| | 213/224 [00:32<00:01,  6.72it/s]\u001b[A\n",
            "Predicting batches:  96%| | 214/224 [00:32<00:01,  6.66it/s]\u001b[A\n",
            "Predicting batches:  96%|| 215/224 [00:32<00:01,  6.62it/s]\u001b[A\n",
            "Predicting batches:  96%|| 216/224 [00:32<00:01,  6.60it/s]\u001b[A\n",
            "Predicting batches:  97%|| 217/224 [00:32<00:01,  6.57it/s]\u001b[A\n",
            "Predicting batches:  97%|| 218/224 [00:32<00:00,  6.66it/s]\u001b[A\n",
            "Predicting batches:  98%|| 219/224 [00:33<00:00,  6.59it/s]\u001b[A\n",
            "Predicting batches:  98%|| 220/224 [00:33<00:00,  6.57it/s]\u001b[A\n",
            "Predicting batches:  99%|| 221/224 [00:33<00:00,  6.56it/s]\u001b[A\n",
            "Predicting batches:  99%|| 222/224 [00:33<00:00,  6.55it/s]\u001b[A\n",
            "Predicting batches: 100%|| 223/224 [00:33<00:00,  6.55it/s]\u001b[A\n",
            "Predicting batches: 100%|| 224/224 [00:33<00:00,  7.07it/s]\u001b[A\n",
            "Processing subjects:  77%|  | 44/57 [05:56<03:34, 16.47s/it, nutrition]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/306 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 53/306 [00:00<00:00, 522.47it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 107/306 [00:00<00:00, 531.78it/s]\u001b[A\n",
            "Formatting batches:  53%|         | 161/306 [00:00<00:00, 534.31it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 215/306 [00:00<00:00, 535.22it/s]\u001b[A\n",
            "Formatting batches:  88%|  | 270/306 [00:00<00:00, 537.38it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/77 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/77 [00:00<00:09,  7.77it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/77 [00:00<00:09,  7.82it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/77 [00:00<00:09,  7.54it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/77 [00:00<00:10,  7.12it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/77 [00:00<00:10,  7.06it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/77 [00:00<00:10,  7.03it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 7/77 [00:00<00:09,  7.23it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/77 [00:01<00:09,  7.21it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 9/77 [00:01<00:09,  7.20it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/77 [00:01<00:09,  7.14it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 11/77 [00:01<00:09,  7.09it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 12/77 [00:01<00:09,  7.05it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 13/77 [00:01<00:09,  7.09it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/77 [00:01<00:08,  7.24it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/77 [00:02<00:08,  7.27it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/77 [00:02<00:08,  7.29it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 17/77 [00:02<00:08,  7.41it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/77 [00:02<00:07,  7.49it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 19/77 [00:02<00:07,  7.56it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/77 [00:02<00:07,  7.44it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 21/77 [00:02<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 22/77 [00:03<00:07,  7.49it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 23/77 [00:03<00:07,  7.29it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 24/77 [00:03<00:07,  7.19it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 25/77 [00:03<00:07,  7.24it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 26/77 [00:03<00:07,  7.13it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 27/77 [00:03<00:06,  7.19it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/77 [00:03<00:06,  7.23it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 29/77 [00:03<00:06,  7.38it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 30/77 [00:04<00:06,  7.37it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 31/77 [00:04<00:06,  7.24it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 32/77 [00:04<00:06,  7.22it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 33/77 [00:04<00:06,  7.20it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 34/77 [00:04<00:06,  7.12it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 35/77 [00:04<00:05,  7.27it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 36/77 [00:04<00:05,  7.02it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 37/77 [00:05<00:05,  7.12it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/77 [00:05<00:05,  7.06it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 39/77 [00:05<00:05,  7.01it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 40/77 [00:05<00:05,  7.04it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 41/77 [00:05<00:04,  7.21it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 42/77 [00:05<00:04,  7.35it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 43/77 [00:05<00:04,  7.42it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 44/77 [00:06<00:04,  7.48it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 45/77 [00:06<00:04,  7.37it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 46/77 [00:06<00:04,  7.22it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 47/77 [00:06<00:04,  7.20it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/77 [00:06<00:04,  7.12it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 49/77 [00:06<00:03,  7.04it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 50/77 [00:06<00:03,  6.88it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 51/77 [00:07<00:03,  6.90it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 52/77 [00:07<00:03,  6.89it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 53/77 [00:07<00:03,  6.96it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 54/77 [00:07<00:03,  6.96it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 55/77 [00:07<00:03,  7.02it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 56/77 [00:07<00:02,  7.12it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 57/77 [00:07<00:02,  7.13it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 58/77 [00:08<00:02,  7.28it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 59/77 [00:08<00:02,  7.28it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 60/77 [00:08<00:02,  7.30it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 61/77 [00:08<00:02,  7.26it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 62/77 [00:08<00:02,  7.14it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 63/77 [00:08<00:01,  7.18it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 64/77 [00:08<00:01,  7.17it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 65/77 [00:09<00:01,  7.23it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 66/77 [00:09<00:01,  7.38it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 67/77 [00:09<00:01,  7.31it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 68/77 [00:09<00:01,  7.19it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 69/77 [00:09<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 70/77 [00:09<00:00,  7.08it/s]\u001b[A\n",
            "Predicting batches:  92%| | 71/77 [00:09<00:00,  7.11it/s]\u001b[A\n",
            "Predicting batches:  94%| | 72/77 [00:10<00:00,  7.07it/s]\u001b[A\n",
            "Predicting batches:  95%| | 73/77 [00:10<00:00,  7.02it/s]\u001b[A\n",
            "Predicting batches:  96%| | 74/77 [00:10<00:00,  7.11it/s]\u001b[A\n",
            "Predicting batches:  97%|| 75/77 [00:10<00:00,  7.17it/s]\u001b[A\n",
            "Predicting batches:  99%|| 76/77 [00:10<00:00,  7.22it/s]\u001b[A\n",
            "Processing subjects:  79%|  | 45/57 [06:07<02:58, 14.89s/it, philosophy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/311 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  20%|                 | 61/311 [00:00<00:00, 603.52it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 123/311 [00:00<00:00, 609.62it/s]\u001b[A\n",
            "Formatting batches:  59%|        | 185/311 [00:00<00:00, 610.26it/s]\u001b[A\n",
            "Formatting batches:  79%|    | 247/311 [00:00<00:00, 611.61it/s]\u001b[A\n",
            "Formatting batches:  99%|| 309/311 [00:00<00:00, 611.94it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/78 [00:00<00:05, 14.19it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/78 [00:00<00:05, 13.94it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/78 [00:00<00:05, 12.92it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/78 [00:00<00:05, 11.99it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/78 [00:00<00:05, 12.60it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/78 [00:00<00:05, 12.72it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/78 [00:01<00:05, 12.33it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/78 [00:01<00:04, 12.45it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/78 [00:01<00:04, 12.90it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/78 [00:01<00:04, 12.96it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 22/78 [00:01<00:04, 13.15it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 24/78 [00:01<00:04, 12.66it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 26/78 [00:02<00:04, 12.81it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/78 [00:02<00:03, 12.84it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 30/78 [00:02<00:03, 12.99it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 32/78 [00:02<00:03, 12.70it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 34/78 [00:02<00:03, 11.84it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 36/78 [00:02<00:03, 12.43it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/78 [00:03<00:03, 12.00it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 40/78 [00:03<00:03, 12.49it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 42/78 [00:03<00:02, 12.53it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 44/78 [00:03<00:02, 12.79it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 46/78 [00:03<00:02, 12.76it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/78 [00:03<00:02, 13.07it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 50/78 [00:03<00:02, 13.15it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 52/78 [00:04<00:01, 13.26it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 54/78 [00:04<00:01, 13.34it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 56/78 [00:04<00:01, 13.22it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 58/78 [00:04<00:01, 13.31it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 60/78 [00:04<00:01, 13.24it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 62/78 [00:04<00:01, 12.83it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 64/78 [00:05<00:01, 12.57it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 66/78 [00:05<00:00, 12.13it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 68/78 [00:05<00:00, 12.47it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 70/78 [00:05<00:00, 12.91it/s]\u001b[A\n",
            "Predicting batches:  92%| | 72/78 [00:05<00:00, 13.07it/s]\u001b[A\n",
            "Predicting batches:  95%| | 74/78 [00:05<00:00, 12.60it/s]\u001b[A\n",
            "Predicting batches:  97%|| 76/78 [00:05<00:00, 12.82it/s]\u001b[A\n",
            "Predicting batches: 100%|| 78/78 [00:06<00:00, 13.10it/s]\u001b[A\n",
            "Processing subjects:  81%|  | 46/57 [06:13<02:16, 12.41s/it, prehistory]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/324 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 54/324 [00:00<00:00, 535.97it/s]\u001b[A\n",
            "Formatting batches:  34%|             | 109/324 [00:00<00:00, 541.00it/s]\u001b[A\n",
            "Formatting batches:  51%|         | 165/324 [00:00<00:00, 548.06it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 220/324 [00:00<00:00, 547.85it/s]\u001b[A\n",
            "Formatting batches:  85%|   | 277/324 [00:00<00:00, 552.40it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/81 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/81 [00:00<00:09,  8.55it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/81 [00:00<00:08,  8.78it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/81 [00:00<00:09,  8.29it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/81 [00:00<00:09,  8.20it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/81 [00:00<00:09,  8.42it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 6/81 [00:00<00:08,  8.58it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 7/81 [00:00<00:08,  8.43it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/81 [00:00<00:08,  8.55it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 9/81 [00:01<00:08,  8.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 10/81 [00:01<00:08,  8.69it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 11/81 [00:01<00:08,  8.47it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/81 [00:01<00:08,  8.57it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 13/81 [00:01<00:07,  8.64it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 14/81 [00:01<00:07,  8.47it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/81 [00:01<00:08,  8.24it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 16/81 [00:01<00:08,  8.05it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 17/81 [00:02<00:08,  7.84it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 18/81 [00:02<00:07,  8.13it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 19/81 [00:02<00:07,  8.10it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 20/81 [00:02<00:07,  8.30it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 21/81 [00:02<00:07,  8.44it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 22/81 [00:02<00:07,  8.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 23/81 [00:02<00:07,  8.09it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 24/81 [00:02<00:06,  8.29it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 25/81 [00:02<00:06,  8.20it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 26/81 [00:03<00:06,  8.13it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 27/81 [00:03<00:06,  8.31it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 28/81 [00:03<00:06,  8.10it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 29/81 [00:03<00:06,  8.07it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 30/81 [00:03<00:06,  7.95it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 31/81 [00:03<00:06,  7.99it/s]\u001b[A\n",
            "Predicting batches:  40%|              | 32/81 [00:03<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 33/81 [00:03<00:05,  8.24it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 34/81 [00:04<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 35/81 [00:04<00:05,  8.38it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 36/81 [00:04<00:05,  8.30it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 37/81 [00:04<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 38/81 [00:04<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 39/81 [00:04<00:05,  8.17it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 40/81 [00:04<00:04,  8.35it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 41/81 [00:04<00:04,  8.28it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 42/81 [00:05<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 43/81 [00:05<00:04,  8.36it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 44/81 [00:05<00:04,  8.13it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 45/81 [00:05<00:04,  8.31it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 46/81 [00:05<00:04,  8.25it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 47/81 [00:05<00:04,  7.97it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 48/81 [00:05<00:04,  7.99it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 49/81 [00:05<00:04,  7.78it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 50/81 [00:06<00:03,  7.84it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 51/81 [00:06<00:03,  7.91it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 52/81 [00:06<00:03,  8.18it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 53/81 [00:06<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 54/81 [00:06<00:03,  8.03it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 55/81 [00:06<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 56/81 [00:06<00:03,  7.97it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 57/81 [00:06<00:02,  8.20it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 58/81 [00:07<00:02,  8.35it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 59/81 [00:07<00:02,  8.15it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 60/81 [00:07<00:02,  8.13it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 61/81 [00:07<00:02,  8.12it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 62/81 [00:07<00:02,  8.31it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 63/81 [00:07<00:02,  8.45it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 64/81 [00:07<00:01,  8.55it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 65/81 [00:07<00:01,  8.41it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 66/81 [00:08<00:01,  8.53it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 67/81 [00:08<00:01,  8.60it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 68/81 [00:08<00:01,  8.44it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 69/81 [00:08<00:01,  8.29it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 70/81 [00:08<00:01,  8.46it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 71/81 [00:08<00:01,  8.32it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 72/81 [00:08<00:01,  8.25it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 73/81 [00:08<00:00,  8.21it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 74/81 [00:08<00:00,  8.40it/s]\u001b[A\n",
            "Predicting batches:  93%| | 75/81 [00:09<00:00,  8.54it/s]\u001b[A\n",
            "Predicting batches:  94%| | 76/81 [00:09<00:00,  8.63it/s]\u001b[A\n",
            "Predicting batches:  95%| | 77/81 [00:09<00:00,  8.68it/s]\u001b[A\n",
            "Predicting batches:  96%|| 78/81 [00:09<00:00,  8.69it/s]\u001b[A\n",
            "Predicting batches:  98%|| 79/81 [00:09<00:00,  8.81it/s]\u001b[A\n",
            "Predicting batches:  99%|| 80/81 [00:09<00:00,  8.56it/s]\u001b[A\n",
            "Predicting batches: 100%|| 81/81 [00:09<00:00,  8.37it/s]\u001b[A\n",
            "Processing subjects:  82%|| 47/57 [06:24<01:57, 11.80s/it, professional account\u001b[A\n",
            "Formatting batches:   0%|                               | 0/282 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  18%|                 | 52/282 [00:00<00:00, 515.40it/s]\u001b[A\n",
            "Formatting batches:  37%|            | 104/282 [00:00<00:00, 511.40it/s]\u001b[A\n",
            "Formatting batches:  56%|        | 157/282 [00:00<00:00, 516.57it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 210/282 [00:00<00:00, 519.92it/s]\u001b[A\n",
            "Formatting batches:  93%| | 262/282 [00:00<00:00, 515.91it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/71 [00:00<00:10,  6.82it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/71 [00:00<00:10,  6.71it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/71 [00:00<00:10,  6.62it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/71 [00:00<00:10,  6.45it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/71 [00:00<00:10,  6.26it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/71 [00:00<00:10,  6.26it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/71 [00:01<00:10,  6.10it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 8/71 [00:01<00:10,  6.21it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 9/71 [00:01<00:09,  6.21it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 10/71 [00:01<00:09,  6.30it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 11/71 [00:01<00:09,  6.37it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 12/71 [00:01<00:09,  6.41it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 13/71 [00:02<00:09,  6.36it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 14/71 [00:02<00:08,  6.41it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 15/71 [00:02<00:08,  6.34it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 16/71 [00:02<00:08,  6.37it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 17/71 [00:02<00:08,  6.33it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 18/71 [00:02<00:08,  6.21it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 19/71 [00:03<00:08,  6.20it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 20/71 [00:03<00:08,  6.27it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 21/71 [00:03<00:07,  6.33it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 22/71 [00:03<00:07,  6.30it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 23/71 [00:03<00:07,  6.27it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 24/71 [00:03<00:07,  6.44it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 25/71 [00:03<00:06,  6.57it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 26/71 [00:04<00:07,  6.36it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 27/71 [00:04<00:06,  6.40it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 28/71 [00:04<00:06,  6.55it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 29/71 [00:04<00:06,  6.54it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 30/71 [00:04<00:06,  6.53it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 31/71 [00:04<00:06,  6.63it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 32/71 [00:04<00:05,  6.71it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 33/71 [00:05<00:05,  6.45it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 34/71 [00:05<00:05,  6.47it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 35/71 [00:05<00:05,  6.47it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 36/71 [00:05<00:05,  6.49it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 37/71 [00:05<00:05,  6.39it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 38/71 [00:05<00:05,  6.51it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 39/71 [00:06<00:04,  6.40it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 40/71 [00:06<00:04,  6.33it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 41/71 [00:06<00:04,  6.37it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 42/71 [00:06<00:04,  6.40it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 43/71 [00:06<00:04,  6.52it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 44/71 [00:06<00:04,  6.51it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 45/71 [00:07<00:04,  6.31it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 46/71 [00:07<00:03,  6.37it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 47/71 [00:07<00:03,  6.33it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 48/71 [00:07<00:03,  6.37it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 49/71 [00:07<00:03,  6.41it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 50/71 [00:07<00:03,  6.35it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 51/71 [00:07<00:03,  6.40it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 52/71 [00:08<00:02,  6.34it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 53/71 [00:08<00:02,  6.40it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 54/71 [00:08<00:02,  6.25it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 55/71 [00:08<00:02,  6.42it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 56/71 [00:08<00:02,  6.35it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 57/71 [00:08<00:02,  6.37it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 58/71 [00:09<00:02,  6.39it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 59/71 [00:09<00:01,  6.42it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 60/71 [00:09<00:01,  6.45it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 61/71 [00:09<00:01,  6.36it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 62/71 [00:09<00:01,  6.31it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 63/71 [00:09<00:01,  6.38it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 64/71 [00:10<00:01,  6.24it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 65/71 [00:10<00:00,  6.31it/s]\u001b[A\n",
            "Predicting batches:  93%| | 66/71 [00:10<00:00,  6.28it/s]\u001b[A\n",
            "Predicting batches:  94%| | 67/71 [00:10<00:00,  6.44it/s]\u001b[A\n",
            "Predicting batches:  96%| | 68/71 [00:10<00:00,  6.23it/s]\u001b[A\n",
            "Predicting batches:  97%|| 69/71 [00:10<00:00,  6.31it/s]\u001b[A\n",
            "Predicting batches:  99%|| 70/71 [00:10<00:00,  6.28it/s]\u001b[A\n",
            "Processing subjects:  84%|| 48/57 [06:35<01:45, 11.74s/it, professional law]\u001b[A\n",
            "Formatting batches:   0%|                              | 0/1534 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   2%|                   | 34/1534 [00:00<00:04, 338.17it/s]\u001b[A\n",
            "Formatting batches:   5%|                   | 71/1534 [00:00<00:04, 353.57it/s]\u001b[A\n",
            "Formatting batches:   7%|                 | 109/1534 [00:00<00:03, 362.26it/s]\u001b[A\n",
            "Formatting batches:  10%|                 | 147/1534 [00:00<00:03, 368.36it/s]\u001b[A\n",
            "Formatting batches:  12%|                | 185/1534 [00:00<00:03, 371.10it/s]\u001b[A\n",
            "Formatting batches:  15%|                | 223/1534 [00:00<00:03, 372.91it/s]\u001b[A\n",
            "Formatting batches:  17%|               | 261/1534 [00:00<00:03, 369.13it/s]\u001b[A\n",
            "Formatting batches:  19%|               | 298/1534 [00:00<00:03, 369.27it/s]\u001b[A\n",
            "Formatting batches:  22%|              | 335/1534 [00:00<00:03, 368.04it/s]\u001b[A\n",
            "Formatting batches:  24%|              | 372/1534 [00:01<00:03, 356.66it/s]\u001b[A\n",
            "Formatting batches:  27%|              | 410/1534 [00:01<00:03, 361.56it/s]\u001b[A\n",
            "Formatting batches:  29%|             | 447/1534 [00:01<00:03, 362.19it/s]\u001b[A\n",
            "Formatting batches:  32%|             | 484/1534 [00:01<00:02, 364.35it/s]\u001b[A\n",
            "Formatting batches:  34%|            | 522/1534 [00:01<00:02, 366.08it/s]\u001b[A\n",
            "Formatting batches:  36%|            | 559/1534 [00:01<00:02, 367.10it/s]\u001b[A\n",
            "Formatting batches:  39%|           | 596/1534 [00:01<00:02, 367.51it/s]\u001b[A\n",
            "Formatting batches:  41%|           | 633/1534 [00:01<00:02, 366.07it/s]\u001b[A\n",
            "Formatting batches:  44%|          | 671/1534 [00:01<00:02, 367.46it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 709/1534 [00:01<00:02, 369.32it/s]\u001b[A\n",
            "Formatting batches:  49%|         | 746/1534 [00:02<00:02, 367.24it/s]\u001b[A\n",
            "Formatting batches:  51%|         | 783/1534 [00:02<00:02, 367.07it/s]\u001b[A\n",
            "Formatting batches:  54%|        | 821/1534 [00:02<00:01, 368.03it/s]\u001b[A\n",
            "Formatting batches:  56%|        | 858/1534 [00:02<00:01, 367.91it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 896/1534 [00:02<00:01, 369.55it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 934/1534 [00:02<00:01, 370.30it/s]\u001b[A\n",
            "Formatting batches:  63%|       | 972/1534 [00:02<00:01, 369.43it/s]\u001b[A\n",
            "Formatting batches:  66%|      | 1009/1534 [00:02<00:01, 369.36it/s]\u001b[A\n",
            "Formatting batches:  68%|     | 1046/1534 [00:02<00:01, 367.45it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 1083/1534 [00:02<00:01, 367.50it/s]\u001b[A\n",
            "Formatting batches:  73%|    | 1121/1534 [00:03<00:01, 368.91it/s]\u001b[A\n",
            "Formatting batches:  75%|    | 1158/1534 [00:03<00:01, 368.41it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 1196/1534 [00:03<00:00, 369.85it/s]\u001b[A\n",
            "Formatting batches:  80%|   | 1234/1534 [00:03<00:00, 370.00it/s]\u001b[A\n",
            "Formatting batches:  83%|   | 1272/1534 [00:03<00:00, 370.54it/s]\u001b[A\n",
            "Formatting batches:  85%|  | 1310/1534 [00:03<00:00, 370.56it/s]\u001b[A\n",
            "Formatting batches:  88%|  | 1348/1534 [00:03<00:00, 369.15it/s]\u001b[A\n",
            "Formatting batches:  90%| | 1385/1534 [00:03<00:00, 358.54it/s]\u001b[A\n",
            "Formatting batches:  93%| | 1422/1534 [00:03<00:00, 360.88it/s]\u001b[A\n",
            "Formatting batches:  95%|| 1460/1534 [00:03<00:00, 364.04it/s]\u001b[A\n",
            "Formatting batches:  98%|| 1498/1534 [00:04<00:00, 366.17it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/384 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   0%|                       | 1/384 [00:00<02:44,  2.32it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 2/384 [00:00<02:46,  2.30it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 3/384 [00:01<02:55,  2.18it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 4/384 [00:01<02:51,  2.21it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 5/384 [00:02<02:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 6/384 [00:02<02:50,  2.22it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 7/384 [00:03<02:50,  2.21it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 8/384 [00:03<02:48,  2.23it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 9/384 [00:04<02:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 10/384 [00:04<02:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 11/384 [00:04<02:44,  2.27it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 12/384 [00:05<02:45,  2.25it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 13/384 [00:05<02:49,  2.18it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 14/384 [00:06<02:47,  2.20it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 15/384 [00:06<02:47,  2.21it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 16/384 [00:07<02:47,  2.20it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 17/384 [00:07<02:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 18/384 [00:08<02:55,  2.08it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 19/384 [00:08<02:48,  2.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 20/384 [00:09<02:47,  2.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 21/384 [00:09<02:49,  2.14it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 22/384 [00:09<02:43,  2.22it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 23/384 [00:10<02:49,  2.14it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 24/384 [00:10<02:45,  2.17it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 25/384 [00:11<02:53,  2.07it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 26/384 [00:11<02:47,  2.14it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 27/384 [00:12<02:42,  2.19it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 28/384 [00:12<02:40,  2.21it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 29/384 [00:13<02:39,  2.23it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 30/384 [00:13<02:35,  2.28it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 31/384 [00:14<02:36,  2.25it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 32/384 [00:14<02:38,  2.21it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 33/384 [00:15<02:37,  2.23it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 34/384 [00:15<02:33,  2.28it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 35/384 [00:15<02:36,  2.24it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 36/384 [00:16<02:32,  2.29it/s]\u001b[A\n",
            "Predicting batches:  10%|                    | 37/384 [00:16<02:32,  2.28it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 38/384 [00:17<02:37,  2.20it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 39/384 [00:17<02:35,  2.21it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 40/384 [00:18<02:34,  2.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 41/384 [00:18<02:32,  2.25it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 42/384 [00:19<02:34,  2.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 43/384 [00:19<02:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 44/384 [00:19<02:36,  2.17it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 45/384 [00:20<02:32,  2.22it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 46/384 [00:20<02:29,  2.26it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 47/384 [00:21<02:33,  2.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 48/384 [00:21<02:30,  2.24it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 49/384 [00:22<02:35,  2.15it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 50/384 [00:22<02:31,  2.20it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 51/384 [00:23<02:32,  2.18it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 52/384 [00:23<02:31,  2.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 53/384 [00:24<02:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 54/384 [00:24<02:28,  2.22it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 55/384 [00:24<02:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 56/384 [00:25<02:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 57/384 [00:25<02:25,  2.25it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 58/384 [00:26<02:23,  2.27it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 59/384 [00:26<02:22,  2.29it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 60/384 [00:27<02:26,  2.21it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 61/384 [00:27<02:24,  2.24it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 62/384 [00:28<02:35,  2.07it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 63/384 [00:28<02:31,  2.12it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 64/384 [00:29<02:29,  2.14it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 65/384 [00:29<02:30,  2.12it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 66/384 [00:29<02:25,  2.18it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 67/384 [00:30<02:22,  2.22it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 68/384 [00:30<02:21,  2.23it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 69/384 [00:31<02:26,  2.15it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 70/384 [00:31<02:23,  2.18it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 71/384 [00:32<02:24,  2.17it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 72/384 [00:32<02:29,  2.08it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 73/384 [00:33<02:27,  2.11it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 74/384 [00:33<02:21,  2.19it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 75/384 [00:34<02:20,  2.20it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 76/384 [00:34<02:16,  2.26it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 77/384 [00:34<02:14,  2.28it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 78/384 [00:35<02:13,  2.28it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 79/384 [00:35<02:17,  2.21it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 80/384 [00:36<02:22,  2.13it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 81/384 [00:36<02:21,  2.15it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 82/384 [00:37<02:23,  2.10it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 83/384 [00:37<02:24,  2.09it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 84/384 [00:38<02:26,  2.05it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 85/384 [00:38<02:21,  2.11it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 86/384 [00:39<02:17,  2.17it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 87/384 [00:39<02:14,  2.21it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 88/384 [00:40<02:15,  2.19it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 89/384 [00:40<02:13,  2.21it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 90/384 [00:41<02:18,  2.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 91/384 [00:41<02:15,  2.16it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 92/384 [00:41<02:13,  2.19it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 93/384 [00:42<02:10,  2.22it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 94/384 [00:42<02:11,  2.21it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 95/384 [00:43<02:09,  2.24it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 96/384 [00:43<02:09,  2.23it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 97/384 [00:44<02:09,  2.21it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 98/384 [00:44<02:07,  2.24it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 99/384 [00:45<02:08,  2.22it/s]\u001b[A\n",
            "Predicting batches:  26%|               | 100/384 [00:45<02:08,  2.21it/s]\u001b[A\n",
            "Predicting batches:  26%|               | 101/384 [00:45<02:07,  2.23it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 102/384 [00:46<02:04,  2.26it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 103/384 [00:46<02:04,  2.26it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 104/384 [00:47<02:05,  2.24it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 105/384 [00:47<02:02,  2.27it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 106/384 [00:48<02:02,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 107/384 [00:48<02:02,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 108/384 [00:49<02:02,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 109/384 [00:49<02:02,  2.24it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 110/384 [00:49<02:03,  2.21it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 111/384 [00:50<02:02,  2.22it/s]\u001b[A\n",
            "Predicting batches:  29%|              | 112/384 [00:50<02:06,  2.14it/s]\u001b[A\n",
            "Predicting batches:  29%|              | 113/384 [00:51<02:09,  2.09it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 114/384 [00:51<02:06,  2.14it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 115/384 [00:52<02:05,  2.14it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 116/384 [00:52<02:02,  2.19it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 117/384 [00:53<02:02,  2.17it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 118/384 [00:53<02:01,  2.20it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 119/384 [00:54<01:58,  2.23it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 120/384 [00:54<01:59,  2.21it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 121/384 [00:55<02:00,  2.19it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 122/384 [00:55<01:57,  2.22it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 123/384 [00:55<01:55,  2.25it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 124/384 [00:56<01:57,  2.22it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 125/384 [00:56<01:55,  2.25it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 126/384 [00:57<01:53,  2.27it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 127/384 [00:57<01:56,  2.20it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 128/384 [00:58<01:57,  2.19it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 129/384 [00:58<01:54,  2.22it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 130/384 [00:59<01:53,  2.25it/s]\u001b[A\n",
            "Predicting batches:  34%|             | 131/384 [00:59<01:51,  2.27it/s]\u001b[A\n",
            "Predicting batches:  34%|             | 132/384 [00:59<01:51,  2.26it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 133/384 [01:00<01:50,  2.28it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 134/384 [01:00<01:48,  2.30it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 135/384 [01:01<01:48,  2.30it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 136/384 [01:01<01:48,  2.28it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 137/384 [01:02<01:49,  2.26it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 138/384 [01:02<01:50,  2.23it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 139/384 [01:03<01:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 140/384 [01:03<01:51,  2.19it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 141/384 [01:03<01:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 142/384 [01:04<01:49,  2.22it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 143/384 [01:04<01:52,  2.13it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 144/384 [01:05<01:52,  2.14it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 145/384 [01:05<01:50,  2.16it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 146/384 [01:06<01:50,  2.16it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 147/384 [01:06<01:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 148/384 [01:07<01:48,  2.17it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 149/384 [01:07<01:46,  2.22it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 150/384 [01:08<01:44,  2.25it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 151/384 [01:08<01:46,  2.19it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 152/384 [01:08<01:45,  2.19it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 153/384 [01:09<01:45,  2.18it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 154/384 [01:09<01:47,  2.14it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 155/384 [01:10<01:48,  2.10it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 156/384 [01:10<01:46,  2.13it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 157/384 [01:11<01:47,  2.11it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 158/384 [01:11<01:44,  2.16it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 159/384 [01:12<01:46,  2.10it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 160/384 [01:12<01:43,  2.16it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 161/384 [01:13<01:40,  2.21it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 162/384 [01:13<01:41,  2.19it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 163/384 [01:14<01:41,  2.18it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 164/384 [01:14<01:43,  2.13it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 165/384 [01:15<01:41,  2.17it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 166/384 [01:15<01:40,  2.16it/s]\u001b[A\n",
            "Predicting batches:  43%|           | 167/384 [01:15<01:42,  2.12it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 168/384 [01:16<01:40,  2.15it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 169/384 [01:16<01:37,  2.20it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 170/384 [01:17<01:39,  2.16it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 171/384 [01:17<01:36,  2.21it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 172/384 [01:18<01:36,  2.20it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 173/384 [01:18<01:36,  2.18it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 174/384 [01:19<01:35,  2.20it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 175/384 [01:19<01:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 176/384 [01:20<01:32,  2.25it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 177/384 [01:20<01:34,  2.19it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 178/384 [01:20<01:32,  2.22it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 179/384 [01:21<01:32,  2.21it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 180/384 [01:21<01:31,  2.24it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 181/384 [01:22<01:31,  2.23it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 182/384 [01:22<01:30,  2.23it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 183/384 [01:23<01:30,  2.22it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 184/384 [01:23<01:31,  2.19it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 185/384 [01:24<01:30,  2.20it/s]\u001b[A\n",
            "Predicting batches:  48%|          | 186/384 [01:24<01:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 187/384 [01:25<01:28,  2.24it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 188/384 [01:25<01:29,  2.18it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 189/384 [01:25<01:28,  2.20it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 190/384 [01:26<01:30,  2.14it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 191/384 [01:26<01:28,  2.19it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 192/384 [01:27<01:27,  2.20it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 193/384 [01:27<01:28,  2.15it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 194/384 [01:28<01:27,  2.16it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 195/384 [01:28<01:25,  2.20it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 196/384 [01:29<01:23,  2.26it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 197/384 [01:29<01:25,  2.19it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 198/384 [01:30<01:25,  2.18it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 199/384 [01:30<01:23,  2.21it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 200/384 [01:31<01:26,  2.14it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 201/384 [01:31<01:25,  2.14it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 202/384 [01:31<01:22,  2.20it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 203/384 [01:32<01:21,  2.21it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 204/384 [01:32<01:23,  2.16it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 205/384 [01:33<01:21,  2.20it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 206/384 [01:33<01:21,  2.18it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 207/384 [01:34<01:22,  2.13it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 208/384 [01:34<01:20,  2.18it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 209/384 [01:35<01:20,  2.18it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 210/384 [01:35<01:17,  2.24it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 211/384 [01:35<01:17,  2.24it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 212/384 [01:36<01:17,  2.21it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 213/384 [01:36<01:17,  2.19it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 214/384 [01:37<01:16,  2.22it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 215/384 [01:37<01:19,  2.12it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 216/384 [01:38<01:17,  2.18it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 217/384 [01:38<01:16,  2.17it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 218/384 [01:39<01:14,  2.22it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 219/384 [01:39<01:12,  2.27it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 220/384 [01:40<01:11,  2.29it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 221/384 [01:40<01:12,  2.24it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 222/384 [01:40<01:12,  2.23it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 223/384 [01:41<01:11,  2.25it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 224/384 [01:41<01:10,  2.28it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 225/384 [01:42<01:12,  2.20it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 226/384 [01:42<01:11,  2.20it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 227/384 [01:43<01:11,  2.21it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 228/384 [01:43<01:08,  2.27it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 229/384 [01:44<01:07,  2.29it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 230/384 [01:44<01:09,  2.22it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 231/384 [01:44<01:08,  2.22it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 232/384 [01:45<01:07,  2.25it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 233/384 [01:45<01:06,  2.27it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 234/384 [01:46<01:06,  2.26it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 235/384 [01:46<01:06,  2.23it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 236/384 [01:47<01:06,  2.22it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 237/384 [01:47<01:05,  2.26it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 238/384 [01:48<01:09,  2.11it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 239/384 [01:48<01:09,  2.09it/s]\u001b[A\n",
            "Predicting batches:  62%|       | 240/384 [01:49<01:07,  2.12it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 241/384 [01:49<01:06,  2.14it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 242/384 [01:50<01:05,  2.17it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 243/384 [01:50<01:03,  2.20it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 244/384 [01:50<01:05,  2.13it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 245/384 [01:51<01:03,  2.20it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 246/384 [01:51<01:02,  2.21it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 247/384 [01:52<01:01,  2.22it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 248/384 [01:52<01:00,  2.25it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 249/384 [01:53<01:00,  2.23it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 250/384 [01:53<01:03,  2.10it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 251/384 [01:54<01:02,  2.12it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 252/384 [01:54<01:03,  2.07it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 253/384 [01:55<01:01,  2.13it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 254/384 [01:55<01:01,  2.11it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 255/384 [01:56<01:00,  2.15it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 256/384 [01:56<01:00,  2.11it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 257/384 [01:56<00:58,  2.16it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 258/384 [01:57<00:56,  2.22it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 259/384 [01:57<00:57,  2.19it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 260/384 [01:58<00:55,  2.23it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 261/384 [01:58<00:56,  2.19it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 262/384 [01:59<00:54,  2.23it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 263/384 [01:59<00:53,  2.26it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 264/384 [02:00<00:53,  2.24it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 265/384 [02:00<00:53,  2.21it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 266/384 [02:01<00:53,  2.19it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 267/384 [02:01<00:55,  2.11it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 268/384 [02:02<00:54,  2.13it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 269/384 [02:02<00:54,  2.10it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 270/384 [02:02<00:54,  2.10it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 271/384 [02:03<00:53,  2.12it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 272/384 [02:03<00:51,  2.16it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 273/384 [02:04<00:52,  2.11it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 274/384 [02:04<00:51,  2.13it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 275/384 [02:05<00:51,  2.13it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 276/384 [02:05<00:50,  2.15it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 277/384 [02:06<00:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 278/384 [02:06<00:48,  2.18it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 279/384 [02:07<00:48,  2.18it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 280/384 [02:07<00:48,  2.13it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 281/384 [02:08<00:48,  2.13it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 282/384 [02:08<00:47,  2.14it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 283/384 [02:08<00:46,  2.19it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 284/384 [02:09<00:44,  2.23it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 285/384 [02:09<00:43,  2.27it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 286/384 [02:10<00:45,  2.17it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 287/384 [02:10<00:43,  2.21it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 288/384 [02:11<00:43,  2.19it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 289/384 [02:11<00:43,  2.18it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 290/384 [02:12<00:42,  2.22it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 291/384 [02:12<00:41,  2.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 292/384 [02:13<00:41,  2.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 293/384 [02:13<00:40,  2.26it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 294/384 [02:13<00:39,  2.27it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 295/384 [02:14<00:39,  2.25it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 296/384 [02:14<00:38,  2.26it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 297/384 [02:15<00:38,  2.29it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 298/384 [02:15<00:37,  2.27it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 299/384 [02:16<00:37,  2.28it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 300/384 [02:16<00:36,  2.28it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 301/384 [02:16<00:37,  2.24it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 302/384 [02:17<00:36,  2.25it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 303/384 [02:17<00:38,  2.12it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 304/384 [02:18<00:39,  2.05it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 305/384 [02:18<00:38,  2.04it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 306/384 [02:19<00:36,  2.14it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 307/384 [02:19<00:35,  2.17it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 308/384 [02:20<00:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 309/384 [02:20<00:33,  2.22it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 310/384 [02:21<00:33,  2.19it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 311/384 [02:21<00:33,  2.19it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 312/384 [02:22<00:34,  2.10it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 313/384 [02:22<00:33,  2.14it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 314/384 [02:23<00:32,  2.17it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 315/384 [02:23<00:31,  2.20it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 316/384 [02:23<00:30,  2.24it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 317/384 [02:24<00:29,  2.26it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 318/384 [02:24<00:29,  2.23it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 319/384 [02:25<00:28,  2.25it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 320/384 [02:25<00:28,  2.23it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 321/384 [02:26<00:28,  2.21it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 322/384 [02:26<00:28,  2.20it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 323/384 [02:27<00:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 324/384 [02:27<00:29,  2.06it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 325/384 [02:28<00:28,  2.03it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 326/384 [02:28<00:27,  2.08it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 327/384 [02:29<00:26,  2.17it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 328/384 [02:29<00:25,  2.19it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 329/384 [02:29<00:24,  2.20it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 330/384 [02:30<00:24,  2.18it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 331/384 [02:30<00:24,  2.16it/s]\u001b[A\n",
            "Predicting batches:  86%|  | 332/384 [02:31<00:24,  2.15it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 333/384 [02:31<00:24,  2.08it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 334/384 [02:32<00:23,  2.10it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 335/384 [02:32<00:22,  2.18it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 336/384 [02:33<00:22,  2.11it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 337/384 [02:33<00:22,  2.06it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 338/384 [02:34<00:21,  2.15it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 339/384 [02:34<00:20,  2.16it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 340/384 [02:35<00:20,  2.16it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 341/384 [02:35<00:19,  2.18it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 342/384 [02:35<00:19,  2.20it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 343/384 [02:36<00:18,  2.23it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 344/384 [02:36<00:17,  2.25it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 345/384 [02:37<00:17,  2.26it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 346/384 [02:37<00:17,  2.20it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 347/384 [02:38<00:16,  2.19it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 348/384 [02:38<00:16,  2.19it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 349/384 [02:39<00:15,  2.21it/s]\u001b[A\n",
            "Predicting batches:  91%| | 350/384 [02:39<00:15,  2.22it/s]\u001b[A\n",
            "Predicting batches:  91%| | 351/384 [02:40<00:15,  2.19it/s]\u001b[A\n",
            "Predicting batches:  92%| | 352/384 [02:40<00:14,  2.13it/s]\u001b[A\n",
            "Predicting batches:  92%| | 353/384 [02:41<00:14,  2.16it/s]\u001b[A\n",
            "Predicting batches:  92%| | 354/384 [02:41<00:13,  2.17it/s]\u001b[A\n",
            "Predicting batches:  92%| | 355/384 [02:41<00:13,  2.11it/s]\u001b[A\n",
            "Predicting batches:  93%| | 356/384 [02:42<00:13,  2.14it/s]\u001b[A\n",
            "Predicting batches:  93%| | 357/384 [02:42<00:12,  2.15it/s]\u001b[A\n",
            "Predicting batches:  93%| | 358/384 [02:43<00:11,  2.19it/s]\u001b[A\n",
            "Predicting batches:  93%| | 359/384 [02:43<00:11,  2.19it/s]\u001b[A\n",
            "Predicting batches:  94%| | 360/384 [02:44<00:10,  2.19it/s]\u001b[A\n",
            "Predicting batches:  94%| | 361/384 [02:44<00:10,  2.15it/s]\u001b[A\n",
            "Predicting batches:  94%| | 362/384 [02:45<00:10,  2.20it/s]\u001b[A\n",
            "Predicting batches:  95%| | 363/384 [02:45<00:09,  2.22it/s]\u001b[A\n",
            "Predicting batches:  95%| | 364/384 [02:46<00:08,  2.24it/s]\u001b[A\n",
            "Predicting batches:  95%| | 365/384 [02:46<00:08,  2.26it/s]\u001b[A\n",
            "Predicting batches:  95%| | 366/384 [02:46<00:08,  2.24it/s]\u001b[A\n",
            "Predicting batches:  96%| | 367/384 [02:47<00:07,  2.24it/s]\u001b[A\n",
            "Predicting batches:  96%|| 368/384 [02:47<00:07,  2.26it/s]\u001b[A\n",
            "Predicting batches:  96%|| 369/384 [02:48<00:06,  2.23it/s]\u001b[A\n",
            "Predicting batches:  96%|| 370/384 [02:48<00:06,  2.27it/s]\u001b[A\n",
            "Predicting batches:  97%|| 371/384 [02:49<00:05,  2.31it/s]\u001b[A\n",
            "Predicting batches:  97%|| 372/384 [02:49<00:05,  2.10it/s]\u001b[A\n",
            "Predicting batches:  97%|| 373/384 [02:50<00:05,  2.16it/s]\u001b[A\n",
            "Predicting batches:  97%|| 374/384 [02:50<00:04,  2.12it/s]\u001b[A\n",
            "Predicting batches:  98%|| 375/384 [02:51<00:04,  2.13it/s]\u001b[A\n",
            "Predicting batches:  98%|| 376/384 [02:51<00:03,  2.18it/s]\u001b[A\n",
            "Predicting batches:  98%|| 377/384 [02:51<00:03,  2.13it/s]\u001b[A\n",
            "Predicting batches:  98%|| 378/384 [02:52<00:02,  2.14it/s]\u001b[A\n",
            "Predicting batches:  99%|| 379/384 [02:52<00:02,  2.17it/s]\u001b[A\n",
            "Predicting batches:  99%|| 380/384 [02:53<00:01,  2.21it/s]\u001b[A\n",
            "Predicting batches:  99%|| 381/384 [02:53<00:01,  2.16it/s]\u001b[A\n",
            "Predicting batches:  99%|| 382/384 [02:54<00:00,  2.17it/s]\u001b[A\n",
            "Predicting batches: 100%|| 383/384 [02:54<00:00,  2.18it/s]\u001b[A\n",
            "Predicting batches: 100%|| 384/384 [02:54<00:00,  2.59it/s]\u001b[A\n",
            "Processing subjects:  86%|| 49/57 [09:35<08:15, 61.96s/it, professional medicin\u001b[A\n",
            "Formatting batches:   0%|                               | 0/272 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 43/272 [00:00<00:00, 421.27it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 89/272 [00:00<00:00, 439.15it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 134/272 [00:00<00:00, 443.20it/s]\u001b[A\n",
            "Formatting batches:  66%|      | 179/272 [00:00<00:00, 443.56it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 224/272 [00:00<00:00, 443.75it/s]\u001b[A\n",
            "Formatting batches:  99%|| 269/272 [00:00<00:00, 444.88it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/68 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/68 [00:00<00:18,  3.66it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/68 [00:00<00:18,  3.52it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/68 [00:00<00:18,  3.58it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/68 [00:01<00:17,  3.66it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/68 [00:01<00:18,  3.41it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 6/68 [00:01<00:18,  3.32it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/68 [00:02<00:17,  3.42it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/68 [00:02<00:17,  3.47it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 9/68 [00:02<00:17,  3.37it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/68 [00:02<00:16,  3.42it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 11/68 [00:03<00:16,  3.48it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/68 [00:03<00:15,  3.58it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 13/68 [00:03<00:15,  3.57it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/68 [00:04<00:15,  3.46it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 15/68 [00:04<00:15,  3.49it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/68 [00:04<00:14,  3.47it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 17/68 [00:04<00:14,  3.42it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 18/68 [00:05<00:14,  3.48it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 19/68 [00:05<00:13,  3.57it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 20/68 [00:05<00:13,  3.66it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 21/68 [00:05<00:12,  3.70it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 22/68 [00:06<00:12,  3.81it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 23/68 [00:06<00:12,  3.72it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 24/68 [00:06<00:12,  3.67it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 25/68 [00:07<00:12,  3.55it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 26/68 [00:07<00:11,  3.55it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 27/68 [00:07<00:11,  3.51it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 28/68 [00:07<00:11,  3.42it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 29/68 [00:08<00:11,  3.49it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 30/68 [00:08<00:10,  3.46it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 31/68 [00:08<00:10,  3.50it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 32/68 [00:09<00:10,  3.59it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 33/68 [00:09<00:10,  3.47it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 34/68 [00:09<00:09,  3.51it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 35/68 [00:09<00:09,  3.41it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 36/68 [00:10<00:09,  3.51it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 37/68 [00:10<00:08,  3.47it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 38/68 [00:10<00:08,  3.51it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 39/68 [00:11<00:08,  3.48it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 40/68 [00:11<00:08,  3.40it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 41/68 [00:11<00:07,  3.47it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 42/68 [00:11<00:07,  3.52it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 43/68 [00:12<00:07,  3.56it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 44/68 [00:12<00:06,  3.51it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 45/68 [00:12<00:06,  3.39it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 46/68 [00:13<00:06,  3.42it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 47/68 [00:13<00:06,  3.38it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 48/68 [00:13<00:06,  3.31it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 49/68 [00:14<00:05,  3.40it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 50/68 [00:14<00:05,  3.32it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 51/68 [00:14<00:05,  3.31it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 52/68 [00:14<00:04,  3.40it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 53/68 [00:15<00:04,  3.37it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 54/68 [00:15<00:04,  3.34it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 55/68 [00:15<00:03,  3.43it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 56/68 [00:16<00:03,  3.49it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 57/68 [00:16<00:03,  3.51it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 58/68 [00:16<00:02,  3.54it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 59/68 [00:16<00:02,  3.50it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 60/68 [00:17<00:02,  3.43it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 61/68 [00:17<00:02,  3.43it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 62/68 [00:17<00:01,  3.38it/s]\u001b[A\n",
            "Predicting batches:  93%| | 63/68 [00:18<00:01,  3.39it/s]\u001b[A\n",
            "Predicting batches:  94%| | 64/68 [00:18<00:01,  3.45it/s]\u001b[A\n",
            "Predicting batches:  96%| | 65/68 [00:18<00:00,  3.48it/s]\u001b[A\n",
            "Predicting batches:  97%|| 66/68 [00:18<00:00,  3.51it/s]\u001b[A\n",
            "Predicting batches:  99%|| 67/68 [00:19<00:00,  3.58it/s]\u001b[A\n",
            "Predicting batches: 100%|| 68/68 [00:19<00:00,  3.60it/s]\u001b[A\n",
            "Processing subjects:  88%|| 50/57 [09:55<05:45, 49.41s/it, professional psychol\u001b[A\n",
            "Formatting batches:   0%|                               | 0/612 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   9%|                   | 54/612 [00:00<00:01, 533.95it/s]\u001b[A\n",
            "Formatting batches:  18%|                | 108/612 [00:00<00:00, 536.41it/s]\u001b[A\n",
            "Formatting batches:  26%|              | 162/612 [00:00<00:00, 534.71it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 216/612 [00:00<00:00, 533.40it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 270/612 [00:00<00:00, 535.09it/s]\u001b[A\n",
            "Formatting batches:  53%|         | 324/612 [00:00<00:00, 535.72it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 379/612 [00:00<00:00, 537.90it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 433/612 [00:00<00:00, 537.61it/s]\u001b[A\n",
            "Formatting batches:  80%|    | 488/612 [00:00<00:00, 538.68it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 543/612 [00:01<00:00, 539.33it/s]\u001b[A\n",
            "Formatting batches:  98%|| 597/612 [00:01<00:00, 526.55it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/153 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 1/153 [00:00<00:20,  7.53it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/153 [00:00<00:19,  7.67it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 3/153 [00:00<00:21,  7.12it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 4/153 [00:00<00:20,  7.15it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 5/153 [00:00<00:21,  7.05it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 6/153 [00:00<00:20,  7.02it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 7/153 [00:00<00:20,  7.07it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 8/153 [00:01<00:20,  7.15it/s]\u001b[A\n",
            "Predicting batches:   6%|                     | 9/153 [00:01<00:19,  7.39it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 10/153 [00:01<00:19,  7.32it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 11/153 [00:01<00:19,  7.18it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 12/153 [00:01<00:19,  7.07it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 13/153 [00:01<00:19,  7.08it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 14/153 [00:01<00:19,  7.16it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 15/153 [00:02<00:19,  7.08it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 16/153 [00:02<00:19,  6.89it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 17/153 [00:02<00:19,  7.03it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 18/153 [00:02<00:19,  6.87it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 19/153 [00:02<00:19,  6.88it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 20/153 [00:02<00:18,  7.09it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 21/153 [00:02<00:18,  7.24it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 22/153 [00:03<00:18,  7.15it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 23/153 [00:03<00:18,  7.13it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 24/153 [00:03<00:17,  7.27it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 25/153 [00:03<00:17,  7.21it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 26/153 [00:03<00:17,  7.40it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 27/153 [00:03<00:17,  7.24it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 28/153 [00:03<00:16,  7.39it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 29/153 [00:04<00:16,  7.48it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 30/153 [00:04<00:16,  7.54it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 31/153 [00:04<00:16,  7.57it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 32/153 [00:04<00:15,  7.57it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 33/153 [00:04<00:16,  7.19it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 34/153 [00:04<00:16,  7.11it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 35/153 [00:04<00:16,  7.16it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 36/153 [00:04<00:16,  7.29it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 37/153 [00:05<00:15,  7.42it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 38/153 [00:05<00:15,  7.47it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 39/153 [00:05<00:15,  7.37it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 40/153 [00:05<00:15,  7.35it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 41/153 [00:05<00:15,  7.43it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 42/153 [00:05<00:15,  7.33it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 43/153 [00:05<00:15,  7.17it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 44/153 [00:06<00:15,  7.21it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 45/153 [00:06<00:14,  7.33it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 46/153 [00:06<00:14,  7.43it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 47/153 [00:06<00:14,  7.33it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 48/153 [00:06<00:14,  7.18it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 49/153 [00:06<00:14,  7.15it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 50/153 [00:06<00:14,  7.20it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 51/153 [00:07<00:13,  7.32it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 52/153 [00:07<00:14,  7.16it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 53/153 [00:07<00:13,  7.19it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 54/153 [00:07<00:13,  7.21it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 55/153 [00:07<00:13,  7.18it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 56/153 [00:07<00:13,  7.10it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 57/153 [00:07<00:13,  7.16it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 58/153 [00:08<00:13,  7.20it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 59/153 [00:08<00:13,  7.17it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 60/153 [00:08<00:12,  7.38it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 61/153 [00:08<00:12,  7.35it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 62/153 [00:08<00:12,  7.35it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 63/153 [00:08<00:12,  7.34it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 64/153 [00:08<00:11,  7.45it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 65/153 [00:08<00:11,  7.40it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 66/153 [00:09<00:11,  7.46it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 67/153 [00:09<00:11,  7.53it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 68/153 [00:09<00:11,  7.56it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 69/153 [00:09<00:11,  7.32it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 70/153 [00:09<00:11,  7.25it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 71/153 [00:09<00:11,  7.00it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 72/153 [00:09<00:11,  7.17it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 73/153 [00:10<00:11,  7.07it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 74/153 [00:10<00:10,  7.21it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 75/153 [00:10<00:10,  7.35it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 76/153 [00:10<00:10,  7.34it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 77/153 [00:10<00:10,  7.41it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 78/153 [00:10<00:10,  7.26it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 79/153 [00:10<00:10,  6.99it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 80/153 [00:11<00:10,  7.19it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 81/153 [00:11<00:09,  7.22it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 82/153 [00:11<00:09,  7.24it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 83/153 [00:11<00:10,  6.78it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 84/153 [00:11<00:09,  7.03it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 85/153 [00:11<00:09,  7.21it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 86/153 [00:11<00:09,  7.34it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 87/153 [00:12<00:09,  7.27it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 88/153 [00:12<00:09,  7.22it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 89/153 [00:12<00:08,  7.24it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 90/153 [00:12<00:08,  7.27it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 91/153 [00:12<00:08,  7.27it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 92/153 [00:12<00:08,  7.37it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 93/153 [00:12<00:08,  7.43it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 94/153 [00:12<00:08,  7.33it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 95/153 [00:13<00:07,  7.32it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 96/153 [00:13<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 97/153 [00:13<00:07,  7.47it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 98/153 [00:13<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 99/153 [00:13<00:07,  7.37it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 100/153 [00:13<00:07,  7.44it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 101/153 [00:13<00:06,  7.58it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 102/153 [00:14<00:06,  7.61it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 103/153 [00:14<00:06,  7.60it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 104/153 [00:14<00:06,  7.50it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 105/153 [00:14<00:06,  7.44it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 106/153 [00:14<00:06,  6.91it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 107/153 [00:14<00:06,  7.10it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 108/153 [00:14<00:06,  7.16it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 109/153 [00:15<00:06,  7.08it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 110/153 [00:15<00:05,  7.23it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 111/153 [00:15<00:05,  7.13it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 112/153 [00:15<00:05,  7.04it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 113/153 [00:15<00:05,  6.97it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 114/153 [00:15<00:05,  6.95it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 115/153 [00:15<00:05,  7.14it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 116/153 [00:16<00:05,  7.27it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 117/153 [00:16<00:05,  7.16it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 118/153 [00:16<00:04,  7.29it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 119/153 [00:16<00:04,  7.24it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 120/153 [00:16<00:04,  7.38it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 121/153 [00:16<00:04,  7.09it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 122/153 [00:16<00:04,  7.14it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 123/153 [00:16<00:04,  7.28it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 124/153 [00:17<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 125/153 [00:17<00:03,  7.23it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 126/153 [00:17<00:03,  7.19it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 127/153 [00:17<00:03,  7.08it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 128/153 [00:17<00:03,  7.26it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 129/153 [00:17<00:03,  7.27it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 130/153 [00:17<00:03,  7.36it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 131/153 [00:18<00:02,  7.34it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 132/153 [00:18<00:02,  7.21it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 133/153 [00:18<00:02,  7.18it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 134/153 [00:18<00:02,  7.22it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 135/153 [00:18<00:02,  7.24it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 136/153 [00:18<00:02,  7.21it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 137/153 [00:18<00:02,  7.18it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 138/153 [00:19<00:02,  7.21it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 139/153 [00:19<00:01,  7.33it/s]\u001b[A\n",
            "Predicting batches:  92%| | 140/153 [00:19<00:01,  7.40it/s]\u001b[A\n",
            "Predicting batches:  92%| | 141/153 [00:19<00:01,  7.25it/s]\u001b[A\n",
            "Predicting batches:  93%| | 142/153 [00:19<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  93%| | 143/153 [00:19<00:01,  7.18it/s]\u001b[A\n",
            "Predicting batches:  94%| | 144/153 [00:19<00:01,  7.38it/s]\u001b[A\n",
            "Predicting batches:  95%| | 145/153 [00:19<00:01,  7.47it/s]\u001b[A\n",
            "Predicting batches:  95%| | 146/153 [00:20<00:00,  7.54it/s]\u001b[A\n",
            "Predicting batches:  96%|| 147/153 [00:20<00:00,  7.08it/s]\u001b[A\n",
            "Predicting batches:  97%|| 148/153 [00:20<00:00,  7.14it/s]\u001b[A\n",
            "Predicting batches:  97%|| 149/153 [00:20<00:00,  7.07it/s]\u001b[A\n",
            "Predicting batches:  98%|| 150/153 [00:20<00:00,  7.00it/s]\u001b[A\n",
            "Predicting batches:  99%|| 151/153 [00:20<00:00,  7.08it/s]\u001b[A\n",
            "Predicting batches:  99%|| 152/153 [00:20<00:00,  7.23it/s]\u001b[A\n",
            "Predicting batches: 100%|| 153/153 [00:21<00:00,  7.33it/s]\u001b[A\n",
            "Processing subjects:  89%|| 51/57 [10:17<04:07, 41.27s/it, public relations]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 59/110 [00:00<00:00, 584.98it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/28 [00:00<00:03,  8.48it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 3/28 [00:00<00:02,  9.98it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/28 [00:00<00:02,  9.97it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 5/28 [00:00<00:02,  9.27it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 6/28 [00:00<00:02,  9.25it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 8/28 [00:00<00:02,  9.89it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 10/28 [00:01<00:01, 10.04it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 11/28 [00:01<00:01, 10.01it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 12/28 [00:01<00:01,  9.96it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 14/28 [00:01<00:01, 10.23it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 16/28 [00:01<00:01, 10.11it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 18/28 [00:01<00:00, 10.26it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 20/28 [00:01<00:00, 10.37it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 22/28 [00:02<00:00, 10.20it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 24/28 [00:02<00:00, 10.32it/s]\u001b[A\n",
            "Predicting batches:  93%| | 26/28 [00:02<00:00, 10.41it/s]\u001b[A\n",
            "Predicting batches: 100%|| 28/28 [00:02<00:00, 11.33it/s]\u001b[A\n",
            "Processing subjects:  91%|| 52/57 [10:20<02:28, 29.76s/it, security studies]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 42/245 [00:00<00:00, 417.78it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 85/245 [00:00<00:00, 421.51it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 128/245 [00:00<00:00, 420.62it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 171/245 [00:00<00:00, 423.20it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 214/245 [00:00<00:00, 423.69it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/62 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/62 [00:00<00:18,  3.30it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/62 [00:00<00:19,  3.13it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/62 [00:00<00:18,  3.22it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/62 [00:01<00:17,  3.25it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/62 [00:01<00:18,  3.14it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/62 [00:01<00:17,  3.17it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 7/62 [00:02<00:17,  3.21it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 8/62 [00:02<00:16,  3.19it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/62 [00:02<00:16,  3.18it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 10/62 [00:03<00:16,  3.17it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 11/62 [00:03<00:16,  3.18it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 12/62 [00:03<00:15,  3.22it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 13/62 [00:04<00:15,  3.21it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 14/62 [00:04<00:15,  3.07it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 15/62 [00:04<00:15,  3.11it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 16/62 [00:05<00:14,  3.14it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 17/62 [00:05<00:14,  3.10it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 18/62 [00:05<00:13,  3.18it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 19/62 [00:05<00:13,  3.19it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 20/62 [00:06<00:12,  3.31it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 21/62 [00:06<00:12,  3.31it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 22/62 [00:06<00:12,  3.28it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 23/62 [00:07<00:12,  3.19it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 24/62 [00:07<00:11,  3.22it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 25/62 [00:07<00:11,  3.16it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 26/62 [00:08<00:11,  3.20it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 27/62 [00:08<00:10,  3.24it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 28/62 [00:08<00:10,  3.21it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 29/62 [00:09<00:10,  3.19it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 30/62 [00:09<00:10,  3.20it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 31/62 [00:09<00:09,  3.26it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 32/62 [00:09<00:09,  3.24it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 33/62 [00:10<00:09,  3.21it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 34/62 [00:10<00:08,  3.31it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 35/62 [00:10<00:07,  3.38it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 36/62 [00:11<00:07,  3.39it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 37/62 [00:11<00:07,  3.31it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 38/62 [00:11<00:07,  3.21it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 39/62 [00:12<00:07,  3.27it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 40/62 [00:12<00:06,  3.29it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 41/62 [00:12<00:06,  3.20it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 42/62 [00:13<00:06,  3.25it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 43/62 [00:13<00:05,  3.22it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 44/62 [00:13<00:05,  3.22it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 45/62 [00:13<00:05,  3.28it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 46/62 [00:14<00:04,  3.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 47/62 [00:14<00:04,  3.23it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 48/62 [00:14<00:04,  3.25it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 49/62 [00:15<00:04,  3.25it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 50/62 [00:15<00:03,  3.24it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 51/62 [00:15<00:03,  3.25it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 52/62 [00:16<00:03,  3.22it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 53/62 [00:16<00:02,  3.20it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 54/62 [00:16<00:02,  3.22it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 55/62 [00:17<00:02,  3.20it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 56/62 [00:17<00:01,  3.21it/s]\u001b[A\n",
            "Predicting batches:  92%| | 57/62 [00:17<00:01,  3.31it/s]\u001b[A\n",
            "Predicting batches:  94%| | 58/62 [00:17<00:01,  3.31it/s]\u001b[A\n",
            "Predicting batches:  95%| | 59/62 [00:18<00:00,  3.20it/s]\u001b[A\n",
            "Predicting batches:  97%|| 60/62 [00:18<00:00,  3.18it/s]\u001b[A\n",
            "Predicting batches:  98%|| 61/62 [00:18<00:00,  3.19it/s]\u001b[A\n",
            "Processing subjects:  93%|| 53/57 [10:39<01:46, 26.71s/it, sociology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/201 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  29%|               | 58/201 [00:00<00:00, 575.31it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 116/201 [00:00<00:00, 578.04it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 175/201 [00:00<00:00, 581.11it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:00<00:04, 10.17it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:00<00:04,  9.61it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:00<00:04,  9.64it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:00<00:04,  9.38it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:00<00:04,  9.48it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:00<00:04,  9.56it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:00<00:04,  9.62it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:01<00:04,  9.65it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:01<00:04,  9.68it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:01<00:04,  9.52it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:01<00:03,  9.61it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:01<00:03,  9.36it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:01<00:03,  9.32it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:01<00:03,  9.29it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:01<00:03,  9.43it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:01<00:03,  9.35it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:02<00:03,  9.30it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:02<00:03,  9.26it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:02<00:03,  9.22it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:02<00:03,  9.20it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:02<00:02,  9.38it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:02<00:02,  9.32it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:02<00:02,  9.29it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:02<00:02,  9.41it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:02<00:02,  9.34it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:02<00:02,  9.47it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:03<00:02,  9.58it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:03<00:02,  9.68it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:03<00:02,  9.75it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:03<00:01,  9.56it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:03<00:01,  9.62it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:03<00:01,  9.66it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:03<00:01,  9.67it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:03<00:01,  9.72it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:03<00:01,  9.73it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:04<00:01,  9.91it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:04<00:01,  9.72it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:04<00:01,  9.76it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:04<00:00,  9.58it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:04<00:00,  9.63it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:04<00:00,  9.49it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:04<00:00,  9.59it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:04<00:00,  9.63it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:04<00:00,  9.50it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:05<00:00,  9.39it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:05<00:00,  9.49it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:05<00:00,  9.56it/s]\u001b[A\n",
            "Processing subjects:  95%|| 54/57 [10:45<01:01, 20.39s/it, us foreign policy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  57%|         | 57/100 [00:00<00:00, 566.55it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02, 10.18it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02, 10.21it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 10.06it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01,  9.98it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 10.05it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01, 10.06it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  9.97it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 10.13it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 10.23it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 10.09it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00, 10.13it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:02<00:00, 10.04it/s]\u001b[A\n",
            "Processing subjects:  96%|| 55/57 [10:48<00:30, 15.07s/it, virology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/166 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  36%|             | 59/166 [00:00<00:00, 588.04it/s]\u001b[A\n",
            "Formatting batches:  72%|     | 120/166 [00:00<00:00, 595.92it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/42 [00:00<00:03, 13.32it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/42 [00:00<00:02, 13.18it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/42 [00:00<00:02, 12.81it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/42 [00:00<00:02, 12.73it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/42 [00:00<00:02, 12.78it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/42 [00:00<00:02, 12.36it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/42 [00:01<00:02, 12.23it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 16/42 [00:01<00:02, 12.43it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 18/42 [00:01<00:01, 12.42it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 20/42 [00:01<00:01, 12.40it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 22/42 [00:01<00:01, 12.57it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 24/42 [00:01<00:01, 12.64it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 26/42 [00:02<00:01, 12.61it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 28/42 [00:02<00:01, 12.29it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 30/42 [00:02<00:00, 12.46it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 32/42 [00:02<00:00, 12.45it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 34/42 [00:02<00:00, 12.21it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 36/42 [00:02<00:00, 12.36it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 38/42 [00:03<00:00, 12.50it/s]\u001b[A\n",
            "Predicting batches:  95%| | 40/42 [00:03<00:00, 10.96it/s]\u001b[A\n",
            "Predicting batches: 100%|| 42/42 [00:03<00:00, 11.98it/s]\u001b[A\n",
            "Processing subjects:  98%|| 56/57 [10:51<00:11, 11.66s/it, world religions]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/171 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 60/171 [00:00<00:00, 599.62it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 122/171 [00:00<00:00, 611.45it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 2/43 [00:00<00:02, 16.84it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 4/43 [00:00<00:02, 16.51it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/43 [00:00<00:02, 16.33it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/43 [00:00<00:02, 15.80it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 10/43 [00:00<00:02, 16.37it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 12/43 [00:00<00:01, 16.31it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/43 [00:00<00:01, 16.29it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 16/43 [00:00<00:01, 15.91it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 18/43 [00:01<00:01, 16.02it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 20/43 [00:01<00:01, 15.68it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 22/43 [00:01<00:01, 15.44it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 24/43 [00:01<00:01, 15.32it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 26/43 [00:01<00:01, 15.57it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 28/43 [00:01<00:00, 16.10it/s]\u001b[A\n",
            "Predicting batches:  70%|       | 30/43 [00:01<00:00, 16.13it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 32/43 [00:02<00:00, 15.75it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 34/43 [00:02<00:00, 16.21it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 36/43 [00:02<00:00, 16.20it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 38/43 [00:02<00:00, 15.81it/s]\u001b[A\n",
            "Predicting batches:  93%| | 40/43 [00:02<00:00, 15.89it/s]\u001b[A\n",
            "Predicting batches:  98%|| 42/43 [00:02<00:00, 15.96it/s]\u001b[A\n",
            "Processing subjects: 100%|| 57/57 [10:54<00:00, 11.49s/it, world religions]\u001b[A\n",
            "        Average: 58.80\n",
            "           STEM: 47.98\n",
            "Social Sciences: 67.99\n",
            "     Humanities: 55.71\n",
            "          Other: 64.62\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli eval InstructionTuning_Freeze_eval.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RgFh-Hgmyf7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# --- NLG Quality Prediction (BLEU / ROUGE) ---\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    model_name_or_path=\"InstructionTuning_LoRA_Alpaca\",\n",
        "    # adapter_name_or_path=\"InstructionTuning_LoRA_Alpaca\",\n",
        "    trust_remote_code=True,\n",
        "    template=\"llama3\",\n",
        "\n",
        "    # Method settings\n",
        "    stage=\"sft\",\n",
        "    do_predict=True,\n",
        "    finetuning_type=\"freeze\",\n",
        "\n",
        "    # Dataset settings\n",
        "    eval_dataset=\"alpaca_en\",\n",
        "    cutoff_len=2048,\n",
        "    max_samples=50,\n",
        "    overwrite_cache=True,\n",
        "    preprocessing_num_workers=16,\n",
        "\n",
        "    # Output and evaluation\n",
        "    output_dir=\"InstructionTuning_freeze_Alpaca/predict\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_eval_batch_size=1,\n",
        "    predict_with_generate=True,\n",
        "    ddp_timeout=180000000,\n",
        ")\n",
        "\n",
        "# write out your predict config\n",
        "json.dump(args, open(\"InstructionTuning_Freeze_eval.json\", \"w\"), indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMPhFPWRmyf7"
      },
      "outputs": [],
      "source": [
        "!llamafactory-cli train InstructionTuning_Freeze_eval.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dHv69-4myf8"
      },
      "source": [
        "### Evalution check on instruction tuning lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X1kfc0zmyf8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# --- GeneralCapability Evaluation ---\n",
        "args = dict(\n",
        "    # Model settings\n",
        "    # This should be the path to the BASE model you fine-tuned with LoRA\n",
        "    # In your case, it seems it might be 'llama3_3b_freeze_instructionTuning'\n",
        "    model_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_3b_freeze_instructionTuning\",\n",
        "\n",
        "    # This is the path to your LoRA adapter weights (which caused the error)\n",
        "    adapter_name_or_path=\"/home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/InstructionTuning_LoRA_Alpaca\",\n",
        "\n",
        "    trust_remote_code=True,\n",
        "    template=\"fewshot\",         # fewshot prompt template\n",
        "\n",
        "    # Method settings\n",
        "    finetuning_type=\"lora\",     # Crucially, keep this as \"lora\" for LoRA evaluation\n",
        "\n",
        "    # Dataset settings\n",
        "    task=\"mmlu_test\",           # or ceval_validation, cmmlu_test\n",
        "    lang=\"en\",\n",
        "    n_shot=5,\n",
        "\n",
        "    # Output and evaluation\n",
        "    save_dir=\"instructiontuning_lora/eval\",\n",
        "    batch_size=4,\n",
        "    seed=42,\n",
        "    # download_mode=\"reuse\",      # reuse existing data if present\n",
        ")\n",
        "\n",
        "# write out your eval config\n",
        "json.dump(args, open(\"instructiontuning_lora_eval.json\", \"w\"), indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58lPI9Wymyf8",
        "outputId": "57601e17-833f-45d5-912d-3500198b8589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:08,863 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:08,863 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:08,863 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:08,863 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:08,863 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:08,863 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 12:17:09,104 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 12:17:09,107 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_3b_freeze_instructionTuning/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 12:17:09,108 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:09,108 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:09,108 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:09,108 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:09,108 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:09,108 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-06-20 12:17:09,108 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-06-20 12:17:09,344 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-06-20 12:17:09,361 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_3b_freeze_instructionTuning/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-06-20 12:17:09,361 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 24,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 12:17:09] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-06-20 12:17:09,448 >> loading weights file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_3b_freeze_instructionTuning/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-06-20 12:17:09,451 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 12:17:09,451 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|| 4/4 [00:05<00:00,  1.35s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-06-20 12:17:14,934 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-06-20 12:17:14,934 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_3b_freeze_instructionTuning.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-06-20 12:17:14,936 >> loading configuration file /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/llama3_3b_freeze_instructionTuning/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-06-20 12:17:14,936 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128008,\n",
            "    128009\n",
            "  ],\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-06-20 12:17:14] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-06-20 12:17:15] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
            "[INFO|2025-06-20 12:17:15] llamafactory.model.adapter:143 >> Loaded adapter(s): /home/ai-research-lab/MCSProjectbyWajahatAliBasharat/LLaMA-Factory/InstructionTuning_LoRA_Alpaca\n",
            "[INFO|2025-06-20 12:17:15] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
            "Processing subjects:   0%|             | 0/57 [00:00<?, ?it/s, abstract algebra]\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  59%|        | 59/100 [00:00<00:00, 584.57it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A[WARNING|logging.py:313] 2025-06-20 12:17:16,085 >> You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:04,  5.29it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  9.29it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:01, 10.50it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:01, 11.28it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:00<00:01, 11.73it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:00<00:01, 12.11it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:00, 12.21it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:00, 12.28it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:01<00:00, 12.31it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:01<00:00, 12.34it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:01<00:00, 12.34it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:01<00:00, 12.37it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:02<00:00, 12.46it/s]\u001b[A\n",
            "Processing subjects:   2%|             | 1/57 [00:02<02:08,  2.29s/it, anatomy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/135 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 63/135 [00:00<00:00, 623.91it/s]\u001b[A\n",
            "Formatting batches:  93%| | 126/135 [00:00<00:00, 625.55it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/34 [00:00<00:02, 13.33it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/34 [00:00<00:02, 12.49it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 6/34 [00:00<00:02, 12.92it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 8/34 [00:00<00:02, 12.72it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 10/34 [00:00<00:01, 12.97it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 12/34 [00:00<00:01, 12.79it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 14/34 [00:01<00:01, 12.67it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 16/34 [00:01<00:01, 12.64it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 18/34 [00:01<00:01, 12.58it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 20/34 [00:01<00:01, 12.32it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 22/34 [00:01<00:00, 12.45it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 24/34 [00:01<00:00, 12.27it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 26/34 [00:02<00:00, 12.40it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 28/34 [00:02<00:00, 12.46it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 30/34 [00:02<00:00, 12.67it/s]\u001b[A\n",
            "Predicting batches:  94%| | 32/34 [00:02<00:00, 12.78it/s]\u001b[A\n",
            "Predicting batches: 100%|| 34/34 [00:02<00:00, 13.40it/s]\u001b[A\n",
            "Processing subjects:   4%|           | 2/57 [00:05<02:25,  2.64s/it, astronomy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/152 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  37%|             | 56/152 [00:00<00:00, 551.31it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 113/152 [00:00<00:00, 559.48it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/38 [00:00<00:04,  8.09it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/38 [00:00<00:04,  7.67it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/38 [00:00<00:04,  7.28it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/38 [00:00<00:04,  7.58it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 5/38 [00:00<00:04,  7.64it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/38 [00:00<00:04,  7.67it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 7/38 [00:00<00:03,  7.83it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 8/38 [00:01<00:03,  7.64it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/38 [00:01<00:03,  7.77it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 10/38 [00:01<00:03,  7.70it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 11/38 [00:01<00:03,  7.69it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/38 [00:01<00:03,  7.80it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 13/38 [00:01<00:03,  7.72it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 14/38 [00:01<00:03,  7.66it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 15/38 [00:01<00:02,  7.77it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 16/38 [00:02<00:02,  7.85it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 17/38 [00:02<00:02,  7.92it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 18/38 [00:02<00:02,  7.85it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 19/38 [00:02<00:02,  7.93it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 20/38 [00:02<00:02,  7.87it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 21/38 [00:02<00:02,  7.70it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 22/38 [00:02<00:02,  7.65it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 23/38 [00:02<00:01,  7.66it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 24/38 [00:03<00:01,  7.67it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 25/38 [00:03<00:01,  7.80it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 26/38 [00:03<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 27/38 [00:03<00:01,  7.91it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 28/38 [00:03<00:01,  7.78it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 29/38 [00:03<00:01,  7.76it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 30/38 [00:03<00:01,  7.44it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 31/38 [00:04<00:00,  7.46it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 32/38 [00:04<00:00,  7.47it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 33/38 [00:04<00:00,  7.47it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 34/38 [00:04<00:00,  7.47it/s]\u001b[A\n",
            "Predicting batches:  92%| | 35/38 [00:04<00:00,  7.66it/s]\u001b[A\n",
            "Predicting batches:  95%| | 36/38 [00:04<00:00,  7.67it/s]\u001b[A\n",
            "Predicting batches:  97%|| 37/38 [00:04<00:00,  7.81it/s]\u001b[A\n",
            "Predicting batches: 100%|| 38/38 [00:04<00:00,  7.90it/s]\u001b[A\n",
            "Processing subjects:   5%|     | 3/57 [00:10<03:25,  3.81s/it, business ethics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  55%|         | 55/100 [00:00<00:00, 540.55it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:03,  7.99it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  8.07it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  7.92it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  7.96it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:02,  7.86it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.79it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:01,  7.75it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.73it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.81it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.77it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.68it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:01,  7.54it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.69it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  7.69it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  7.63it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.65it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  7.78it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.62it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00,  7.83it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.90it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  7.76it/s]\u001b[A\n",
            "Processing subjects:   7%|  | 4/57 [00:13<03:13,  3.66s/it, clinical knowledge]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/265 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  23%|                | 61/265 [00:00<00:00, 604.36it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 122/265 [00:00<00:00, 607.05it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 184/265 [00:00<00:00, 609.34it/s]\u001b[A\n",
            "Formatting batches:  93%| | 246/265 [00:00<00:00, 610.31it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/67 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/67 [00:00<00:05, 11.61it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/67 [00:00<00:05, 11.08it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 6/67 [00:00<00:05, 11.06it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/67 [00:00<00:05, 11.25it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/67 [00:00<00:05, 11.22it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/67 [00:01<00:04, 11.18it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/67 [00:01<00:04, 11.45it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/67 [00:01<00:04, 11.17it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 18/67 [00:01<00:04, 11.31it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 20/67 [00:01<00:04, 11.14it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 22/67 [00:01<00:03, 11.25it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 24/67 [00:02<00:03, 11.21it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 26/67 [00:02<00:03, 11.46it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 28/67 [00:02<00:03, 11.36it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 30/67 [00:02<00:03, 11.42it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 32/67 [00:02<00:03, 11.60it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 34/67 [00:03<00:02, 11.47it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 36/67 [00:03<00:02, 11.36it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 38/67 [00:03<00:02, 11.29it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 40/67 [00:03<00:02, 10.99it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 42/67 [00:03<00:02, 10.90it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 44/67 [00:03<00:02, 10.79it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 46/67 [00:04<00:01, 11.02it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 48/67 [00:04<00:01, 11.04it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 50/67 [00:04<00:01, 11.06it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 52/67 [00:04<00:01, 11.06it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 54/67 [00:04<00:01, 10.97it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 56/67 [00:05<00:00, 11.01it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 58/67 [00:05<00:00, 11.16it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 60/67 [00:05<00:00, 11.28it/s]\u001b[A\n",
            "Predicting batches:  93%| | 62/67 [00:05<00:00, 11.21it/s]\u001b[A\n",
            "Predicting batches:  96%| | 64/67 [00:05<00:00, 11.30it/s]\u001b[A\n",
            "Predicting batches:  99%|| 66/67 [00:05<00:00, 11.22it/s]\u001b[A\n",
            "Processing subjects:   9%|     | 5/57 [00:20<04:00,  4.63s/it, college biology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/144 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 58/144 [00:00<00:00, 573.32it/s]\u001b[A\n",
            "Formatting batches:  81%|    | 116/144 [00:00<00:00, 567.23it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/36 [00:00<00:03, 10.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/36 [00:00<00:03,  9.34it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 5/36 [00:00<00:03,  9.42it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 6/36 [00:00<00:03,  9.46it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 7/36 [00:00<00:03,  8.96it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 8/36 [00:00<00:03,  9.13it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 9/36 [00:00<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 10/36 [00:01<00:02,  9.25it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 11/36 [00:01<00:02,  8.70it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 12/36 [00:01<00:02,  8.86it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 13/36 [00:01<00:02,  9.06it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 14/36 [00:01<00:02,  8.82it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 15/36 [00:01<00:02,  8.54it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 16/36 [00:01<00:02,  8.82it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 17/36 [00:01<00:02,  8.92it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 18/36 [00:01<00:02,  8.75it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 20/36 [00:02<00:01,  8.96it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 22/36 [00:02<00:01,  9.41it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 23/36 [00:02<00:01,  9.13it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 24/36 [00:02<00:01,  8.81it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 25/36 [00:02<00:01,  8.99it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 26/36 [00:02<00:01,  9.06it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 27/36 [00:02<00:00,  9.20it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 28/36 [00:03<00:00,  8.97it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 29/36 [00:03<00:00,  8.80it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 30/36 [00:03<00:00,  8.91it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 31/36 [00:03<00:00,  9.00it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 32/36 [00:03<00:00,  9.16it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 33/36 [00:03<00:00,  9.26it/s]\u001b[A\n",
            "Predicting batches:  94%| | 34/36 [00:03<00:00,  9.21it/s]\u001b[A\n",
            "Predicting batches:  97%|| 35/36 [00:03<00:00,  9.20it/s]\u001b[A\n",
            "Predicting batches: 100%|| 36/36 [00:03<00:00,  9.19it/s]\u001b[A\n",
            "Processing subjects:  11%|   | 6/57 [00:24<03:49,  4.49s/it, college chemistry]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 58/100 [00:00<00:00, 577.29it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  8.27it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  8.34it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.15it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  8.02it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  7.99it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:02,  7.80it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.74it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:01,  7.90it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.81it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.94it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.96it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.69it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:01,  7.76it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.85it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.90it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  8.22it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  8.14it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.90it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  8.02it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00,  7.61it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00,  7.49it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.69it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  8.05it/s]\u001b[A\n",
            "Processing subjects:  12%| | 7/57 [00:27<03:25,  4.12s/it, college computer scie\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  51%|          | 51/100 [00:00<00:00, 505.07it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:04,  4.80it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:04,  4.94it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:04,  4.94it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:04,  4.99it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:03,  5.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:01<00:03,  5.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:01<00:03,  5.20it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:03,  5.02it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:03,  5.16it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:02,  5.23it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:02<00:02,  5.06it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:02<00:02,  4.93it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:02<00:02,  4.93it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:02<00:02,  4.98it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:02<00:01,  5.07it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:03<00:01,  4.94it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:03<00:01,  5.06it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:03<00:01,  5.00it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:03<00:01,  5.09it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:03<00:01,  4.95it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:04<00:00,  5.08it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:04<00:00,  5.06it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:04<00:00,  5.01it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:04<00:00,  4.98it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:04<00:00,  4.89it/s]\u001b[A\n",
            "Processing subjects:  14%| | 8/57 [00:32<03:38,  4.46s/it, college mathematics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  57%|         | 57/100 [00:00<00:00, 569.79it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:03,  7.66it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:03,  7.35it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  7.38it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02,  7.38it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:02,  7.26it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:02,  7.22it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:02,  7.18it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:02,  7.17it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:01<00:02,  7.01it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:01<00:02,  7.11it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01,  7.24it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01,  7.41it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01,  7.40it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01,  7.45it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:02<00:01,  7.47it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:02<00:01,  7.38it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:02<00:01,  7.37it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:02<00:00,  7.16it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:02<00:00,  7.16it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:02<00:00,  7.35it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:02<00:00,  7.51it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:03<00:00,  7.40it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:03<00:00,  7.45it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:03<00:00,  7.48it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:03<00:00,  7.50it/s]\u001b[A\n",
            "Processing subjects:  16%|    | 9/57 [00:36<03:20,  4.18s/it, college medicine]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/173 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 57/173 [00:00<00:00, 566.53it/s]\u001b[A\n",
            "Formatting batches:  66%|      | 114/173 [00:00<00:00, 562.14it/s]\u001b[A\n",
            "Formatting batches:  99%|| 171/173 [00:00<00:00, 563.01it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/44 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/44 [00:00<00:05,  8.47it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 2/44 [00:00<00:10,  4.15it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/44 [00:00<00:07,  5.51it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 4/44 [00:00<00:06,  6.36it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 5/44 [00:00<00:05,  7.21it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/44 [00:00<00:04,  7.85it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 7/44 [00:00<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 8/44 [00:01<00:04,  7.97it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 10/44 [00:01<00:03,  8.73it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 11/44 [00:01<00:03,  8.50it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 12/44 [00:01<00:03,  8.66it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 13/44 [00:01<00:05,  5.79it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 15/44 [00:02<00:04,  6.90it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 16/44 [00:02<00:03,  7.37it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 17/44 [00:02<00:05,  5.36it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 18/44 [00:02<00:04,  6.03it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 19/44 [00:02<00:03,  6.52it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 20/44 [00:02<00:03,  7.07it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 21/44 [00:03<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 22/44 [00:03<00:02,  7.81it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 23/44 [00:03<00:02,  7.86it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 24/44 [00:03<00:03,  5.35it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 25/44 [00:03<00:03,  6.09it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 27/44 [00:03<00:02,  7.36it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 28/44 [00:04<00:02,  7.79it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 29/44 [00:04<00:01,  7.91it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 30/44 [00:04<00:01,  8.21it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 31/44 [00:04<00:01,  8.45it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 32/44 [00:04<00:01,  8.61it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 33/44 [00:04<00:01,  8.48it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 34/44 [00:04<00:01,  8.75it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 35/44 [00:04<00:01,  8.95it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 36/44 [00:04<00:00,  8.98it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 38/44 [00:05<00:00,  9.22it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 39/44 [00:05<00:00,  9.19it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 40/44 [00:05<00:00,  9.16it/s]\u001b[A\n",
            "Predicting batches:  93%| | 41/44 [00:05<00:00,  5.97it/s]\u001b[A\n",
            "Predicting batches:  95%| | 42/44 [00:05<00:00,  6.60it/s]\u001b[A\n",
            "Predicting batches:  98%|| 43/44 [00:05<00:00,  7.22it/s]\u001b[A\n",
            "Processing subjects:  18%|    | 10/57 [00:42<03:46,  4.81s/it, college physics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/102 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 59/102 [00:00<00:00, 585.62it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/26 [00:00<00:02,  8.67it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/26 [00:00<00:02,  8.95it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/26 [00:00<00:02,  9.03it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/26 [00:00<00:02,  8.75it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 5/26 [00:00<00:02,  8.56it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 6/26 [00:00<00:02,  8.72it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 7/26 [00:00<00:02,  8.86it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 8/26 [00:00<00:02,  8.67it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 9/26 [00:01<00:01,  8.77it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 10/26 [00:01<00:01,  8.87it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 11/26 [00:01<00:01,  8.91it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 12/26 [00:01<00:01,  8.95it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 13/26 [00:01<00:01,  8.98it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 14/26 [00:01<00:01,  9.00it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 15/26 [00:01<00:01,  9.01it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 16/26 [00:01<00:01,  9.03it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 17/26 [00:01<00:01,  8.80it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 18/26 [00:02<00:00,  8.86it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 19/26 [00:02<00:00,  8.70it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 20/26 [00:02<00:00,  8.79it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 21/26 [00:02<00:00,  8.87it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 22/26 [00:02<00:00,  8.95it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 23/26 [00:02<00:00,  9.00it/s]\u001b[A\n",
            "Predicting batches:  92%| | 24/26 [00:02<00:00,  9.02it/s]\u001b[A\n",
            "Predicting batches:  96%| | 25/26 [00:02<00:00,  8.79it/s]\u001b[A\n",
            "Processing subjects:  19%|  | 11/57 [00:45<03:16,  4.28s/it, computer security]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 61/100 [00:00<00:00, 606.70it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:02,  9.56it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02,  9.53it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:02,  8.95it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:00<00:01, 10.20it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:00<00:01, 10.49it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:00<00:01, 10.51it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:01<00:01, 10.87it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:01<00:01, 10.87it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:01<00:00, 11.32it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:01<00:00, 10.94it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:01<00:00, 10.92it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:01<00:00, 11.09it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:02<00:00, 11.36it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:02<00:00, 11.41it/s]\u001b[A\n",
            "Processing subjects:  21%| | 12/57 [00:48<02:47,  3.73s/it, conceptual physics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/235 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  27%|               | 64/235 [00:00<00:00, 637.82it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 128/235 [00:00<00:00, 638.27it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 193/235 [00:00<00:00, 640.47it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/59 [00:00<00:03, 15.29it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/59 [00:00<00:03, 14.88it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/59 [00:00<00:03, 14.95it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/59 [00:00<00:03, 14.49it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/59 [00:00<00:03, 14.50it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/59 [00:00<00:03, 14.54it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 14/59 [00:00<00:03, 14.66it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/59 [00:01<00:02, 14.62it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 18/59 [00:01<00:02, 14.57it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 20/59 [00:01<00:02, 14.69it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/59 [00:01<00:02, 14.43it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 24/59 [00:01<00:02, 14.27it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 26/59 [00:01<00:02, 14.32it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/59 [00:01<00:02, 14.52it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 30/59 [00:02<00:02, 14.45it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 32/59 [00:02<00:01, 14.48it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 34/59 [00:02<00:01, 14.75it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 36/59 [00:02<00:01, 14.83it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 38/59 [00:02<00:01, 14.69it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 40/59 [00:02<00:01, 14.44it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 42/59 [00:02<00:01, 14.50it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 44/59 [00:03<00:01, 14.55it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 46/59 [00:03<00:00, 14.57it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 48/59 [00:03<00:00, 14.35it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 50/59 [00:03<00:00, 14.39it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 52/59 [00:03<00:00, 14.52it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 54/59 [00:03<00:00, 14.45it/s]\u001b[A\n",
            "Predicting batches:  95%| | 56/59 [00:03<00:00, 14.62it/s]\u001b[A\n",
            "Predicting batches:  98%|| 58/59 [00:03<00:00, 14.67it/s]\u001b[A\n",
            "Processing subjects:  23%|      | 13/57 [00:52<02:53,  3.93s/it, econometrics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/114 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 56/114 [00:00<00:00, 550.63it/s]\u001b[A\n",
            "Formatting batches:  98%|| 112/114 [00:00<00:00, 554.81it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/29 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/29 [00:00<00:03,  7.44it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/29 [00:00<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 3/29 [00:00<00:03,  7.01it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/29 [00:00<00:03,  6.86it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 5/29 [00:00<00:03,  6.80it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 6/29 [00:00<00:03,  6.92it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 7/29 [00:00<00:03,  7.20it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 8/29 [00:01<00:03,  6.91it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 9/29 [00:01<00:02,  6.97it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 10/29 [00:01<00:02,  6.99it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 11/29 [00:01<00:02,  6.89it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 12/29 [00:01<00:02,  6.94it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 13/29 [00:01<00:02,  6.97it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 14/29 [00:02<00:02,  6.87it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 15/29 [00:02<00:02,  6.82it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 16/29 [00:02<00:01,  6.77it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 17/29 [00:02<00:01,  6.74it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 18/29 [00:02<00:01,  6.63it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 19/29 [00:02<00:01,  6.83it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 20/29 [00:02<00:01,  6.77it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 21/29 [00:03<00:01,  6.75it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 22/29 [00:03<00:01,  6.92it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 23/29 [00:03<00:00,  6.95it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 24/29 [00:03<00:00,  6.98it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 25/29 [00:03<00:00,  6.88it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 26/29 [00:03<00:00,  7.00it/s]\u001b[A\n",
            "Predicting batches:  93%| | 27/29 [00:03<00:00,  6.80it/s]\u001b[A\n",
            "Predicting batches:  97%|| 28/29 [00:04<00:00,  6.76it/s]\u001b[A\n",
            "Processing subjects:  25%|| 14/57 [00:57<02:54,  4.06s/it, electrical engineeri\u001b[A\n",
            "Formatting batches:   0%|                               | 0/145 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  41%|            | 59/145 [00:00<00:00, 588.67it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 119/145 [00:00<00:00, 594.99it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/37 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/37 [00:00<00:03, 10.10it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/37 [00:00<00:03, 10.14it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/37 [00:00<00:03, 10.09it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 8/37 [00:00<00:02, 10.12it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 10/37 [00:00<00:02, 10.11it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/37 [00:01<00:02, 10.13it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 14/37 [00:01<00:02, 10.24it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 16/37 [00:01<00:02, 10.40it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 18/37 [00:01<00:01, 10.42it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 20/37 [00:01<00:01, 10.45it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 22/37 [00:02<00:01, 10.33it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 24/37 [00:02<00:01, 10.27it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 26/37 [00:02<00:01, 10.23it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 28/37 [00:02<00:00, 10.22it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 30/37 [00:02<00:00, 10.19it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 32/37 [00:03<00:00, 10.09it/s]\u001b[A\n",
            "Predicting batches:  92%| | 34/37 [00:03<00:00, 10.18it/s]\u001b[A\n",
            "Predicting batches:  97%|| 36/37 [00:03<00:00, 10.05it/s]\u001b[A\n",
            "Processing subjects:  26%|| 15/57 [01:00<02:47,  3.99s/it, elementary mathemati\u001b[A\n",
            "Formatting batches:   0%|                               | 0/378 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 58/378 [00:00<00:00, 579.09it/s]\u001b[A\n",
            "Formatting batches:  31%|             | 117/378 [00:00<00:00, 582.47it/s]\u001b[A\n",
            "Formatting batches:  47%|          | 176/378 [00:00<00:00, 582.88it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 235/378 [00:00<00:00, 584.69it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 294/378 [00:00<00:00, 585.01it/s]\u001b[A\n",
            "Formatting batches:  93%| | 353/378 [00:00<00:00, 577.94it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/95 [00:00<00:09,  9.47it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/95 [00:00<00:10,  8.76it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 3/95 [00:00<00:11,  8.33it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 4/95 [00:00<00:10,  8.30it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 5/95 [00:00<00:11,  8.17it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 6/95 [00:00<00:10,  8.22it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 7/95 [00:00<00:10,  8.46it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 8/95 [00:00<00:10,  8.37it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 9/95 [00:01<00:10,  8.30it/s]\u001b[A\n",
            "Predicting batches:  11%|                    | 10/95 [00:01<00:10,  8.15it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 11/95 [00:01<00:10,  8.08it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 12/95 [00:01<00:10,  8.00it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 13/95 [00:01<00:10,  8.08it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 14/95 [00:01<00:10,  7.89it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 15/95 [00:01<00:10,  7.91it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 16/95 [00:01<00:09,  7.99it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 17/95 [00:02<00:09,  8.05it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 18/95 [00:02<00:09,  8.12it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 19/95 [00:02<00:09,  7.94it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 20/95 [00:02<00:09,  8.03it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 21/95 [00:02<00:08,  8.31it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 22/95 [00:02<00:08,  8.27it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 23/95 [00:02<00:08,  8.18it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 24/95 [00:02<00:08,  8.17it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 25/95 [00:03<00:08,  8.20it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 26/95 [00:03<00:08,  8.11it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 27/95 [00:03<00:08,  8.04it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 28/95 [00:03<00:08,  8.01it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 29/95 [00:03<00:08,  8.08it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 30/95 [00:03<00:07,  8.14it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 31/95 [00:03<00:07,  8.09it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 32/95 [00:03<00:07,  8.05it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 33/95 [00:04<00:07,  7.76it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 34/95 [00:04<00:07,  7.83it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 35/95 [00:04<00:07,  7.92it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 36/95 [00:04<00:07,  8.00it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 37/95 [00:04<00:07,  8.04it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 38/95 [00:04<00:07,  8.07it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 39/95 [00:04<00:06,  8.03it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 40/95 [00:04<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 41/95 [00:05<00:06,  8.09it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 42/95 [00:05<00:06,  8.15it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 43/95 [00:05<00:06,  8.18it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 44/95 [00:05<00:06,  8.41it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 45/95 [00:05<00:06,  8.32it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 46/95 [00:05<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 47/95 [00:05<00:05,  8.08it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 48/95 [00:05<00:05,  8.14it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 49/95 [00:06<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 50/95 [00:06<00:05,  8.10it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 51/95 [00:06<00:05,  8.15it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 52/95 [00:06<00:05,  8.04it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 53/95 [00:06<00:05,  8.11it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 54/95 [00:06<00:05,  8.16it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 55/95 [00:06<00:04,  8.20it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 56/95 [00:06<00:04,  8.22it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 57/95 [00:07<00:04,  7.98it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 58/95 [00:07<00:04,  7.93it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 59/95 [00:07<00:04,  8.02it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 60/95 [00:07<00:04,  8.10it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 61/95 [00:07<00:04,  8.11it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 62/95 [00:07<00:04,  8.13it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 63/95 [00:07<00:04,  7.94it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 64/95 [00:07<00:03,  8.03it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 65/95 [00:08<00:03,  8.01it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 66/95 [00:08<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 67/95 [00:08<00:03,  7.99it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 68/95 [00:08<00:03,  8.03it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 69/95 [00:08<00:03,  8.10it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 70/95 [00:08<00:03,  8.12it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 71/95 [00:08<00:02,  8.13it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 72/95 [00:08<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 73/95 [00:09<00:02,  8.11it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 74/95 [00:09<00:02,  8.16it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 75/95 [00:09<00:02,  8.20it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 76/95 [00:09<00:02,  8.08it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 77/95 [00:09<00:02,  8.03it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 78/95 [00:09<00:02,  8.10it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 79/95 [00:09<00:01,  8.34it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 80/95 [00:09<00:01,  8.20it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 81/95 [00:10<00:01,  7.84it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 82/95 [00:10<00:01,  7.87it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 83/95 [00:10<00:01,  7.86it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 84/95 [00:10<00:01,  7.98it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 85/95 [00:10<00:01,  8.06it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 86/95 [00:10<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 87/95 [00:10<00:00,  8.03it/s]\u001b[A\n",
            "Predicting batches:  93%| | 88/95 [00:10<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  94%| | 89/95 [00:10<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  95%| | 90/95 [00:11<00:00,  8.25it/s]\u001b[A\n",
            "Predicting batches:  96%| | 91/95 [00:11<00:00,  8.23it/s]\u001b[A\n",
            "Predicting batches:  97%|| 92/95 [00:11<00:00,  8.24it/s]\u001b[A\n",
            "Predicting batches:  98%|| 93/95 [00:11<00:00,  8.15it/s]\u001b[A\n",
            "Predicting batches:  99%|| 94/95 [00:11<00:00,  8.16it/s]\u001b[A\n",
            "Processing subjects:  28%|     | 16/57 [01:13<04:26,  6.49s/it, formal logic]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  44%|           | 56/126 [00:00<00:00, 558.19it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 112/126 [00:00<00:00, 551.15it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/32 [00:00<00:04,  6.61it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/32 [00:00<00:04,  6.88it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 3/32 [00:00<00:04,  7.08it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/32 [00:00<00:03,  7.07it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 5/32 [00:00<00:03,  7.33it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 6/32 [00:00<00:03,  7.32it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 7/32 [00:00<00:03,  7.37it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 8/32 [00:01<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 9/32 [00:01<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 10/32 [00:01<00:03,  7.28it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 11/32 [00:01<00:02,  7.07it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 12/32 [00:01<00:02,  6.84it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 13/32 [00:01<00:02,  6.58it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 14/32 [00:01<00:02,  6.90it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 15/32 [00:02<00:02,  6.96it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 16/32 [00:02<00:02,  6.84it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 17/32 [00:02<00:02,  7.02it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 18/32 [00:02<00:02,  6.78it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 19/32 [00:02<00:01,  6.97it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 20/32 [00:02<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 21/32 [00:02<00:01,  6.88it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 22/32 [00:03<00:01,  6.79it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 23/32 [00:03<00:01,  6.85it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 24/32 [00:03<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 25/32 [00:03<00:01,  6.98it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 26/32 [00:03<00:00,  6.77it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 27/32 [00:03<00:00,  6.97it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 28/32 [00:04<00:00,  7.05it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 29/32 [00:04<00:00,  7.06it/s]\u001b[A\n",
            "Predicting batches:  94%| | 30/32 [00:04<00:00,  7.26it/s]\u001b[A\n",
            "Predicting batches:  97%|| 31/32 [00:04<00:00,  7.27it/s]\u001b[A\n",
            "Processing subjects:  30%|     | 17/57 [01:17<03:58,  5.96s/it, global facts]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  61%|        | 61/100 [00:00<00:00, 602.90it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:01, 11.65it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:01, 11.54it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 11.46it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 11.21it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 11.08it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01, 11.02it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01, 10.95it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 10.64it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 10.68it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 10.86it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00, 10.64it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:02<00:00, 10.86it/s]\u001b[A\n",
            "Processing subjects:  32%|| 18/57 [01:20<03:11,  4.91s/it, high school biology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/310 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  18%|                 | 57/310 [00:00<00:00, 568.94it/s]\u001b[A\n",
            "Formatting batches:  37%|            | 115/310 [00:00<00:00, 572.55it/s]\u001b[A\n",
            "Formatting batches:  56%|        | 173/310 [00:00<00:00, 574.43it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 231/310 [00:00<00:00, 574.96it/s]\u001b[A\n",
            "Formatting batches:  93%| | 289/310 [00:00<00:00, 572.80it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/78 [00:00<00:08,  9.42it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/78 [00:00<00:08,  9.28it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/78 [00:00<00:08,  8.58it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/78 [00:00<00:08,  8.74it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/78 [00:00<00:08,  8.42it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/78 [00:00<00:08,  8.38it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 7/78 [00:00<00:08,  8.34it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/78 [00:00<00:08,  8.34it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 9/78 [00:01<00:08,  8.53it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/78 [00:01<00:07,  8.66it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 11/78 [00:01<00:08,  8.36it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/78 [00:01<00:08,  8.21it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 13/78 [00:01<00:07,  8.43it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/78 [00:01<00:07,  8.59it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/78 [00:01<00:07,  8.37it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/78 [00:01<00:07,  8.34it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 17/78 [00:02<00:07,  8.53it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/78 [00:02<00:07,  8.18it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 19/78 [00:02<00:07,  8.17it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/78 [00:02<00:06,  8.40it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 21/78 [00:02<00:06,  8.31it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 22/78 [00:02<00:06,  8.28it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 23/78 [00:02<00:06,  8.47it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 24/78 [00:02<00:06,  8.60it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 25/78 [00:02<00:06,  8.69it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 26/78 [00:03<00:06,  8.27it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 27/78 [00:03<00:06,  8.11it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/78 [00:03<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 29/78 [00:03<00:06,  7.98it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 30/78 [00:03<00:05,  8.24it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 31/78 [00:03<00:05,  8.23it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 32/78 [00:03<00:05,  8.42it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 33/78 [00:03<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 34/78 [00:04<00:05,  8.29it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 35/78 [00:04<00:05,  8.23it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 36/78 [00:04<00:04,  8.44it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 37/78 [00:04<00:05,  8.05it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/78 [00:04<00:04,  8.08it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 39/78 [00:04<00:04,  8.32it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 40/78 [00:04<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 41/78 [00:04<00:04,  8.15it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 42/78 [00:05<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 43/78 [00:05<00:04,  8.19it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 44/78 [00:05<00:04,  8.21it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 45/78 [00:05<00:04,  8.19it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 46/78 [00:05<00:04,  7.95it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 47/78 [00:05<00:03,  8.00it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/78 [00:05<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 49/78 [00:05<00:03,  8.31it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 50/78 [00:06<00:03,  8.52it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 51/78 [00:06<00:03,  8.30it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 52/78 [00:06<00:03,  8.18it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 53/78 [00:06<00:03,  8.06it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 54/78 [00:06<00:03,  7.98it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 55/78 [00:06<00:02,  8.05it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 56/78 [00:06<00:02,  8.11it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 57/78 [00:06<00:02,  8.15it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 58/78 [00:06<00:02,  8.37it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 59/78 [00:07<00:02,  8.33it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 60/78 [00:07<00:02,  8.50it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 61/78 [00:07<00:02,  8.36it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 62/78 [00:07<00:01,  8.20it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 63/78 [00:07<00:01,  8.10it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 64/78 [00:07<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 65/78 [00:07<00:01,  8.00it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 66/78 [00:07<00:01,  8.26it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 67/78 [00:08<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 68/78 [00:08<00:01,  8.05it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 69/78 [00:08<00:01,  8.10it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 70/78 [00:08<00:01,  7.83it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 71/78 [00:08<00:00,  8.12it/s]\u001b[A\n",
            "Predicting batches:  92%| | 72/78 [00:08<00:00,  8.34it/s]\u001b[A\n",
            "Predicting batches:  94%| | 73/78 [00:08<00:00,  8.29it/s]\u001b[A\n",
            "Predicting batches:  95%| | 74/78 [00:08<00:00,  8.22it/s]\u001b[A\n",
            "Predicting batches:  96%| | 75/78 [00:09<00:00,  8.53it/s]\u001b[A\n",
            "Predicting batches:  97%|| 76/78 [00:09<00:00,  8.44it/s]\u001b[A\n",
            "Predicting batches:  99%|| 77/78 [00:09<00:00,  8.36it/s]\u001b[A\n",
            "Processing subjects:  33%|| 19/57 [01:30<04:03,  6.41s/it, high school chemistr\u001b[A\n",
            "Formatting batches:   0%|                               | 0/203 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  29%|               | 59/203 [00:00<00:00, 588.09it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 118/203 [00:00<00:00, 588.79it/s]\u001b[A\n",
            "Formatting batches:  88%|  | 178/203 [00:00<00:00, 589.65it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/51 [00:00<00:05,  9.34it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:00<00:05,  9.23it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/51 [00:00<00:05,  9.14it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:00<00:05,  8.70it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:00<00:05,  8.92it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:00<00:04,  9.06it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:00<00:04,  9.33it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:00<00:04,  9.21it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:01<00:04,  8.74it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:01<00:04,  8.80it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:01<00:04,  8.96it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:01<00:04,  8.95it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:01<00:04,  8.96it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:01<00:04,  9.08it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:01<00:04,  8.67it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:01<00:04,  8.53it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:01<00:03,  8.77it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:02<00:03,  8.44it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:02<00:03,  8.36it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:02<00:03,  8.21it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:02<00:03,  7.97it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:02<00:03,  7.94it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:02<00:03,  8.03it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:02<00:03,  8.05it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:02<00:03,  8.39it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:03<00:03,  8.33it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:03<00:02,  8.62it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:03<00:02,  8.37it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:03<00:02,  8.17it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:03<00:02,  8.39it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:03<00:02,  8.64it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:03<00:02,  8.84it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:03<00:02,  8.47it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:03<00:02,  8.24it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:04<00:01,  8.54it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:04<00:01,  8.44it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:04<00:01,  8.22it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 38/51 [00:04<00:01,  8.17it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:04<00:01,  8.07it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:04<00:01,  8.01it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:04<00:01,  8.28it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:04<00:01,  8.13it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:05<00:00,  8.34it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:05<00:00,  8.26it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:05<00:00,  8.45it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:05<00:00,  8.60it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:05<00:00,  8.72it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:05<00:00,  8.77it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:05<00:00,  8.80it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:05<00:00,  8.61it/s]\u001b[A\n",
            "Processing subjects:  35%|| 20/57 [01:36<03:55,  6.38s/it, high school computer\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  47%|           | 47/100 [00:00<00:00, 464.33it/s]\u001b[A\n",
            "Formatting batches:  97%|| 97/100 [00:00<00:00, 483.44it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/25 [00:00<00:05,  4.44it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:05,  4.55it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 3/25 [00:00<00:04,  4.73it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:04,  4.55it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 5/25 [00:01<00:04,  4.58it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:01<00:04,  4.66it/s]\u001b[A\n",
            "Predicting batches:  28%|                 | 7/25 [00:01<00:03,  4.70it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:01<00:03,  4.28it/s]\u001b[A\n",
            "Predicting batches:  36%|               | 9/25 [00:02<00:03,  4.34it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:02<00:03,  4.51it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 11/25 [00:02<00:03,  4.54it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:02<00:02,  4.72it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 13/25 [00:02<00:02,  4.76it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:03<00:02,  4.65it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 15/25 [00:03<00:02,  4.51it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:03<00:01,  4.65it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 17/25 [00:03<00:01,  4.54it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:03<00:01,  4.54it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 19/25 [00:04<00:01,  4.50it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:04<00:01,  4.52it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 21/25 [00:04<00:00,  4.49it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:04<00:00,  4.69it/s]\u001b[A\n",
            "Predicting batches:  92%| | 23/25 [00:05<00:00,  4.73it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:05<00:00,  4.79it/s]\u001b[A\n",
            "Predicting batches: 100%|| 25/25 [00:05<00:00,  4.81it/s]\u001b[A\n",
            "Processing subjects:  37%|| 21/57 [01:42<03:41,  6.16s/it, high school european\u001b[A\n",
            "Formatting batches:   0%|                               | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 28/165 [00:00<00:00, 277.19it/s]\u001b[A\n",
            "Formatting batches:  34%|             | 56/165 [00:00<00:00, 278.48it/s]\u001b[A\n",
            "Formatting batches:  51%|          | 84/165 [00:00<00:00, 277.08it/s]\u001b[A\n",
            "Formatting batches:  68%|      | 113/165 [00:00<00:00, 279.34it/s]\u001b[A\n",
            "Formatting batches:  86%|  | 142/165 [00:00<00:00, 280.41it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/42 [00:00<00:34,  1.18it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/42 [00:01<00:34,  1.17it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/42 [00:02<00:32,  1.20it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/42 [00:03<00:31,  1.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 5/42 [00:04<00:31,  1.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/42 [00:05<00:30,  1.20it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 7/42 [00:05<00:28,  1.22it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/42 [00:06<00:28,  1.19it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 9/42 [00:07<00:30,  1.07it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/42 [00:08<00:29,  1.10it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 11/42 [00:09<00:27,  1.14it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/42 [00:10<00:26,  1.15it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 13/42 [00:11<00:24,  1.18it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/42 [00:11<00:23,  1.21it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 15/42 [00:12<00:22,  1.21it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 16/42 [00:13<00:21,  1.23it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 17/42 [00:14<00:20,  1.21it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 18/42 [00:15<00:20,  1.19it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 19/42 [00:16<00:19,  1.18it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 20/42 [00:16<00:18,  1.19it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 21/42 [00:17<00:17,  1.19it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 22/42 [00:18<00:16,  1.21it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 23/42 [00:19<00:15,  1.23it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 24/42 [00:20<00:14,  1.20it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 25/42 [00:21<00:14,  1.19it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 26/42 [00:21<00:13,  1.19it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 27/42 [00:22<00:12,  1.17it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 28/42 [00:23<00:11,  1.17it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 29/42 [00:24<00:10,  1.18it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 30/42 [00:25<00:10,  1.20it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 31/42 [00:26<00:09,  1.20it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 32/42 [00:27<00:08,  1.19it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 33/42 [00:27<00:07,  1.21it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 34/42 [00:28<00:06,  1.21it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 35/42 [00:29<00:05,  1.18it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 36/42 [00:30<00:05,  1.17it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 37/42 [00:31<00:04,  1.19it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 38/42 [00:32<00:03,  1.19it/s]\u001b[A\n",
            "Predicting batches:  93%| | 39/42 [00:32<00:02,  1.20it/s]\u001b[A\n",
            "Predicting batches:  95%| | 40/42 [00:33<00:01,  1.20it/s]\u001b[A\n",
            "Predicting batches:  98%|| 41/42 [00:34<00:00,  1.23it/s]\u001b[A\n",
            "Predicting batches: 100%|| 42/42 [00:34<00:00,  1.62it/s]\u001b[A\n",
            "Processing subjects:  39%|| 22/57 [02:17<08:40, 14.88s/it, high school geograph\u001b[A\n",
            "Formatting batches:   0%|                               | 0/198 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  28%|               | 56/198 [00:00<00:00, 551.65it/s]\u001b[A\n",
            "Formatting batches:  59%|        | 116/198 [00:00<00:00, 575.44it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 176/198 [00:00<00:00, 585.52it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/50 [00:00<00:03, 12.67it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/50 [00:00<00:03, 12.29it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/50 [00:00<00:03, 12.05it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/50 [00:00<00:03, 12.08it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/50 [00:00<00:03, 11.76it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/50 [00:01<00:03, 11.83it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 14/50 [00:01<00:03, 11.91it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 16/50 [00:01<00:02, 11.89it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 18/50 [00:01<00:02, 11.97it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 20/50 [00:01<00:02, 11.93it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 22/50 [00:01<00:02, 11.90it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 24/50 [00:02<00:02, 11.53it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 26/50 [00:02<00:02, 11.62it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 28/50 [00:02<00:01, 11.70it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 30/50 [00:02<00:01, 11.77it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 32/50 [00:02<00:01, 11.78it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 34/50 [00:02<00:01, 11.80it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 36/50 [00:03<00:01, 11.89it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 38/50 [00:03<00:01, 11.97it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 40/50 [00:03<00:00, 11.75it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 42/50 [00:03<00:00, 11.75it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 44/50 [00:03<00:00, 11.87it/s]\u001b[A\n",
            "Predicting batches:  92%| | 46/50 [00:03<00:00, 11.83it/s]\u001b[A\n",
            "Predicting batches:  96%| | 48/50 [00:04<00:00, 11.63it/s]\u001b[A\n",
            "Predicting batches: 100%|| 50/50 [00:04<00:00, 12.37it/s]\u001b[A\n",
            "Processing subjects:  40%|| 23/57 [02:21<06:40, 11.78s/it, high school governme\u001b[A\n",
            "Formatting batches:   0%|                               | 0/193 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 58/193 [00:00<00:00, 572.10it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 116/193 [00:00<00:00, 567.76it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 174/193 [00:00<00:00, 571.67it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/49 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/49 [00:00<00:05,  9.47it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/49 [00:00<00:04,  9.47it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/49 [00:00<00:04,  9.41it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/49 [00:00<00:04,  9.37it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/49 [00:00<00:04,  9.34it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/49 [00:00<00:04,  9.33it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/49 [00:00<00:04,  9.48it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/49 [00:00<00:04,  9.42it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/49 [00:00<00:04,  9.37it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/49 [00:01<00:04,  9.21it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 11/49 [00:01<00:04,  9.10it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/49 [00:01<00:04,  9.14it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 13/49 [00:01<00:03,  9.18it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 14/49 [00:01<00:03,  9.20it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 15/49 [00:01<00:03,  9.11it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 16/49 [00:01<00:03,  9.16it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 17/49 [00:01<00:03,  9.37it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 18/49 [00:01<00:03,  9.35it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 19/49 [00:02<00:03,  9.33it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 20/49 [00:02<00:03,  9.32it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 21/49 [00:02<00:03,  9.31it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 22/49 [00:02<00:02,  9.29it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 23/49 [00:02<00:02,  9.27it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 24/49 [00:02<00:02,  9.27it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 25/49 [00:02<00:02,  9.28it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 26/49 [00:02<00:02,  9.18it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 27/49 [00:02<00:02,  9.21it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 28/49 [00:03<00:02,  9.24it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 29/49 [00:03<00:02,  9.26it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 30/49 [00:03<00:02,  9.27it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 31/49 [00:03<00:01,  9.27it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 32/49 [00:03<00:01,  9.27it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 33/49 [00:03<00:01,  9.44it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 34/49 [00:03<00:01,  9.39it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 35/49 [00:03<00:01,  9.34it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 36/49 [00:03<00:01,  9.32it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 37/49 [00:03<00:01,  9.32it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 38/49 [00:04<00:01,  9.32it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 39/49 [00:04<00:01,  9.31it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 40/49 [00:04<00:01,  8.94it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 41/49 [00:04<00:00,  8.94it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 42/49 [00:04<00:00,  8.66it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 43/49 [00:04<00:00,  8.84it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 44/49 [00:04<00:00,  8.97it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 45/49 [00:04<00:00,  9.07it/s]\u001b[A\n",
            "Predicting batches:  94%| | 46/49 [00:04<00:00,  9.14it/s]\u001b[A\n",
            "Predicting batches:  96%| | 47/49 [00:05<00:00,  9.08it/s]\u001b[A\n",
            "Predicting batches:  98%|| 48/49 [00:05<00:00,  9.05it/s]\u001b[A\n",
            "Processing subjects:  42%|| 24/57 [02:27<05:27,  9.92s/it, high school macroeco\u001b[A\n",
            "Formatting batches:   0%|                               | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 60/390 [00:00<00:00, 594.76it/s]\u001b[A\n",
            "Formatting batches:  31%|             | 120/390 [00:00<00:00, 596.56it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 180/390 [00:00<00:00, 592.10it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 241/390 [00:00<00:00, 598.00it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 302/390 [00:00<00:00, 601.57it/s]\u001b[A\n",
            "Formatting batches:  93%| | 363/390 [00:00<00:00, 602.33it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/98 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/98 [00:00<00:08, 11.98it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 4/98 [00:00<00:07, 12.13it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 6/98 [00:00<00:08, 11.50it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 8/98 [00:00<00:07, 11.65it/s]\u001b[A\n",
            "Predicting batches:  10%|                    | 10/98 [00:00<00:07, 11.71it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 12/98 [00:01<00:07, 11.76it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 14/98 [00:01<00:07, 11.44it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 16/98 [00:01<00:07, 11.58it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 18/98 [00:01<00:06, 11.69it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 20/98 [00:01<00:06, 11.59it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 22/98 [00:01<00:06, 11.64it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 24/98 [00:02<00:06, 11.70it/s]\u001b[A\n",
            "Predicting batches:  27%|                 | 26/98 [00:02<00:06, 11.41it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 28/98 [00:02<00:06, 11.65it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 30/98 [00:02<00:05, 11.79it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 32/98 [00:02<00:05, 11.72it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 34/98 [00:02<00:05, 11.43it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 36/98 [00:03<00:05, 11.51it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 38/98 [00:03<00:05, 11.27it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 40/98 [00:03<00:05, 11.23it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 42/98 [00:03<00:04, 11.39it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 44/98 [00:03<00:04, 11.71it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 46/98 [00:03<00:04, 11.55it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 48/98 [00:04<00:04, 11.44it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 50/98 [00:04<00:04, 11.56it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 52/98 [00:04<00:04, 11.43it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 54/98 [00:04<00:03, 11.53it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 56/98 [00:04<00:03, 11.61it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 58/98 [00:05<00:03, 11.76it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 60/98 [00:05<00:03, 11.78it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 62/98 [00:05<00:03, 11.60it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 64/98 [00:05<00:02, 11.69it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 66/98 [00:05<00:02, 11.65it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 68/98 [00:05<00:02, 11.71it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 70/98 [00:06<00:02, 11.44it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 72/98 [00:06<00:02, 11.38it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 74/98 [00:06<00:02, 11.58it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 76/98 [00:06<00:01, 11.84it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 78/98 [00:06<00:01, 11.84it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 80/98 [00:06<00:01, 11.82it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 82/98 [00:07<00:01, 11.64it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 84/98 [00:07<00:01, 11.67it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 86/98 [00:07<00:01, 11.39it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 88/98 [00:07<00:00, 11.43it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 90/98 [00:07<00:00, 11.53it/s]\u001b[A\n",
            "Predicting batches:  94%| | 92/98 [00:07<00:00, 11.64it/s]\u001b[A\n",
            "Predicting batches:  96%| | 94/98 [00:08<00:00, 11.68it/s]\u001b[A\n",
            "Predicting batches:  98%|| 96/98 [00:08<00:00, 11.69it/s]\u001b[A\n",
            "Predicting batches: 100%|| 98/98 [00:08<00:00, 12.63it/s]\u001b[A\n",
            "Processing subjects:  44%|| 25/57 [02:36<05:09,  9.66s/it, high school mathemat\u001b[A\n",
            "Formatting batches:   0%|                               | 0/270 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  21%|                | 57/270 [00:00<00:00, 569.43it/s]\u001b[A\n",
            "Formatting batches:  42%|           | 114/270 [00:00<00:00, 567.45it/s]\u001b[A\n",
            "Formatting batches:  64%|       | 172/270 [00:00<00:00, 572.38it/s]\u001b[A\n",
            "Formatting batches:  85%|   | 230/270 [00:00<00:00, 575.27it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/68 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/68 [00:00<00:08,  8.34it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/68 [00:00<00:08,  7.66it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/68 [00:00<00:08,  7.89it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/68 [00:00<00:08,  7.87it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/68 [00:00<00:07,  7.97it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 6/68 [00:00<00:07,  8.26it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/68 [00:00<00:07,  8.19it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/68 [00:00<00:07,  8.18it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 9/68 [00:01<00:07,  8.13it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/68 [00:01<00:07,  8.12it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 11/68 [00:01<00:07,  7.89it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/68 [00:01<00:07,  7.87it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 13/68 [00:01<00:07,  7.73it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/68 [00:01<00:06,  7.73it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 15/68 [00:01<00:06,  8.03it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/68 [00:02<00:06,  7.92it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 17/68 [00:02<00:06,  7.95it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 18/68 [00:02<00:06,  7.98it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 19/68 [00:02<00:06,  8.02it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 20/68 [00:02<00:06,  7.92it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 21/68 [00:02<00:05,  8.17it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 22/68 [00:02<00:05,  8.05it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 23/68 [00:02<00:05,  7.99it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 24/68 [00:03<00:05,  7.93it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 25/68 [00:03<00:05,  8.18it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 26/68 [00:03<00:05,  8.36it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 27/68 [00:03<00:04,  8.28it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 28/68 [00:03<00:04,  8.21it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 29/68 [00:03<00:04,  8.19it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 30/68 [00:03<00:04,  8.08it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 31/68 [00:03<00:04,  8.08it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 32/68 [00:03<00:04,  7.99it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 33/68 [00:04<00:04,  8.04it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 34/68 [00:04<00:04,  7.93it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 35/68 [00:04<00:04,  7.96it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 36/68 [00:04<00:04,  7.65it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 37/68 [00:04<00:03,  7.79it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 38/68 [00:04<00:03,  7.90it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 39/68 [00:04<00:03,  8.18it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 40/68 [00:04<00:03,  8.13it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 41/68 [00:05<00:03,  8.33it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 42/68 [00:05<00:03,  8.13it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 43/68 [00:05<00:03,  8.13it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 44/68 [00:05<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 45/68 [00:05<00:02,  8.03it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 46/68 [00:05<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 47/68 [00:05<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 48/68 [00:05<00:02,  8.04it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 49/68 [00:06<00:02,  7.96it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 50/68 [00:06<00:02,  8.01it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 51/68 [00:06<00:02,  8.06it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 52/68 [00:06<00:01,  8.04it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 53/68 [00:06<00:01,  7.96it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 54/68 [00:06<00:01,  8.00it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 55/68 [00:06<00:01,  8.02it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 56/68 [00:06<00:01,  7.94it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 57/68 [00:07<00:01,  7.89it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 58/68 [00:07<00:01,  7.96it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 59/68 [00:07<00:01,  7.88it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 60/68 [00:07<00:01,  7.85it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 61/68 [00:07<00:00,  7.93it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 62/68 [00:07<00:00,  7.76it/s]\u001b[A\n",
            "Predicting batches:  93%| | 63/68 [00:07<00:00,  7.87it/s]\u001b[A\n",
            "Predicting batches:  94%| | 64/68 [00:08<00:00,  7.84it/s]\u001b[A\n",
            "Predicting batches:  96%| | 65/68 [00:08<00:00,  7.93it/s]\u001b[A\n",
            "Predicting batches:  97%|| 66/68 [00:08<00:00,  8.00it/s]\u001b[A\n",
            "Predicting batches:  99%|| 67/68 [00:08<00:00,  7.67it/s]\u001b[A\n",
            "Processing subjects:  46%|| 26/57 [02:45<04:52,  9.44s/it, high school microeco\u001b[A\n",
            "Formatting batches:   0%|                               | 0/238 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  25%|               | 59/238 [00:00<00:00, 585.59it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 119/238 [00:00<00:00, 589.75it/s]\u001b[A\n",
            "Formatting batches:  75%|     | 179/238 [00:00<00:00, 591.39it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/60 [00:00<00:05,  9.89it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/60 [00:00<00:05, 10.54it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/60 [00:00<00:05, 10.23it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 7/60 [00:00<00:05, 10.46it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/60 [00:00<00:04, 10.26it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 11/60 [00:01<00:04, 10.45it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 13/60 [00:01<00:04, 10.70it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 15/60 [00:01<00:04, 10.46it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 17/60 [00:01<00:03, 10.80it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 19/60 [00:01<00:03, 10.92it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 21/60 [00:01<00:03, 10.71it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 23/60 [00:02<00:03, 10.98it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 25/60 [00:02<00:03, 10.90it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 27/60 [00:02<00:03, 10.98it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 29/60 [00:02<00:02, 10.90it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 31/60 [00:02<00:02, 10.98it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 33/60 [00:03<00:02, 10.77it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 35/60 [00:03<00:02, 10.75it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 37/60 [00:03<00:02, 10.59it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 39/60 [00:03<00:01, 10.63it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 41/60 [00:03<00:01, 10.63it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 43/60 [00:04<00:01, 10.81it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 45/60 [00:04<00:01, 10.81it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 47/60 [00:04<00:01, 10.56it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 49/60 [00:04<00:01, 10.62it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 51/60 [00:04<00:00, 10.67it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 53/60 [00:04<00:00, 10.81it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 55/60 [00:05<00:00, 10.91it/s]\u001b[A\n",
            "Predicting batches:  95%| | 57/60 [00:05<00:00, 10.59it/s]\u001b[A\n",
            "Predicting batches:  98%|| 59/60 [00:05<00:00, 10.38it/s]\u001b[A\n",
            "Processing subjects:  47%|| 27/57 [02:51<04:12,  8.41s/it, high school physics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/151 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  36%|             | 55/151 [00:00<00:00, 545.17it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 112/151 [00:00<00:00, 558.50it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/38 [00:00<00:04,  7.50it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/38 [00:00<00:05,  6.72it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 3/38 [00:00<00:04,  7.02it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 4/38 [00:00<00:04,  7.02it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 5/38 [00:00<00:04,  7.16it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 6/38 [00:00<00:04,  7.37it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 7/38 [00:00<00:04,  7.37it/s]\u001b[A\n",
            "Predicting batches:  21%|                   | 8/38 [00:01<00:04,  7.38it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 9/38 [00:01<00:03,  7.50it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 10/38 [00:01<00:03,  7.56it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 11/38 [00:01<00:03,  7.49it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 12/38 [00:01<00:03,  7.65it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 13/38 [00:01<00:03,  7.16it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 14/38 [00:01<00:03,  7.41it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 15/38 [00:02<00:03,  7.39it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 16/38 [00:02<00:02,  7.56it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 17/38 [00:02<00:02,  7.59it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 18/38 [00:02<00:02,  7.40it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 19/38 [00:02<00:02,  7.51it/s]\u001b[A\n",
            "Predicting batches:  53%|           | 20/38 [00:02<00:02,  7.35it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 21/38 [00:02<00:02,  7.49it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 22/38 [00:02<00:02,  7.57it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 23/38 [00:03<00:01,  7.60it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 24/38 [00:03<00:01,  7.64it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 25/38 [00:03<00:01,  7.65it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 26/38 [00:03<00:01,  7.56it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 27/38 [00:03<00:01,  7.60it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 28/38 [00:03<00:01,  7.53it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 29/38 [00:03<00:01,  7.35it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 30/38 [00:04<00:01,  7.08it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 31/38 [00:04<00:00,  7.33it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 32/38 [00:04<00:00,  7.18it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 33/38 [00:04<00:00,  7.35it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 34/38 [00:04<00:00,  7.57it/s]\u001b[A\n",
            "Predicting batches:  92%| | 35/38 [00:04<00:00,  7.63it/s]\u001b[A\n",
            "Predicting batches:  95%| | 36/38 [00:04<00:00,  7.55it/s]\u001b[A\n",
            "Predicting batches:  97%|| 37/38 [00:04<00:00,  7.59it/s]\u001b[A\n",
            "Predicting batches: 100%|| 38/38 [00:05<00:00,  8.12it/s]\u001b[A\n",
            "Processing subjects:  49%|| 28/57 [02:56<03:37,  7.49s/it, high school psycholo\u001b[A\n",
            "Formatting batches:   0%|                               | 0/545 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  10%|                   | 55/545 [00:00<00:00, 543.27it/s]\u001b[A\n",
            "Formatting batches:  20%|                | 111/545 [00:00<00:00, 547.76it/s]\u001b[A\n",
            "Formatting batches:  31%|             | 168/545 [00:00<00:00, 554.83it/s]\u001b[A\n",
            "Formatting batches:  41%|           | 225/545 [00:00<00:00, 556.70it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 281/545 [00:00<00:00, 556.62it/s]\u001b[A\n",
            "Formatting batches:  62%|       | 338/545 [00:00<00:00, 559.66it/s]\u001b[A\n",
            "Formatting batches:  72%|     | 395/545 [00:00<00:00, 560.02it/s]\u001b[A\n",
            "Formatting batches:  83%|   | 452/545 [00:00<00:00, 557.65it/s]\u001b[A\n",
            "Formatting batches:  93%| | 509/545 [00:00<00:00, 558.86it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/137 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 1/137 [00:00<00:13,  9.79it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/137 [00:00<00:15,  8.79it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 3/137 [00:00<00:15,  8.90it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 4/137 [00:00<00:14,  8.90it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 5/137 [00:00<00:14,  8.90it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 6/137 [00:00<00:14,  8.93it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 7/137 [00:00<00:14,  9.04it/s]\u001b[A\n",
            "Predicting batches:   6%|                     | 8/137 [00:00<00:15,  8.58it/s]\u001b[A\n",
            "Predicting batches:   7%|                     | 9/137 [00:01<00:15,  8.44it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 10/137 [00:01<00:16,  7.75it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 11/137 [00:01<00:15,  8.15it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 12/137 [00:01<00:14,  8.35it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 13/137 [00:01<00:14,  8.28it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 14/137 [00:01<00:14,  8.47it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 15/137 [00:01<00:15,  7.95it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 16/137 [00:01<00:14,  8.19it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 17/137 [00:02<00:14,  8.48it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 18/137 [00:02<00:13,  8.57it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 19/137 [00:02<00:13,  8.77it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 20/137 [00:02<00:13,  8.54it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 21/137 [00:02<00:13,  8.64it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 22/137 [00:02<00:13,  8.68it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 23/137 [00:02<00:13,  8.74it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 24/137 [00:02<00:12,  8.78it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 25/137 [00:02<00:12,  8.92it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 26/137 [00:03<00:12,  8.90it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 27/137 [00:03<00:12,  9.00it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 28/137 [00:03<00:12,  8.66it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 29/137 [00:03<00:12,  8.82it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 30/137 [00:03<00:12,  8.82it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 31/137 [00:03<00:12,  8.49it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 32/137 [00:03<00:12,  8.60it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 33/137 [00:03<00:12,  8.41it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 34/137 [00:03<00:12,  8.52it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 35/137 [00:04<00:11,  8.60it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 36/137 [00:04<00:11,  8.69it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 37/137 [00:04<00:11,  8.72it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 38/137 [00:04<00:11,  8.87it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 39/137 [00:04<00:10,  8.98it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 40/137 [00:04<00:10,  8.93it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 41/137 [00:04<00:10,  9.03it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 42/137 [00:04<00:11,  8.62it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 43/137 [00:04<00:10,  8.71it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 44/137 [00:05<00:10,  8.75it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 45/137 [00:05<00:10,  8.78it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 46/137 [00:05<00:10,  8.90it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 47/137 [00:05<00:10,  8.66it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 48/137 [00:05<00:10,  8.70it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 49/137 [00:05<00:10,  8.74it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 50/137 [00:05<00:09,  8.89it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 51/137 [00:05<00:09,  8.99it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 52/137 [00:06<00:09,  8.69it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 53/137 [00:06<00:09,  8.47it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 54/137 [00:06<00:09,  8.58it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 55/137 [00:06<00:09,  8.44it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 56/137 [00:06<00:09,  8.55it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 57/137 [00:06<00:09,  8.74it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 58/137 [00:06<00:09,  8.76it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 59/137 [00:06<00:08,  8.81it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 60/137 [00:06<00:09,  8.25it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 61/137 [00:07<00:08,  8.53it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 62/137 [00:07<00:09,  7.99it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 63/137 [00:07<00:09,  8.21it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 64/137 [00:07<00:08,  8.41it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 65/137 [00:07<00:08,  8.55it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 66/137 [00:07<00:08,  8.65it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 67/137 [00:07<00:08,  8.70it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 68/137 [00:07<00:08,  8.49it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 69/137 [00:07<00:07,  8.69it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 70/137 [00:08<00:07,  8.75it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 71/137 [00:08<00:07,  8.52it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 72/137 [00:08<00:07,  8.63it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 73/137 [00:08<00:07,  8.79it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 74/137 [00:08<00:07,  8.57it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 75/137 [00:08<00:07,  8.41it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 76/137 [00:08<00:07,  8.53it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 77/137 [00:08<00:06,  8.72it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 78/137 [00:09<00:06,  8.87it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 79/137 [00:09<00:06,  8.96it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 80/137 [00:09<00:06,  8.68it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 81/137 [00:09<00:06,  8.85it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 82/137 [00:09<00:06,  8.84it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 83/137 [00:09<00:06,  8.35it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 84/137 [00:09<00:06,  8.61it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 85/137 [00:09<00:06,  8.43it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 86/137 [00:09<00:06,  8.33it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 87/137 [00:10<00:05,  8.58it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 88/137 [00:10<00:05,  8.44it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 89/137 [00:10<00:05,  8.34it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 90/137 [00:10<00:05,  8.51it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 91/137 [00:10<00:05,  8.40it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 92/137 [00:10<00:05,  8.19it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 93/137 [00:10<00:05,  8.39it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 94/137 [00:10<00:05,  8.53it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 95/137 [00:11<00:04,  8.73it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 96/137 [00:11<00:04,  8.75it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 97/137 [00:11<00:04,  8.22it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 98/137 [00:11<00:04,  8.42it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 99/137 [00:11<00:04,  8.57it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 100/137 [00:11<00:04,  8.39it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 101/137 [00:11<00:04,  8.63it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 102/137 [00:11<00:03,  8.80it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 103/137 [00:11<00:03,  8.59it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 104/137 [00:12<00:03,  8.77it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 105/137 [00:12<00:03,  8.23it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 106/137 [00:12<00:03,  8.43it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 107/137 [00:12<00:03,  8.29it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 108/137 [00:12<00:03,  8.12it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 109/137 [00:12<00:03,  8.43it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 110/137 [00:12<00:03,  8.53it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 111/137 [00:12<00:02,  8.71it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 112/137 [00:13<00:02,  8.85it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 113/137 [00:13<00:02,  8.97it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 114/137 [00:13<00:02,  8.94it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 115/137 [00:13<00:02,  8.90it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 116/137 [00:13<00:02,  8.65it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 117/137 [00:13<00:02,  8.49it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 118/137 [00:13<00:02,  8.69it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 119/137 [00:13<00:02,  8.52it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 120/137 [00:13<00:01,  8.64it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 121/137 [00:14<00:01,  8.79it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 122/137 [00:14<00:01,  8.45it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 123/137 [00:14<00:01,  8.57it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 124/137 [00:14<00:01,  8.64it/s]\u001b[A\n",
            "Predicting batches:  91%| | 125/137 [00:14<00:01,  8.80it/s]\u001b[A\n",
            "Predicting batches:  92%| | 126/137 [00:14<00:01,  8.15it/s]\u001b[A\n",
            "Predicting batches:  93%| | 127/137 [00:14<00:01,  8.34it/s]\u001b[A\n",
            "Predicting batches:  93%| | 128/137 [00:14<00:01,  8.49it/s]\u001b[A\n",
            "Predicting batches:  94%| | 129/137 [00:15<00:00,  8.38it/s]\u001b[A\n",
            "Predicting batches:  95%| | 130/137 [00:15<00:00,  8.18it/s]\u001b[A\n",
            "Predicting batches:  96%| | 131/137 [00:15<00:00,  8.39it/s]\u001b[A\n",
            "Predicting batches:  96%|| 132/137 [00:15<00:00,  8.28it/s]\u001b[A\n",
            "Predicting batches:  97%|| 133/137 [00:15<00:00,  8.42it/s]\u001b[A\n",
            "Predicting batches:  98%|| 134/137 [00:15<00:00,  8.52it/s]\u001b[A\n",
            "Predicting batches:  99%|| 135/137 [00:15<00:00,  8.59it/s]\u001b[A\n",
            "Predicting batches:  99%|| 136/137 [00:15<00:00,  8.33it/s]\u001b[A\n",
            "Processing subjects:  51%|| 29/57 [03:13<04:48, 10.31s/it, high school statisti\u001b[A\n",
            "Formatting batches:   0%|                               | 0/216 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  23%|                | 49/216 [00:00<00:00, 489.21it/s]\u001b[A\n",
            "Formatting batches:  46%|          | 100/216 [00:00<00:00, 498.15it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 151/216 [00:00<00:00, 500.45it/s]\u001b[A\n",
            "Formatting batches:  94%| | 202/216 [00:00<00:00, 500.24it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/54 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/54 [00:00<00:09,  5.45it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/54 [00:00<00:10,  5.08it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/54 [00:00<00:09,  5.25it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/54 [00:00<00:09,  5.43it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 5/54 [00:00<00:09,  5.40it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 6/54 [00:01<00:08,  5.41it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 7/54 [00:01<00:08,  5.36it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 8/54 [00:01<00:08,  5.17it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 9/54 [00:01<00:08,  5.12it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 10/54 [00:01<00:08,  5.00it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 11/54 [00:02<00:08,  5.10it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 12/54 [00:02<00:08,  5.06it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 13/54 [00:02<00:08,  4.97it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 14/54 [00:02<00:07,  5.07it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 15/54 [00:02<00:07,  5.17it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 16/54 [00:03<00:07,  5.12it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 17/54 [00:03<00:07,  5.18it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 18/54 [00:03<00:06,  5.20it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 19/54 [00:03<00:06,  5.26it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 20/54 [00:03<00:06,  5.31it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 21/54 [00:04<00:06,  5.27it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 22/54 [00:04<00:06,  5.11it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 23/54 [00:04<00:06,  5.07it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 24/54 [00:04<00:05,  5.04it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 25/54 [00:04<00:05,  5.08it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 26/54 [00:05<00:05,  5.18it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 27/54 [00:05<00:05,  5.22it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 28/54 [00:05<00:04,  5.26it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 29/54 [00:05<00:04,  5.24it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 30/54 [00:05<00:04,  5.24it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 31/54 [00:05<00:04,  5.16it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 32/54 [00:06<00:04,  5.21it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 33/54 [00:06<00:04,  5.14it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 34/54 [00:06<00:03,  5.17it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 35/54 [00:06<00:03,  4.80it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 36/54 [00:07<00:03,  4.91it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 37/54 [00:07<00:03,  4.90it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 38/54 [00:07<00:03,  4.90it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 39/54 [00:07<00:03,  4.85it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 40/54 [00:07<00:02,  5.09it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 41/54 [00:07<00:02,  5.13it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 42/54 [00:08<00:02,  5.20it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 43/54 [00:08<00:02,  5.11it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 44/54 [00:08<00:01,  5.29it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 45/54 [00:08<00:01,  5.25it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 46/54 [00:08<00:01,  5.14it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 47/54 [00:09<00:01,  5.16it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 48/54 [00:09<00:01,  5.17it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 49/54 [00:09<00:00,  5.23it/s]\u001b[A\n",
            "Predicting batches:  93%| | 50/54 [00:09<00:00,  5.38it/s]\u001b[A\n",
            "Predicting batches:  94%| | 51/54 [00:09<00:00,  5.32it/s]\u001b[A\n",
            "Predicting batches:  96%|| 52/54 [00:10<00:00,  5.28it/s]\u001b[A\n",
            "Predicting batches:  98%|| 53/54 [00:10<00:00,  5.41it/s]\u001b[A\n",
            "Predicting batches: 100%|| 54/54 [00:10<00:00,  5.27it/s]\u001b[A\n",
            "Processing subjects:  53%|| 30/57 [03:24<04:43, 10.48s/it, high school us histo\u001b[A\n",
            "Formatting batches:   0%|                               | 0/204 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 32/204 [00:00<00:00, 318.83it/s]\u001b[A\n",
            "Formatting batches:  31%|              | 64/204 [00:00<00:00, 313.93it/s]\u001b[A\n",
            "Formatting batches:  48%|           | 97/204 [00:00<00:00, 319.30it/s]\u001b[A\n",
            "Formatting batches:  63%|       | 129/204 [00:00<00:00, 319.30it/s]\u001b[A\n",
            "Formatting batches:  79%|    | 161/204 [00:00<00:00, 314.04it/s]\u001b[A\n",
            "Formatting batches:  95%| | 193/204 [00:00<00:00, 313.73it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/51 [00:00<00:30,  1.63it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:01<00:30,  1.63it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 3/51 [00:01<00:29,  1.63it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:02<00:28,  1.63it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:03<00:28,  1.63it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:03<00:27,  1.62it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:04<00:27,  1.63it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:04<00:26,  1.60it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:05<00:26,  1.58it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:06<00:26,  1.56it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:06<00:25,  1.56it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:07<00:24,  1.58it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:09<00:22,  1.59it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:10<00:22,  1.57it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:10<00:21,  1.56it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:11<00:21,  1.56it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:11<00:20,  1.58it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:12<00:19,  1.59it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:13<00:18,  1.60it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:13<00:18,  1.58it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:14<00:17,  1.59it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:15<00:16,  1.60it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:15<00:16,  1.61it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:16<00:15,  1.61it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:16<00:15,  1.58it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:17<00:14,  1.59it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:18<00:14,  1.57it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:18<00:13,  1.56it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:19<00:12,  1.57it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:20<00:11,  1.59it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:20<00:11,  1.60it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:21<00:10,  1.60it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:22<00:10,  1.58it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:22<00:09,  1.59it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:23<00:08,  1.61it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 38/51 [00:23<00:08,  1.61it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:24<00:07,  1.61it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:25<00:06,  1.59it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:25<00:06,  1.58it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:26<00:05,  1.58it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:27<00:05,  1.59it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:27<00:04,  1.56it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:28<00:03,  1.55it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:28<00:03,  1.55it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:29<00:02,  1.54it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:30<00:01,  1.57it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:30<00:01,  1.58it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:31<00:00,  1.59it/s]\u001b[A\n",
            "Predicting batches: 100%|| 51/51 [00:32<00:00,  1.59it/s]\u001b[A\n",
            "Processing subjects:  54%|| 31/57 [03:57<07:26, 17.17s/it, high school world hi\u001b[A\n",
            "Formatting batches:   0%|                               | 0/237 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 39/237 [00:00<00:00, 386.65it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 79/237 [00:00<00:00, 390.08it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 119/237 [00:00<00:00, 393.64it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 159/237 [00:00<00:00, 390.90it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 199/237 [00:00<00:00, 391.42it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/60 [00:00<00:23,  2.51it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/60 [00:00<00:21,  2.69it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/60 [00:01<00:22,  2.56it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/60 [00:01<00:21,  2.58it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/60 [00:01<00:21,  2.58it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/60 [00:02<00:22,  2.41it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 7/60 [00:02<00:21,  2.41it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 8/60 [00:03<00:20,  2.53it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/60 [00:03<00:21,  2.41it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/60 [00:04<00:20,  2.41it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 11/60 [00:04<00:19,  2.47it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/60 [00:04<00:19,  2.51it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 13/60 [00:05<00:18,  2.49it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 14/60 [00:05<00:19,  2.40it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 15/60 [00:06<00:18,  2.49it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/60 [00:06<00:17,  2.52it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 17/60 [00:06<00:16,  2.61it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 18/60 [00:07<00:16,  2.61it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 19/60 [00:07<00:16,  2.51it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 20/60 [00:07<00:15,  2.55it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 21/60 [00:08<00:15,  2.51it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/60 [00:08<00:16,  2.35it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 23/60 [00:09<00:15,  2.42it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 24/60 [00:09<00:14,  2.43it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 25/60 [00:10<00:14,  2.47it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 26/60 [00:10<00:13,  2.52it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 27/60 [00:10<00:13,  2.41it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/60 [00:11<00:13,  2.36it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 29/60 [00:11<00:13,  2.32it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 30/60 [00:12<00:12,  2.43it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 31/60 [00:12<00:11,  2.50it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 32/60 [00:12<00:11,  2.44it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 33/60 [00:13<00:10,  2.47it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 34/60 [00:13<00:10,  2.50it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 35/60 [00:14<00:10,  2.45it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 36/60 [00:14<00:09,  2.50it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 37/60 [00:14<00:09,  2.55it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 38/60 [00:15<00:08,  2.60it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 39/60 [00:15<00:08,  2.54it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 40/60 [00:16<00:07,  2.57it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 41/60 [00:16<00:07,  2.58it/s]\u001b[A\n",
            "Predicting batches:  70%|       | 42/60 [00:16<00:07,  2.45it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 43/60 [00:17<00:07,  2.40it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 44/60 [00:17<00:06,  2.41it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 45/60 [00:18<00:06,  2.38it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 46/60 [00:18<00:05,  2.48it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 47/60 [00:18<00:05,  2.50it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 48/60 [00:19<00:04,  2.42it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 49/60 [00:19<00:04,  2.53it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 50/60 [00:20<00:03,  2.57it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 51/60 [00:20<00:03,  2.58it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 52/60 [00:20<00:03,  2.47it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 53/60 [00:21<00:02,  2.45it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 54/60 [00:21<00:02,  2.41it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 55/60 [00:22<00:02,  2.48it/s]\u001b[A\n",
            "Predicting batches:  93%| | 56/60 [00:22<00:01,  2.55it/s]\u001b[A\n",
            "Predicting batches:  95%| | 57/60 [00:22<00:01,  2.45it/s]\u001b[A\n",
            "Predicting batches:  97%|| 58/60 [00:23<00:00,  2.51it/s]\u001b[A\n",
            "Predicting batches:  98%|| 59/60 [00:23<00:00,  2.63it/s]\u001b[A\n",
            "Processing subjects:  56%|    | 32/57 [04:21<08:03, 19.34s/it, human aging]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/223 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  26%|               | 59/223 [00:00<00:00, 583.19it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 121/223 [00:00<00:00, 600.06it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 182/223 [00:00<00:00, 595.71it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/56 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/56 [00:00<00:03, 13.95it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/56 [00:00<00:03, 13.45it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 6/56 [00:00<00:03, 13.44it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/56 [00:00<00:03, 13.51it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 10/56 [00:00<00:03, 13.68it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 12/56 [00:00<00:03, 13.61it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 14/56 [00:01<00:03, 13.75it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 16/56 [00:01<00:02, 13.63it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 18/56 [00:01<00:02, 13.54it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 20/56 [00:01<00:02, 13.72it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 22/56 [00:01<00:02, 13.67it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 24/56 [00:01<00:02, 13.73it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 26/56 [00:01<00:02, 13.75it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 28/56 [00:02<00:02, 13.85it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 30/56 [00:02<00:01, 13.87it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 32/56 [00:02<00:01, 13.76it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 34/56 [00:02<00:01, 13.79it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 36/56 [00:02<00:01, 13.68it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 38/56 [00:02<00:01, 13.57it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 40/56 [00:02<00:01, 13.46it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 42/56 [00:03<00:01, 13.47it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 44/56 [00:03<00:00, 13.57it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 46/56 [00:03<00:00, 13.70it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 48/56 [00:03<00:00, 13.61it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 50/56 [00:03<00:00, 13.38it/s]\u001b[A\n",
            "Predicting batches:  93%| | 52/56 [00:03<00:00, 13.50it/s]\u001b[A\n",
            "Predicting batches:  96%|| 54/56 [00:03<00:00, 13.63it/s]\u001b[A\n",
            "Predicting batches: 100%|| 56/56 [00:04<00:00, 14.13it/s]\u001b[A\n",
            "Processing subjects:  58%|  | 33/57 [04:26<05:57, 14.88s/it, human sexuality]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  46%|           | 60/131 [00:00<00:00, 590.47it/s]\u001b[A\n",
            "Formatting batches:  92%| | 121/131 [00:00<00:00, 597.22it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/33 [00:00<00:02, 12.56it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 4/33 [00:00<00:02, 12.34it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 6/33 [00:00<00:02, 12.59it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 8/33 [00:00<00:02, 12.23it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 10/33 [00:00<00:01, 12.37it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 12/33 [00:00<00:01, 12.36it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 14/33 [00:01<00:01, 12.65it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 16/33 [00:01<00:01, 12.57it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 18/33 [00:01<00:01, 12.61it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 20/33 [00:01<00:01, 12.40it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 22/33 [00:01<00:00, 12.60it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 24/33 [00:01<00:00, 12.51it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 26/33 [00:02<00:00, 11.77it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 28/33 [00:02<00:00, 11.86it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 30/33 [00:02<00:00, 11.92it/s]\u001b[A\n",
            "Predicting batches:  97%|| 32/33 [00:02<00:00, 12.03it/s]\u001b[A\n",
            "Processing subjects:  60%| | 34/57 [04:29<04:19, 11.28s/it, international law]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/121 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  41%|            | 50/121 [00:00<00:00, 499.94it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 102/121 [00:00<00:00, 506.51it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 1/31 [00:00<00:04,  6.88it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 2/31 [00:00<00:04,  6.71it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 3/31 [00:00<00:04,  6.82it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 4/31 [00:00<00:04,  6.70it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 5/31 [00:00<00:03,  6.52it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 6/31 [00:00<00:03,  6.52it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 7/31 [00:01<00:03,  6.42it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 8/31 [00:01<00:03,  6.46it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 9/31 [00:01<00:03,  6.48it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 10/31 [00:01<00:03,  6.62it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 11/31 [00:01<00:03,  6.59it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 12/31 [00:01<00:02,  6.57it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 13/31 [00:01<00:02,  6.66it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 14/31 [00:02<00:02,  6.73it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 15/31 [00:02<00:02,  6.66it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 16/31 [00:02<00:02,  6.62it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 17/31 [00:02<00:02,  6.59it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 18/31 [00:02<00:01,  6.67it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 19/31 [00:02<00:01,  6.60it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 20/31 [00:03<00:01,  6.55it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 21/31 [00:03<00:01,  6.43it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 22/31 [00:03<00:01,  6.56it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 23/31 [00:03<00:01,  6.55it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 24/31 [00:03<00:01,  6.64it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 25/31 [00:03<00:00,  6.70it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 26/31 [00:03<00:00,  6.63it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 27/31 [00:04<00:00,  6.69it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 28/31 [00:04<00:00,  6.74it/s]\u001b[A\n",
            "Predicting batches:  94%| | 29/31 [00:04<00:00,  6.65it/s]\u001b[A\n",
            "Predicting batches:  97%|| 30/31 [00:04<00:00,  6.61it/s]\u001b[A\n",
            "Processing subjects:  61%|  | 35/57 [04:34<03:25,  9.35s/it, jurisprudence]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/108 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 58/108 [00:00<00:00, 575.16it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/27 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/27 [00:00<00:02, 11.01it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/27 [00:00<00:02, 10.36it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 6/27 [00:00<00:02, 10.49it/s]\u001b[A\n",
            "Predicting batches:  30%|                 | 8/27 [00:00<00:01, 10.70it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 10/27 [00:00<00:01, 10.67it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 12/27 [00:01<00:01, 10.80it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 14/27 [00:01<00:01, 10.87it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 16/27 [00:01<00:01, 10.52it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 18/27 [00:01<00:00, 10.55it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 20/27 [00:01<00:00, 10.33it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 22/27 [00:02<00:00, 10.67it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 24/27 [00:02<00:00, 10.96it/s]\u001b[A\n",
            "Predicting batches:  96%|| 26/27 [00:02<00:00, 10.85it/s]\u001b[A\n",
            "Processing subjects:  63%| | 36/57 [04:36<02:34,  7.36s/it, logical fallacies]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/163 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  34%|             | 56/163 [00:00<00:00, 553.72it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 113/163 [00:00<00:00, 558.87it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/41 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/41 [00:00<00:04,  9.77it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/41 [00:00<00:04,  9.51it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 3/41 [00:00<00:03,  9.67it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/41 [00:00<00:03,  9.77it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 5/41 [00:00<00:03,  9.58it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 6/41 [00:00<00:03,  9.46it/s]\u001b[A\n",
            "Predicting batches:  17%|                    | 7/41 [00:00<00:03,  9.56it/s]\u001b[A\n",
            "Predicting batches:  20%|                   | 8/41 [00:00<00:03,  9.32it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 9/41 [00:00<00:03,  9.28it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/41 [00:01<00:03,  9.25it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 11/41 [00:01<00:03,  9.13it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/41 [00:01<00:03,  9.15it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 13/41 [00:01<00:03,  9.17it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 14/41 [00:01<00:02,  9.18it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 15/41 [00:01<00:02,  9.19it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 16/41 [00:01<00:02,  9.10it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 17/41 [00:01<00:02,  9.32it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 18/41 [00:01<00:02,  9.30it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 19/41 [00:02<00:02,  9.18it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 20/41 [00:02<00:02,  9.18it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 21/41 [00:02<00:02,  9.38it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 22/41 [00:02<00:02,  9.34it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 23/41 [00:02<00:01,  9.20it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 24/41 [00:02<00:01,  9.19it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 25/41 [00:02<00:01,  9.34it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 26/41 [00:02<00:01,  9.29it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 27/41 [00:02<00:01,  9.25it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 28/41 [00:03<00:01,  9.22it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 29/41 [00:03<00:01,  9.36it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 30/41 [00:03<00:01,  9.45it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 31/41 [00:03<00:01,  9.37it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 32/41 [00:03<00:01,  8.94it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 33/41 [00:03<00:00,  9.20it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 34/41 [00:03<00:00,  9.20it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 35/41 [00:03<00:00,  9.19it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 36/41 [00:03<00:00,  9.36it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 37/41 [00:03<00:00,  9.17it/s]\u001b[A\n",
            "Predicting batches:  93%| | 38/41 [00:04<00:00,  9.08it/s]\u001b[A\n",
            "Predicting batches:  95%| | 39/41 [00:04<00:00,  9.11it/s]\u001b[A\n",
            "Predicting batches:  98%|| 40/41 [00:04<00:00,  9.13it/s]\u001b[A\n",
            "Processing subjects:  65%| | 37/57 [04:41<02:11,  6.56s/it, machine learning]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  46%|           | 52/112 [00:00<00:00, 513.52it/s]\u001b[A\n",
            "Formatting batches:  93%| | 104/112 [00:00<00:00, 506.08it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/28 [00:00<00:03,  6.81it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 2/28 [00:00<00:03,  6.67it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 3/28 [00:00<00:03,  6.61it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 4/28 [00:00<00:03,  6.57it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 5/28 [00:00<00:03,  6.54it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 6/28 [00:00<00:03,  6.52it/s]\u001b[A\n",
            "Predicting batches:  25%|                  | 7/28 [00:01<00:03,  6.52it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 8/28 [00:01<00:03,  6.31it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 9/28 [00:01<00:02,  6.36it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 10/28 [00:01<00:02,  6.39it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 11/28 [00:01<00:02,  6.23it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 12/28 [00:01<00:02,  6.31it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 13/28 [00:02<00:02,  6.28it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 14/28 [00:02<00:02,  6.44it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 15/28 [00:02<00:02,  6.37it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 16/28 [00:02<00:01,  6.40it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 17/28 [00:02<00:01,  6.25it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 18/28 [00:02<00:01,  6.31it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 19/28 [00:02<00:01,  6.37it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 20/28 [00:03<00:01,  6.33it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 21/28 [00:03<00:01,  6.36it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 22/28 [00:03<00:00,  6.38it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 23/28 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 24/28 [00:03<00:00,  6.36it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 25/28 [00:03<00:00,  6.31it/s]\u001b[A\n",
            "Predicting batches:  93%| | 26/28 [00:04<00:00,  5.94it/s]\u001b[A\n",
            "Predicting batches:  96%|| 27/28 [00:04<00:00,  5.75it/s]\u001b[A\n",
            "Predicting batches: 100%|| 28/28 [00:04<00:00,  5.85it/s]\u001b[A\n",
            "Processing subjects:  67%|   | 38/57 [04:46<01:54,  6.00s/it, management]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/103 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 60/103 [00:00<00:00, 597.73it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/26 [00:00<00:01, 15.61it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 4/26 [00:00<00:01, 15.06it/s]\u001b[A\n",
            "Predicting batches:  23%|                  | 6/26 [00:00<00:01, 15.07it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 8/26 [00:00<00:01, 15.04it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 10/26 [00:00<00:01, 15.06it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 12/26 [00:00<00:00, 15.05it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 14/26 [00:00<00:00, 15.01it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 16/26 [00:01<00:00, 15.02it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 18/26 [00:01<00:00, 15.02it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 20/26 [00:01<00:00, 14.88it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 22/26 [00:01<00:00, 14.78it/s]\u001b[A\n",
            "Predicting batches:  92%| | 24/26 [00:01<00:00, 14.84it/s]\u001b[A\n",
            "Predicting batches: 100%|| 26/26 [00:01<00:00, 15.37it/s]\u001b[A\n",
            "Processing subjects:  68%|   | 39/57 [04:48<01:25,  4.77s/it, marketing]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/234 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  24%|                | 56/234 [00:00<00:00, 558.94it/s]\u001b[A\n",
            "Formatting batches:  49%|          | 114/234 [00:00<00:00, 567.24it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 173/234 [00:00<00:00, 576.20it/s]\u001b[A\n",
            "Formatting batches:  99%|| 231/234 [00:00<00:00, 576.56it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/59 [00:00<00:05, 10.67it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 4/59 [00:00<00:05, 10.75it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/59 [00:00<00:04, 10.78it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 8/59 [00:00<00:04, 10.76it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 10/59 [00:00<00:04, 10.76it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 12/59 [00:01<00:04, 10.76it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 14/59 [00:01<00:04, 10.86it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 16/59 [00:01<00:03, 10.81it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 18/59 [00:01<00:03, 10.76it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 20/59 [00:01<00:03, 10.83it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 22/59 [00:02<00:03, 10.62it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 24/59 [00:02<00:03, 10.53it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 26/59 [00:02<00:03, 10.58it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 28/59 [00:02<00:02, 10.48it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 30/59 [00:02<00:02, 10.66it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 32/59 [00:02<00:02, 10.79it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 34/59 [00:03<00:02, 10.62it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 36/59 [00:03<00:02, 10.63it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 38/59 [00:03<00:01, 10.89it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 40/59 [00:03<00:01, 10.67it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 42/59 [00:03<00:01, 10.66it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 44/59 [00:04<00:01, 10.56it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 46/59 [00:04<00:01, 10.48it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 48/59 [00:04<00:01, 10.68it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 50/59 [00:04<00:00, 10.66it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 52/59 [00:04<00:00, 10.66it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 54/59 [00:05<00:00, 10.53it/s]\u001b[A\n",
            "Predicting batches:  95%| | 56/59 [00:05<00:00, 10.59it/s]\u001b[A\n",
            "Predicting batches:  98%|| 58/59 [00:05<00:00, 10.71it/s]\u001b[A\n",
            "Processing subjects:  70%| | 40/57 [04:53<01:26,  5.11s/it, medical genetics]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 60/100 [00:00<00:00, 592.42it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:01, 13.00it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:01, 12.99it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 12.82it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 12.63it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 12.53it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:00<00:01, 12.47it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:00, 12.32it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 12.27it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 12.56it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 12.67it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:01<00:00, 12.73it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:01<00:00, 12.77it/s]\u001b[A\n",
            "Processing subjects:  72%|  | 41/57 [04:56<01:07,  4.22s/it, miscellaneous]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/783 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   8%|                   | 62/783 [00:00<00:01, 616.60it/s]\u001b[A\n",
            "Formatting batches:  16%|                | 124/783 [00:00<00:01, 616.18it/s]\u001b[A\n",
            "Formatting batches:  24%|               | 188/783 [00:00<00:00, 625.19it/s]\u001b[A\n",
            "Formatting batches:  32%|             | 251/783 [00:00<00:00, 626.45it/s]\u001b[A\n",
            "Formatting batches:  40%|            | 315/783 [00:00<00:00, 630.35it/s]\u001b[A\n",
            "Formatting batches:  48%|          | 379/783 [00:00<00:00, 631.73it/s]\u001b[A\n",
            "Formatting batches:  57%|        | 443/783 [00:00<00:00, 633.71it/s]\u001b[A\n",
            "Formatting batches:  65%|       | 507/783 [00:00<00:00, 628.11it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 571/783 [00:00<00:00, 630.37it/s]\u001b[A\n",
            "Formatting batches:  81%|   | 635/783 [00:01<00:00, 626.44it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 699/783 [00:01<00:00, 629.23it/s]\u001b[A\n",
            "Formatting batches:  97%|| 762/783 [00:01<00:00, 627.65it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/196 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/196 [00:00<00:13, 14.71it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 4/196 [00:00<00:13, 14.33it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 6/196 [00:00<00:13, 14.35it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 8/196 [00:00<00:12, 14.71it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 10/196 [00:00<00:12, 14.86it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 12/196 [00:00<00:12, 14.65it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 14/196 [00:00<00:12, 14.66it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 16/196 [00:01<00:12, 14.42it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 18/196 [00:01<00:12, 14.37it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 20/196 [00:01<00:13, 13.51it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 22/196 [00:01<00:12, 13.99it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 24/196 [00:01<00:12, 14.08it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 26/196 [00:01<00:11, 14.36it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 28/196 [00:01<00:11, 14.19it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 30/196 [00:02<00:12, 13.29it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 32/196 [00:02<00:11, 13.78it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 34/196 [00:02<00:11, 13.79it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 36/196 [00:02<00:11, 13.88it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 38/196 [00:02<00:11, 14.08it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 40/196 [00:02<00:10, 14.40it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 42/196 [00:02<00:10, 14.29it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 44/196 [00:03<00:10, 14.22it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 46/196 [00:03<00:10, 13.86it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 48/196 [00:03<00:10, 13.80it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 50/196 [00:03<00:10, 13.96it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 52/196 [00:03<00:10, 14.14it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 54/196 [00:03<00:10, 13.65it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 56/196 [00:03<00:10, 13.37it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 58/196 [00:04<00:10, 13.70it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 60/196 [00:04<00:09, 14.09it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 62/196 [00:04<00:09, 14.24it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 64/196 [00:04<00:09, 14.03it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 66/196 [00:04<00:09, 13.37it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 68/196 [00:04<00:09, 13.56it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 70/196 [00:04<00:09, 13.98it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 72/196 [00:05<00:08, 14.12it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 74/196 [00:05<00:08, 14.40it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 76/196 [00:05<00:08, 14.58it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 78/196 [00:05<00:08, 14.42it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 80/196 [00:05<00:08, 14.29it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 82/196 [00:05<00:07, 14.51it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 84/196 [00:05<00:07, 14.53it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 86/196 [00:06<00:07, 14.53it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 88/196 [00:06<00:07, 14.37it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 90/196 [00:06<00:07, 14.22it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 92/196 [00:06<00:07, 14.05it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 94/196 [00:06<00:07, 14.31it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 96/196 [00:06<00:06, 14.37it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 98/196 [00:06<00:06, 14.57it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 100/196 [00:07<00:06, 14.34it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 102/196 [00:07<00:06, 14.36it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 104/196 [00:07<00:06, 14.03it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 106/196 [00:07<00:06, 14.13it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 108/196 [00:07<00:06, 13.87it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 110/196 [00:07<00:06, 13.89it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 112/196 [00:07<00:06, 13.87it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 114/196 [00:08<00:05, 14.21it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 116/196 [00:08<00:05, 14.46it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 118/196 [00:08<00:05, 13.63it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 120/196 [00:08<00:05, 13.59it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 122/196 [00:08<00:05, 13.32it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 124/196 [00:08<00:05, 13.68it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 126/196 [00:08<00:05, 13.35it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 128/196 [00:09<00:05, 13.43it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 130/196 [00:09<00:05, 12.97it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 132/196 [00:09<00:04, 13.36it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 134/196 [00:09<00:04, 13.81it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 136/196 [00:09<00:04, 13.81it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 138/196 [00:09<00:04, 12.89it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 140/196 [00:10<00:04, 13.31it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 142/196 [00:10<00:04, 13.39it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 144/196 [00:10<00:03, 13.13it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 146/196 [00:10<00:03, 13.37it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 148/196 [00:10<00:03, 12.83it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 150/196 [00:10<00:03, 12.74it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 152/196 [00:10<00:03, 12.31it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 154/196 [00:11<00:03, 12.87it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 156/196 [00:11<00:03, 13.32it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 158/196 [00:11<00:02, 13.79it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 160/196 [00:11<00:02, 13.97it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 162/196 [00:11<00:02, 12.82it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 164/196 [00:11<00:02, 13.03it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 166/196 [00:11<00:02, 13.41it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 168/196 [00:12<00:02, 13.54it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 170/196 [00:12<00:01, 13.96it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 172/196 [00:12<00:01, 13.96it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 174/196 [00:12<00:01, 13.74it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 176/196 [00:12<00:01, 13.95it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 178/196 [00:12<00:01, 14.27it/s]\u001b[A\n",
            "Predicting batches:  92%| | 180/196 [00:12<00:01, 13.89it/s]\u001b[A\n",
            "Predicting batches:  93%| | 182/196 [00:13<00:00, 14.24it/s]\u001b[A\n",
            "Predicting batches:  94%| | 184/196 [00:13<00:00, 14.47it/s]\u001b[A\n",
            "Predicting batches:  95%| | 186/196 [00:13<00:00, 14.44it/s]\u001b[A\n",
            "Predicting batches:  96%|| 188/196 [00:13<00:00, 14.62it/s]\u001b[A\n",
            "Predicting batches:  97%|| 190/196 [00:13<00:00, 14.38it/s]\u001b[A\n",
            "Predicting batches:  98%|| 192/196 [00:13<00:00, 14.19it/s]\u001b[A\n",
            "Predicting batches:  99%|| 194/196 [00:13<00:00, 13.88it/s]\u001b[A\n",
            "Predicting batches: 100%|| 196/196 [00:14<00:00, 14.50it/s]\u001b[A\n",
            "Processing subjects:  74%| | 42/57 [05:11<01:53,  7.56s/it, moral disputes]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/346 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  15%|                 | 53/346 [00:00<00:00, 527.62it/s]\u001b[A\n",
            "Formatting batches:  32%|             | 109/346 [00:00<00:00, 543.72it/s]\u001b[A\n",
            "Formatting batches:  48%|          | 166/346 [00:00<00:00, 553.89it/s]\u001b[A\n",
            "Formatting batches:  64%|       | 222/346 [00:00<00:00, 550.58it/s]\u001b[A\n",
            "Formatting batches:  81%|   | 279/346 [00:00<00:00, 554.85it/s]\u001b[A\n",
            "Formatting batches:  97%|| 335/346 [00:00<00:00, 552.66it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/87 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/87 [00:00<00:09,  9.42it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/87 [00:00<00:09,  9.41it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 3/87 [00:00<00:09,  9.19it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 4/87 [00:00<00:08,  9.23it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/87 [00:00<00:09,  9.07it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 6/87 [00:00<00:08,  9.11it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 7/87 [00:00<00:08,  9.01it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 8/87 [00:00<00:08,  8.97it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 9/87 [00:01<00:09,  8.63it/s]\u001b[A\n",
            "Predicting batches:  11%|                    | 10/87 [00:01<00:08,  8.81it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 11/87 [00:01<00:08,  8.83it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 12/87 [00:01<00:08,  8.93it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 13/87 [00:01<00:08,  8.65it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 14/87 [00:01<00:08,  8.71it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 15/87 [00:01<00:08,  8.85it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 16/87 [00:01<00:07,  8.94it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 17/87 [00:01<00:07,  9.00it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 18/87 [00:02<00:07,  8.93it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 19/87 [00:02<00:07,  9.00it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 20/87 [00:02<00:07,  9.06it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 21/87 [00:02<00:07,  9.09it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 22/87 [00:02<00:07,  9.00it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 23/87 [00:02<00:07,  8.94it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 24/87 [00:02<00:06,  9.01it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 25/87 [00:02<00:06,  9.05it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 26/87 [00:02<00:06,  9.00it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 27/87 [00:03<00:06,  8.95it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 28/87 [00:03<00:06,  8.92it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 29/87 [00:03<00:06,  9.00it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 30/87 [00:03<00:06,  9.06it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 31/87 [00:03<00:06,  8.97it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 32/87 [00:03<00:06,  8.94it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 33/87 [00:03<00:06,  8.91it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 34/87 [00:03<00:05,  8.86it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 35/87 [00:03<00:05,  8.94it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 36/87 [00:04<00:05,  8.89it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 37/87 [00:04<00:05,  8.97it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 38/87 [00:04<00:05,  8.65it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 39/87 [00:04<00:05,  8.80it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 40/87 [00:04<00:05,  8.91it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 41/87 [00:04<00:05,  8.98it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 42/87 [00:04<00:04,  9.04it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 43/87 [00:04<00:04,  8.96it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 44/87 [00:04<00:04,  8.93it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 45/87 [00:05<00:04,  9.00it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 46/87 [00:05<00:04,  9.06it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 47/87 [00:05<00:04,  8.71it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 48/87 [00:05<00:04,  8.85it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 49/87 [00:05<00:04,  8.95it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 50/87 [00:05<00:04,  8.92it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 51/87 [00:05<00:04,  8.99it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 52/87 [00:05<00:03,  9.05it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 53/87 [00:05<00:03,  8.97it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 54/87 [00:06<00:03,  9.03it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 55/87 [00:06<00:03,  9.05it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 56/87 [00:06<00:03,  8.99it/s]\u001b[A\n",
            "Predicting batches:  66%|        | 57/87 [00:06<00:03,  9.05it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 58/87 [00:06<00:03,  8.97it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 59/87 [00:06<00:03,  8.92it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 60/87 [00:06<00:03,  8.90it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 61/87 [00:06<00:02,  8.88it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 62/87 [00:06<00:02,  8.97it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 63/87 [00:07<00:02,  9.03it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 64/87 [00:07<00:02,  8.97it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 65/87 [00:07<00:02,  8.64it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 66/87 [00:07<00:02,  8.79it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 67/87 [00:07<00:02,  8.81it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 68/87 [00:07<00:02,  8.92it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 69/87 [00:07<00:02,  8.87it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 70/87 [00:07<00:01,  8.62it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 71/87 [00:07<00:01,  8.68it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 72/87 [00:08<00:01,  8.73it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 73/87 [00:08<00:01,  8.86it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 74/87 [00:08<00:01,  8.86it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 75/87 [00:08<00:01,  8.95it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 76/87 [00:08<00:01,  8.92it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 77/87 [00:08<00:01,  8.97it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 78/87 [00:08<00:00,  9.03it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 79/87 [00:08<00:00,  9.07it/s]\u001b[A\n",
            "Predicting batches:  92%| | 80/87 [00:08<00:00,  8.98it/s]\u001b[A\n",
            "Predicting batches:  93%| | 81/87 [00:09<00:00,  9.02it/s]\u001b[A\n",
            "Predicting batches:  94%| | 82/87 [00:09<00:00,  9.07it/s]\u001b[A\n",
            "Predicting batches:  95%| | 83/87 [00:09<00:00,  9.09it/s]\u001b[A\n",
            "Predicting batches:  97%|| 84/87 [00:09<00:00,  9.12it/s]\u001b[A\n",
            "Predicting batches:  98%|| 85/87 [00:09<00:00,  9.14it/s]\u001b[A\n",
            "Predicting batches:  99%|| 86/87 [00:09<00:00,  9.03it/s]\u001b[A\n",
            "Processing subjects:  75%| | 43/57 [05:21<01:57,  8.38s/it, moral scenarios]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/895 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   6%|                   | 51/895 [00:00<00:01, 503.70it/s]\u001b[A\n",
            "Formatting batches:  12%|                 | 105/895 [00:00<00:01, 521.13it/s]\u001b[A\n",
            "Formatting batches:  18%|                | 160/895 [00:00<00:01, 530.07it/s]\u001b[A\n",
            "Formatting batches:  24%|               | 214/895 [00:00<00:01, 530.52it/s]\u001b[A\n",
            "Formatting batches:  30%|              | 268/895 [00:00<00:01, 533.39it/s]\u001b[A\n",
            "Formatting batches:  36%|            | 322/895 [00:00<00:01, 533.13it/s]\u001b[A\n",
            "Formatting batches:  42%|           | 377/895 [00:00<00:00, 536.03it/s]\u001b[A\n",
            "Formatting batches:  48%|          | 431/895 [00:00<00:00, 534.18it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 485/895 [00:00<00:00, 532.05it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 539/895 [00:01<00:00, 528.86it/s]\u001b[A\n",
            "Formatting batches:  66%|      | 592/895 [00:01<00:00, 527.80it/s]\u001b[A\n",
            "Formatting batches:  72%|     | 646/895 [00:01<00:00, 528.86it/s]\u001b[A\n",
            "Formatting batches:  78%|    | 701/895 [00:01<00:00, 532.85it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 755/895 [00:01<00:00, 531.69it/s]\u001b[A\n",
            "Formatting batches:  91%|  | 810/895 [00:01<00:00, 534.87it/s]\u001b[A\n",
            "Formatting batches:  97%|| 864/895 [00:01<00:00, 533.59it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/224 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   0%|                       | 1/224 [00:00<00:30,  7.27it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/224 [00:00<00:32,  6.85it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 3/224 [00:00<00:32,  6.88it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 4/224 [00:00<00:32,  6.73it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 5/224 [00:00<00:32,  6.78it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 6/224 [00:00<00:32,  6.68it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 7/224 [00:01<00:32,  6.64it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 8/224 [00:01<00:32,  6.61it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 9/224 [00:01<00:32,  6.59it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 10/224 [00:01<00:32,  6.59it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 11/224 [00:01<00:32,  6.58it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 12/224 [00:01<00:32,  6.57it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 13/224 [00:01<00:32,  6.56it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 14/224 [00:02<00:31,  6.65it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 15/224 [00:02<00:31,  6.71it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 16/224 [00:02<00:31,  6.66it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 17/224 [00:02<00:31,  6.62it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 18/224 [00:02<00:31,  6.60it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 19/224 [00:02<00:30,  6.68it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 20/224 [00:03<00:30,  6.63it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 21/224 [00:03<00:30,  6.70it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 22/224 [00:03<00:29,  6.74it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 23/224 [00:03<00:30,  6.66it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 24/224 [00:03<00:30,  6.62it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 25/224 [00:03<00:30,  6.60it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 26/224 [00:03<00:30,  6.58it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 27/224 [00:04<00:29,  6.57it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 28/224 [00:04<00:29,  6.56it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 29/224 [00:04<00:29,  6.55it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 30/224 [00:04<00:29,  6.54it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 31/224 [00:04<00:29,  6.63it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 32/224 [00:04<00:29,  6.59it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 33/224 [00:04<00:29,  6.57it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 34/224 [00:05<00:28,  6.56it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 35/224 [00:05<00:28,  6.55it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 36/224 [00:05<00:28,  6.54it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 37/224 [00:05<00:28,  6.54it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 38/224 [00:05<00:28,  6.53it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 39/224 [00:05<00:28,  6.51it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 40/224 [00:06<00:27,  6.61it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 41/224 [00:06<00:27,  6.58it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 42/224 [00:06<00:27,  6.57it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 43/224 [00:06<00:27,  6.55it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 44/224 [00:06<00:27,  6.54it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 45/224 [00:06<00:27,  6.54it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 46/224 [00:06<00:27,  6.53it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 47/224 [00:07<00:27,  6.51it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 48/224 [00:07<00:27,  6.51it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 49/224 [00:07<00:26,  6.51it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 50/224 [00:07<00:26,  6.51it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 51/224 [00:07<00:26,  6.52it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 52/224 [00:07<00:26,  6.52it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 53/224 [00:08<00:25,  6.61it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 54/224 [00:08<00:25,  6.58it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 55/224 [00:08<00:25,  6.65it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 56/224 [00:08<00:25,  6.71it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 57/224 [00:08<00:25,  6.65it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 58/224 [00:08<00:25,  6.61it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 59/224 [00:08<00:25,  6.58it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 60/224 [00:09<00:24,  6.56it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 61/224 [00:09<00:24,  6.55it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 62/224 [00:09<00:24,  6.64it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 63/224 [00:09<00:24,  6.70it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 64/224 [00:09<00:24,  6.64it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 65/224 [00:09<00:24,  6.60it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 66/224 [00:09<00:24,  6.58it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 67/224 [00:10<00:23,  6.66it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 68/224 [00:10<00:23,  6.72it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 69/224 [00:10<00:23,  6.63it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 70/224 [00:10<00:23,  6.59it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 71/224 [00:10<00:23,  6.57it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 72/224 [00:10<00:22,  6.66it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 73/224 [00:11<00:22,  6.61it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 74/224 [00:11<00:22,  6.58it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 75/224 [00:11<00:22,  6.56it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 76/224 [00:11<00:22,  6.64it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 77/224 [00:11<00:22,  6.58it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 78/224 [00:11<00:22,  6.56it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 79/224 [00:11<00:22,  6.55it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 80/224 [00:12<00:22,  6.54it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 81/224 [00:12<00:21,  6.63it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 82/224 [00:12<00:21,  6.59it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 83/224 [00:12<00:21,  6.57it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 84/224 [00:12<00:21,  6.55it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 85/224 [00:12<00:21,  6.54it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 86/224 [00:13<00:21,  6.53it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 87/224 [00:13<00:21,  6.52it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 88/224 [00:13<00:20,  6.50it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 89/224 [00:13<00:20,  6.51it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 90/224 [00:13<00:20,  6.51it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 91/224 [00:13<00:20,  6.61it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 92/224 [00:13<00:20,  6.58it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 93/224 [00:14<00:19,  6.56it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 94/224 [00:14<00:19,  6.55it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 95/224 [00:14<00:19,  6.54it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 96/224 [00:14<00:19,  6.54it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 97/224 [00:14<00:19,  6.53it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 98/224 [00:14<00:19,  6.63it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 99/224 [00:15<00:18,  6.59it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 100/224 [00:15<00:18,  6.67it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 101/224 [00:15<00:18,  6.62it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 102/224 [00:15<00:18,  6.59it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 103/224 [00:15<00:18,  6.56it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 104/224 [00:15<00:18,  6.55it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 105/224 [00:15<00:18,  6.54it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 106/224 [00:16<00:18,  6.52it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 107/224 [00:16<00:17,  6.52it/s]\u001b[A\n",
            "Predicting batches:  48%|          | 108/224 [00:16<00:17,  6.52it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 109/224 [00:16<00:17,  6.52it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 110/224 [00:16<00:17,  6.61it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 111/224 [00:16<00:17,  6.57it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 112/224 [00:16<00:17,  6.55it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 113/224 [00:17<00:16,  6.63it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 114/224 [00:17<00:16,  6.58it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 115/224 [00:17<00:16,  6.66it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 116/224 [00:17<00:16,  6.61it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 117/224 [00:17<00:16,  6.58it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 118/224 [00:17<00:15,  6.65it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 119/224 [00:18<00:15,  6.71it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 120/224 [00:18<00:15,  6.64it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 121/224 [00:18<00:15,  6.70it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 122/224 [00:18<00:15,  6.62it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 123/224 [00:18<00:15,  6.59it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 124/224 [00:18<00:15,  6.55it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 125/224 [00:18<00:15,  6.53it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 126/224 [00:19<00:15,  6.52it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 127/224 [00:19<00:14,  6.52it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 128/224 [00:19<00:14,  6.62it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 129/224 [00:19<00:14,  6.58it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 130/224 [00:19<00:14,  6.56it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 131/224 [00:19<00:14,  6.54it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 132/224 [00:20<00:13,  6.63it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 133/224 [00:20<00:13,  6.68it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 134/224 [00:20<00:13,  6.63it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 135/224 [00:20<00:13,  6.59it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 136/224 [00:20<00:13,  6.66it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 137/224 [00:20<00:12,  6.70it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 138/224 [00:20<00:12,  6.64it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 139/224 [00:21<00:12,  6.57it/s]\u001b[A\n",
            "Predicting batches:  62%|       | 140/224 [00:21<00:12,  6.55it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 141/224 [00:21<00:12,  6.54it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 142/224 [00:21<00:12,  6.52it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 143/224 [00:21<00:12,  6.49it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 144/224 [00:21<00:12,  6.50it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 145/224 [00:22<00:12,  6.51it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 146/224 [00:22<00:11,  6.51it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 147/224 [00:22<00:11,  6.51it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 148/224 [00:22<00:11,  6.51it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 149/224 [00:22<00:11,  6.60it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 150/224 [00:22<00:11,  6.67it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 151/224 [00:22<00:10,  6.72it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 152/224 [00:23<00:10,  6.65it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 153/224 [00:23<00:10,  6.70it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 154/224 [00:23<00:10,  6.63it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 155/224 [00:23<00:10,  6.58it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 156/224 [00:23<00:10,  6.56it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 157/224 [00:23<00:10,  6.54it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 158/224 [00:23<00:10,  6.52it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 159/224 [00:24<00:09,  6.61it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 160/224 [00:24<00:09,  6.67it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 161/224 [00:24<00:09,  6.60it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 162/224 [00:24<00:09,  6.56it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 163/224 [00:24<00:09,  6.55it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 164/224 [00:24<00:09,  6.63it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 165/224 [00:25<00:08,  6.69it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 166/224 [00:25<00:08,  6.72it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 167/224 [00:25<00:08,  6.74it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 168/224 [00:25<00:08,  6.66it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 169/224 [00:25<00:08,  6.72it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 170/224 [00:25<00:08,  6.65it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 171/224 [00:25<00:08,  6.60it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 172/224 [00:26<00:07,  6.57it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 173/224 [00:26<00:07,  6.55it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 174/224 [00:26<00:07,  6.64it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 175/224 [00:26<00:07,  6.58it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 176/224 [00:26<00:07,  6.56it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 177/224 [00:26<00:07,  6.52it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 178/224 [00:26<00:07,  6.51it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 179/224 [00:27<00:06,  6.60it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 180/224 [00:27<00:06,  6.56it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 181/224 [00:27<00:06,  6.54it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 182/224 [00:27<00:06,  6.54it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 183/224 [00:27<00:06,  6.53it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 184/224 [00:27<00:06,  6.52it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 185/224 [00:28<00:05,  6.52it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 186/224 [00:28<00:05,  6.51it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 187/224 [00:28<00:05,  6.59it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 188/224 [00:28<00:05,  6.56it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 189/224 [00:28<00:05,  6.53it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 190/224 [00:28<00:05,  6.62it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 191/224 [00:28<00:05,  6.57it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 192/224 [00:29<00:04,  6.54it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 193/224 [00:29<00:04,  6.63it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 194/224 [00:29<00:04,  6.58it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 195/224 [00:29<00:04,  6.56it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 196/224 [00:29<00:04,  6.54it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 197/224 [00:29<00:04,  6.62it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 198/224 [00:30<00:03,  6.57it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 199/224 [00:30<00:03,  6.54it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 200/224 [00:30<00:03,  6.53it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 201/224 [00:30<00:03,  6.62it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 202/224 [00:30<00:03,  6.58it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 203/224 [00:30<00:03,  6.56it/s]\u001b[A\n",
            "Predicting batches:  91%| | 204/224 [00:30<00:03,  6.54it/s]\u001b[A\n",
            "Predicting batches:  92%| | 205/224 [00:31<00:02,  6.63it/s]\u001b[A\n",
            "Predicting batches:  92%| | 206/224 [00:31<00:02,  6.58it/s]\u001b[A\n",
            "Predicting batches:  92%| | 207/224 [00:31<00:02,  6.56it/s]\u001b[A\n",
            "Predicting batches:  93%| | 208/224 [00:31<00:02,  6.55it/s]\u001b[A\n",
            "Predicting batches:  93%| | 209/224 [00:31<00:02,  6.52it/s]\u001b[A\n",
            "Predicting batches:  94%| | 210/224 [00:31<00:02,  6.60it/s]\u001b[A\n",
            "Predicting batches:  94%| | 211/224 [00:32<00:01,  6.56it/s]\u001b[A\n",
            "Predicting batches:  95%| | 212/224 [00:32<00:01,  6.63it/s]\u001b[A\n",
            "Predicting batches:  95%| | 213/224 [00:32<00:01,  6.68it/s]\u001b[A\n",
            "Predicting batches:  96%| | 214/224 [00:32<00:01,  6.61it/s]\u001b[A\n",
            "Predicting batches:  96%|| 215/224 [00:32<00:01,  6.58it/s]\u001b[A\n",
            "Predicting batches:  96%|| 216/224 [00:32<00:01,  6.56it/s]\u001b[A\n",
            "Predicting batches:  97%|| 217/224 [00:32<00:01,  6.53it/s]\u001b[A\n",
            "Predicting batches:  97%|| 218/224 [00:33<00:00,  6.62it/s]\u001b[A\n",
            "Predicting batches:  98%|| 219/224 [00:33<00:00,  6.56it/s]\u001b[A\n",
            "Predicting batches:  98%|| 220/224 [00:33<00:00,  6.54it/s]\u001b[A\n",
            "Predicting batches:  99%|| 221/224 [00:33<00:00,  6.53it/s]\u001b[A\n",
            "Predicting batches:  99%|| 222/224 [00:33<00:00,  6.52it/s]\u001b[A\n",
            "Predicting batches: 100%|| 223/224 [00:33<00:00,  6.51it/s]\u001b[A\n",
            "Predicting batches: 100%|| 224/224 [00:33<00:00,  7.02it/s]\u001b[A\n",
            "Processing subjects:  77%|  | 44/57 [05:57<03:35, 16.57s/it, nutrition]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/306 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 52/306 [00:00<00:00, 516.28it/s]\u001b[A\n",
            "Formatting batches:  35%|             | 106/306 [00:00<00:00, 526.86it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 159/306 [00:00<00:00, 508.12it/s]\u001b[A\n",
            "Formatting batches:  70%|      | 214/306 [00:00<00:00, 521.80it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 267/306 [00:00<00:00, 518.51it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/77 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/77 [00:00<00:09,  7.70it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/77 [00:00<00:09,  7.78it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/77 [00:00<00:09,  7.49it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/77 [00:00<00:10,  7.08it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/77 [00:00<00:10,  7.03it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/77 [00:00<00:10,  6.99it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 7/77 [00:00<00:09,  7.19it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/77 [00:01<00:09,  7.16it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 9/77 [00:01<00:09,  7.13it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/77 [00:01<00:09,  7.07it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 11/77 [00:01<00:09,  7.04it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 12/77 [00:01<00:09,  7.00it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 13/77 [00:01<00:09,  7.04it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/77 [00:01<00:08,  7.21it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/77 [00:02<00:08,  7.22it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/77 [00:02<00:08,  7.23it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 17/77 [00:02<00:08,  7.36it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/77 [00:02<00:07,  7.44it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 19/77 [00:02<00:07,  7.51it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/77 [00:02<00:07,  7.38it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 21/77 [00:02<00:07,  7.36it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 22/77 [00:03<00:07,  7.44it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 23/77 [00:03<00:07,  7.24it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 24/77 [00:03<00:07,  7.14it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 25/77 [00:03<00:07,  7.18it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 26/77 [00:03<00:07,  7.07it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 27/77 [00:03<00:07,  7.14it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/77 [00:03<00:06,  7.19it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 29/77 [00:04<00:06,  7.34it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 30/77 [00:04<00:06,  7.31it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 31/77 [00:04<00:06,  7.19it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 32/77 [00:04<00:06,  7.16it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 33/77 [00:04<00:06,  7.14it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 34/77 [00:04<00:06,  7.06it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 35/77 [00:04<00:05,  7.21it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 36/77 [00:05<00:05,  6.97it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 37/77 [00:05<00:05,  7.08it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/77 [00:05<00:05,  7.03it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 39/77 [00:05<00:05,  6.97it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 40/77 [00:05<00:05,  7.00it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 41/77 [00:05<00:05,  7.16it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 42/77 [00:05<00:04,  7.32it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 43/77 [00:05<00:04,  7.39it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 44/77 [00:06<00:04,  7.45it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 45/77 [00:06<00:04,  7.34it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 46/77 [00:06<00:04,  7.18it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 47/77 [00:06<00:04,  7.15it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/77 [00:06<00:04,  7.07it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 49/77 [00:06<00:04,  6.99it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 50/77 [00:06<00:03,  6.83it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 51/77 [00:07<00:03,  6.85it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 52/77 [00:07<00:03,  6.83it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 53/77 [00:07<00:03,  6.92it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 54/77 [00:07<00:03,  6.92it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 55/77 [00:07<00:03,  6.97it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 56/77 [00:07<00:02,  7.06it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 57/77 [00:07<00:02,  7.07it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 58/77 [00:08<00:02,  7.21it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 59/77 [00:08<00:02,  7.23it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 60/77 [00:08<00:02,  7.24it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 61/77 [00:08<00:02,  7.20it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 62/77 [00:08<00:02,  7.08it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 63/77 [00:08<00:01,  7.14it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 64/77 [00:08<00:01,  7.14it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 65/77 [00:09<00:01,  7.19it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 66/77 [00:09<00:01,  7.33it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 67/77 [00:09<00:01,  7.25it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 68/77 [00:09<00:01,  7.14it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 69/77 [00:09<00:01,  6.93it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 70/77 [00:09<00:00,  7.02it/s]\u001b[A\n",
            "Predicting batches:  92%| | 71/77 [00:09<00:00,  7.05it/s]\u001b[A\n",
            "Predicting batches:  94%| | 72/77 [00:10<00:00,  7.00it/s]\u001b[A\n",
            "Predicting batches:  95%| | 73/77 [00:10<00:00,  6.95it/s]\u001b[A\n",
            "Predicting batches:  96%| | 74/77 [00:10<00:00,  7.05it/s]\u001b[A\n",
            "Predicting batches:  97%|| 75/77 [00:10<00:00,  7.13it/s]\u001b[A\n",
            "Predicting batches:  99%|| 76/77 [00:10<00:00,  7.17it/s]\u001b[A\n",
            "Processing subjects:  79%|  | 45/57 [06:08<02:59, 14.99s/it, philosophy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/311 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  19%|                 | 60/311 [00:00<00:00, 591.14it/s]\u001b[A\n",
            "Formatting batches:  39%|            | 120/311 [00:00<00:00, 588.44it/s]\u001b[A\n",
            "Formatting batches:  58%|        | 181/311 [00:00<00:00, 594.66it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 241/311 [00:00<00:00, 594.97it/s]\u001b[A\n",
            "Formatting batches:  97%|| 302/311 [00:00<00:00, 597.14it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/78 [00:00<00:05, 14.23it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/78 [00:00<00:05, 13.88it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/78 [00:00<00:05, 12.83it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/78 [00:00<00:05, 11.91it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 10/78 [00:00<00:05, 12.51it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/78 [00:00<00:05, 12.62it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 14/78 [00:01<00:05, 12.21it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 16/78 [00:01<00:05, 12.33it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 18/78 [00:01<00:04, 12.78it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 20/78 [00:01<00:04, 12.83it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 22/78 [00:01<00:04, 13.00it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 24/78 [00:01<00:04, 12.51it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 26/78 [00:02<00:04, 12.66it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 28/78 [00:02<00:03, 12.72it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 30/78 [00:02<00:03, 12.87it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 32/78 [00:02<00:03, 12.60it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 34/78 [00:02<00:03, 11.76it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 36/78 [00:02<00:03, 12.33it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 38/78 [00:03<00:03, 11.90it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 40/78 [00:03<00:03, 12.37it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 42/78 [00:03<00:02, 12.40it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 44/78 [00:03<00:02, 12.64it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 46/78 [00:03<00:02, 12.64it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 48/78 [00:03<00:02, 12.94it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 50/78 [00:03<00:02, 13.01it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 52/78 [00:04<00:01, 13.12it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 54/78 [00:04<00:01, 13.20it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 56/78 [00:04<00:01, 13.09it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 58/78 [00:04<00:01, 13.17it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 60/78 [00:04<00:01, 13.08it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 62/78 [00:04<00:01, 12.67it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 64/78 [00:05<00:01, 12.45it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 66/78 [00:05<00:00, 12.03it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 68/78 [00:05<00:00, 12.36it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 70/78 [00:05<00:00, 12.78it/s]\u001b[A\n",
            "Predicting batches:  92%| | 72/78 [00:05<00:00, 12.94it/s]\u001b[A\n",
            "Predicting batches:  95%| | 74/78 [00:05<00:00, 12.48it/s]\u001b[A\n",
            "Predicting batches:  97%|| 76/78 [00:06<00:00, 12.68it/s]\u001b[A\n",
            "Predicting batches: 100%|| 78/78 [00:06<00:00, 12.95it/s]\u001b[A\n",
            "Processing subjects:  81%|  | 46/57 [06:15<02:17, 12.50s/it, prehistory]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/324 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 53/324 [00:00<00:00, 516.31it/s]\u001b[A\n",
            "Formatting batches:  33%|             | 108/324 [00:00<00:00, 534.82it/s]\u001b[A\n",
            "Formatting batches:  50%|          | 162/324 [00:00<00:00, 535.24it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 216/324 [00:00<00:00, 520.35it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 271/324 [00:00<00:00, 528.80it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/81 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/81 [00:00<00:09,  8.52it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 2/81 [00:00<00:09,  8.74it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/81 [00:00<00:09,  8.23it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 4/81 [00:00<00:09,  8.13it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 5/81 [00:00<00:09,  8.36it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 6/81 [00:00<00:08,  8.51it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 7/81 [00:00<00:08,  8.37it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 8/81 [00:00<00:08,  8.49it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 9/81 [00:01<00:08,  8.58it/s]\u001b[A\n",
            "Predicting batches:  12%|                    | 10/81 [00:01<00:08,  8.64it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 11/81 [00:01<00:08,  8.41it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 12/81 [00:01<00:08,  8.50it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 13/81 [00:01<00:07,  8.57it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 14/81 [00:01<00:07,  8.41it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 15/81 [00:01<00:08,  8.19it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 16/81 [00:01<00:08,  8.00it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 17/81 [00:02<00:08,  7.77it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 18/81 [00:02<00:07,  8.06it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 19/81 [00:02<00:07,  8.03it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 20/81 [00:02<00:07,  8.22it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 21/81 [00:02<00:07,  8.35it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 22/81 [00:02<00:07,  8.22it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 23/81 [00:02<00:07,  8.04it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 24/81 [00:02<00:06,  8.23it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 25/81 [00:03<00:06,  8.13it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 26/81 [00:03<00:06,  8.07it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 27/81 [00:03<00:06,  8.25it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 28/81 [00:03<00:06,  8.05it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 29/81 [00:03<00:06,  8.01it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 30/81 [00:03<00:06,  7.88it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 31/81 [00:03<00:06,  7.94it/s]\u001b[A\n",
            "Predicting batches:  40%|              | 32/81 [00:03<00:06,  7.95it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 33/81 [00:04<00:05,  8.18it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 34/81 [00:04<00:05,  8.13it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 35/81 [00:04<00:05,  8.30it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 36/81 [00:04<00:05,  8.23it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 37/81 [00:04<00:05,  8.14it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 38/81 [00:04<00:05,  8.31it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 39/81 [00:04<00:05,  8.11it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 40/81 [00:04<00:04,  8.30it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 41/81 [00:04<00:04,  8.23it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 42/81 [00:05<00:04,  8.15it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 43/81 [00:05<00:04,  8.30it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 44/81 [00:05<00:04,  8.07it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 45/81 [00:05<00:04,  8.26it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 46/81 [00:05<00:04,  8.17it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 47/81 [00:05<00:04,  7.90it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 48/81 [00:05<00:04,  7.90it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 49/81 [00:06<00:04,  7.70it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 50/81 [00:06<00:03,  7.77it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 51/81 [00:06<00:03,  7.85it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 52/81 [00:06<00:03,  8.11it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 53/81 [00:06<00:03,  8.00it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 54/81 [00:06<00:03,  7.99it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 55/81 [00:06<00:03,  8.01it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 56/81 [00:06<00:03,  7.93it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 57/81 [00:06<00:02,  8.16it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 58/81 [00:07<00:02,  8.31it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 59/81 [00:07<00:02,  8.11it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 60/81 [00:07<00:02,  8.08it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 61/81 [00:07<00:02,  8.08it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 62/81 [00:07<00:02,  8.27it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 63/81 [00:07<00:02,  8.41it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 64/81 [00:07<00:02,  8.49it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 65/81 [00:07<00:01,  8.36it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 66/81 [00:08<00:01,  8.48it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 67/81 [00:08<00:01,  8.54it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 68/81 [00:08<00:01,  8.39it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 69/81 [00:08<00:01,  8.25it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 70/81 [00:08<00:01,  8.40it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 71/81 [00:08<00:01,  8.25it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 72/81 [00:08<00:01,  8.19it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 73/81 [00:08<00:00,  8.15it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 74/81 [00:09<00:00,  8.34it/s]\u001b[A\n",
            "Predicting batches:  93%| | 75/81 [00:09<00:00,  8.47it/s]\u001b[A\n",
            "Predicting batches:  94%| | 76/81 [00:09<00:00,  8.58it/s]\u001b[A\n",
            "Predicting batches:  95%| | 77/81 [00:09<00:00,  8.63it/s]\u001b[A\n",
            "Predicting batches:  96%|| 78/81 [00:09<00:00,  8.65it/s]\u001b[A\n",
            "Predicting batches:  98%|| 79/81 [00:09<00:00,  8.78it/s]\u001b[A\n",
            "Predicting batches:  99%|| 80/81 [00:09<00:00,  8.53it/s]\u001b[A\n",
            "Predicting batches: 100%|| 81/81 [00:09<00:00,  8.34it/s]\u001b[A\n",
            "Processing subjects:  82%|| 47/57 [06:25<01:58, 11.89s/it, professional account\u001b[A\n",
            "Formatting batches:   0%|                               | 0/282 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  17%|                 | 49/282 [00:00<00:00, 486.00it/s]\u001b[A\n",
            "Formatting batches:  36%|            | 101/282 [00:00<00:00, 504.61it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 152/282 [00:00<00:00, 506.42it/s]\u001b[A\n",
            "Formatting batches:  72%|     | 203/282 [00:00<00:00, 507.64it/s]\u001b[A\n",
            "Formatting batches:  90%|  | 254/282 [00:00<00:00, 503.47it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/71 [00:00<00:10,  6.83it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/71 [00:00<00:10,  6.70it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/71 [00:00<00:10,  6.59it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/71 [00:00<00:10,  6.42it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/71 [00:00<00:10,  6.24it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 6/71 [00:00<00:10,  6.23it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/71 [00:01<00:10,  6.07it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 8/71 [00:01<00:10,  6.19it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 9/71 [00:01<00:10,  6.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 10/71 [00:01<00:09,  6.28it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 11/71 [00:01<00:09,  6.35it/s]\u001b[A\n",
            "Predicting batches:  17%|                   | 12/71 [00:01<00:09,  6.39it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 13/71 [00:02<00:09,  6.32it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 14/71 [00:02<00:08,  6.38it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 15/71 [00:02<00:08,  6.31it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 16/71 [00:02<00:08,  6.35it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 17/71 [00:02<00:08,  6.31it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 18/71 [00:02<00:08,  6.20it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 19/71 [00:03<00:08,  6.18it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 20/71 [00:03<00:08,  6.25it/s]\u001b[A\n",
            "Predicting batches:  30%|                | 21/71 [00:03<00:07,  6.31it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 22/71 [00:03<00:07,  6.28it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 23/71 [00:03<00:07,  6.25it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 24/71 [00:03<00:07,  6.42it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 25/71 [00:03<00:07,  6.55it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 26/71 [00:04<00:07,  6.35it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 27/71 [00:04<00:06,  6.38it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 28/71 [00:04<00:06,  6.53it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 29/71 [00:04<00:06,  6.51it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 30/71 [00:04<00:06,  6.51it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 31/71 [00:04<00:06,  6.61it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 32/71 [00:05<00:05,  6.70it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 33/71 [00:05<00:05,  6.44it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 34/71 [00:05<00:05,  6.45it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 35/71 [00:05<00:05,  6.45it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 36/71 [00:05<00:05,  6.47it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 37/71 [00:05<00:05,  6.37it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 38/71 [00:05<00:05,  6.51it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 39/71 [00:06<00:04,  6.40it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 40/71 [00:06<00:04,  6.33it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 41/71 [00:06<00:04,  6.35it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 42/71 [00:06<00:04,  6.37it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 43/71 [00:06<00:04,  6.51it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 44/71 [00:06<00:04,  6.48it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 45/71 [00:07<00:04,  6.30it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 46/71 [00:07<00:03,  6.36it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 47/71 [00:07<00:03,  6.32it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 48/71 [00:07<00:03,  6.36it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 49/71 [00:07<00:03,  6.41it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 50/71 [00:07<00:03,  6.35it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 51/71 [00:07<00:03,  6.40it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 52/71 [00:08<00:03,  6.33it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 53/71 [00:08<00:02,  6.38it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 54/71 [00:08<00:02,  6.24it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 55/71 [00:08<00:02,  6.41it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 56/71 [00:08<00:02,  6.33it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 57/71 [00:08<00:02,  6.37it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 58/71 [00:09<00:02,  6.39it/s]\u001b[A\n",
            "Predicting batches:  83%|    | 59/71 [00:09<00:01,  6.41it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 60/71 [00:09<00:01,  6.43it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 61/71 [00:09<00:01,  6.34it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 62/71 [00:09<00:01,  6.29it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 63/71 [00:09<00:01,  6.35it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 64/71 [00:10<00:01,  6.22it/s]\u001b[A\n",
            "Predicting batches:  92%|  | 65/71 [00:10<00:00,  6.29it/s]\u001b[A\n",
            "Predicting batches:  93%| | 66/71 [00:10<00:00,  6.27it/s]\u001b[A\n",
            "Predicting batches:  94%| | 67/71 [00:10<00:00,  6.43it/s]\u001b[A\n",
            "Predicting batches:  96%| | 68/71 [00:10<00:00,  6.21it/s]\u001b[A\n",
            "Predicting batches:  97%|| 69/71 [00:10<00:00,  6.28it/s]\u001b[A\n",
            "Predicting batches:  99%|| 70/71 [00:11<00:00,  6.25it/s]\u001b[A\n",
            "Processing subjects:  84%|| 48/57 [06:37<01:46, 11.82s/it, professional law]\u001b[A\n",
            "Formatting batches:   0%|                              | 0/1534 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   2%|                   | 37/1534 [00:00<00:04, 363.60it/s]\u001b[A\n",
            "Formatting batches:   5%|                   | 74/1534 [00:00<00:04, 356.04it/s]\u001b[A\n",
            "Formatting batches:   7%|                 | 110/1534 [00:00<00:04, 344.98it/s]\u001b[A\n",
            "Formatting batches:   9%|                 | 145/1534 [00:00<00:04, 343.26it/s]\u001b[A\n",
            "Formatting batches:  12%|                | 182/1534 [00:00<00:03, 350.56it/s]\u001b[A\n",
            "Formatting batches:  14%|                | 218/1534 [00:00<00:03, 348.28it/s]\u001b[A\n",
            "Formatting batches:  16%|               | 253/1534 [00:00<00:03, 346.91it/s]\u001b[A\n",
            "Formatting batches:  19%|               | 288/1534 [00:00<00:03, 344.09it/s]\u001b[A\n",
            "Formatting batches:  21%|               | 324/1534 [00:00<00:03, 346.20it/s]\u001b[A\n",
            "Formatting batches:  23%|              | 359/1534 [00:01<00:03, 346.16it/s]\u001b[A\n",
            "Formatting batches:  26%|              | 394/1534 [00:01<00:03, 345.12it/s]\u001b[A\n",
            "Formatting batches:  28%|             | 429/1534 [00:01<00:03, 346.02it/s]\u001b[A\n",
            "Formatting batches:  30%|             | 465/1534 [00:01<00:03, 349.32it/s]\u001b[A\n",
            "Formatting batches:  33%|            | 500/1534 [00:01<00:02, 346.39it/s]\u001b[A\n",
            "Formatting batches:  35%|            | 537/1534 [00:01<00:02, 351.06it/s]\u001b[A\n",
            "Formatting batches:  37%|            | 573/1534 [00:01<00:02, 350.56it/s]\u001b[A\n",
            "Formatting batches:  40%|           | 610/1534 [00:01<00:02, 353.53it/s]\u001b[A\n",
            "Formatting batches:  42%|           | 646/1534 [00:01<00:02, 350.20it/s]\u001b[A\n",
            "Formatting batches:  44%|          | 682/1534 [00:01<00:02, 351.22it/s]\u001b[A\n",
            "Formatting batches:  47%|          | 718/1534 [00:02<00:02, 351.54it/s]\u001b[A\n",
            "Formatting batches:  49%|         | 755/1534 [00:02<00:02, 352.86it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 791/1534 [00:02<00:02, 353.69it/s]\u001b[A\n",
            "Formatting batches:  54%|        | 827/1534 [00:02<00:01, 353.53it/s]\u001b[A\n",
            "Formatting batches:  56%|        | 863/1534 [00:02<00:01, 353.46it/s]\u001b[A\n",
            "Formatting batches:  59%|       | 899/1534 [00:02<00:01, 354.39it/s]\u001b[A\n",
            "Formatting batches:  61%|       | 935/1534 [00:02<00:01, 354.53it/s]\u001b[A\n",
            "Formatting batches:  63%|       | 971/1534 [00:02<00:01, 355.77it/s]\u001b[A\n",
            "Formatting batches:  66%|      | 1007/1534 [00:02<00:01, 356.51it/s]\u001b[A\n",
            "Formatting batches:  68%|     | 1043/1534 [00:02<00:01, 354.89it/s]\u001b[A\n",
            "Formatting batches:  70%|     | 1079/1534 [00:03<00:01, 355.88it/s]\u001b[A\n",
            "Formatting batches:  73%|     | 1115/1534 [00:03<00:01, 351.08it/s]\u001b[A\n",
            "Formatting batches:  75%|    | 1151/1534 [00:03<00:01, 352.99it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 1187/1534 [00:03<00:00, 353.73it/s]\u001b[A\n",
            "Formatting batches:  80%|   | 1223/1534 [00:03<00:00, 353.95it/s]\u001b[A\n",
            "Formatting batches:  82%|   | 1259/1534 [00:03<00:00, 351.42it/s]\u001b[A\n",
            "Formatting batches:  84%|  | 1295/1534 [00:03<00:00, 352.17it/s]\u001b[A\n",
            "Formatting batches:  87%|  | 1331/1534 [00:03<00:00, 352.55it/s]\u001b[A\n",
            "Formatting batches:  89%|  | 1367/1534 [00:03<00:00, 353.93it/s]\u001b[A\n",
            "Formatting batches:  91%| | 1403/1534 [00:03<00:00, 351.30it/s]\u001b[A\n",
            "Formatting batches:  94%| | 1439/1534 [00:04<00:00, 353.77it/s]\u001b[A\n",
            "Formatting batches:  96%|| 1475/1534 [00:04<00:00, 353.06it/s]\u001b[A\n",
            "Formatting batches:  99%|| 1511/1534 [00:04<00:00, 353.20it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/384 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   0%|                       | 1/384 [00:00<02:44,  2.33it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 2/384 [00:00<02:46,  2.30it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 3/384 [00:01<02:54,  2.18it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 4/384 [00:01<02:51,  2.21it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 5/384 [00:02<02:48,  2.25it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 6/384 [00:02<02:49,  2.22it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 7/384 [00:03<02:50,  2.21it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 8/384 [00:03<02:48,  2.23it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 9/384 [00:04<02:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 10/384 [00:04<02:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 11/384 [00:04<02:44,  2.27it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 12/384 [00:05<02:45,  2.25it/s]\u001b[A\n",
            "Predicting batches:   3%|                     | 13/384 [00:05<02:49,  2.19it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 14/384 [00:06<02:47,  2.21it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 15/384 [00:06<02:47,  2.21it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 16/384 [00:07<02:47,  2.20it/s]\u001b[A\n",
            "Predicting batches:   4%|                     | 17/384 [00:07<02:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 18/384 [00:08<02:55,  2.08it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 19/384 [00:08<02:48,  2.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 20/384 [00:09<02:47,  2.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                    | 21/384 [00:09<02:49,  2.14it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 22/384 [00:09<02:43,  2.22it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 23/384 [00:10<02:48,  2.14it/s]\u001b[A\n",
            "Predicting batches:   6%|                    | 24/384 [00:10<02:45,  2.17it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 25/384 [00:11<02:52,  2.08it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 26/384 [00:11<02:46,  2.15it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 27/384 [00:12<02:42,  2.20it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 28/384 [00:12<02:40,  2.22it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 29/384 [00:13<02:39,  2.23it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 30/384 [00:13<02:34,  2.29it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 31/384 [00:14<02:36,  2.25it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 32/384 [00:14<02:38,  2.22it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 33/384 [00:14<02:37,  2.23it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 34/384 [00:15<02:32,  2.29it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 35/384 [00:15<02:35,  2.24it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 36/384 [00:16<02:32,  2.29it/s]\u001b[A\n",
            "Predicting batches:  10%|                    | 37/384 [00:16<02:32,  2.28it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 38/384 [00:17<02:37,  2.20it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 39/384 [00:17<02:35,  2.22it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 40/384 [00:18<02:34,  2.23it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 41/384 [00:18<02:32,  2.26it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 42/384 [00:19<02:34,  2.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 43/384 [00:19<02:33,  2.22it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 44/384 [00:19<02:36,  2.17it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 45/384 [00:20<02:32,  2.23it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 46/384 [00:20<02:29,  2.26it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 47/384 [00:21<02:33,  2.20it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 48/384 [00:21<02:29,  2.24it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 49/384 [00:22<02:35,  2.16it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 50/384 [00:22<02:31,  2.21it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 51/384 [00:23<02:32,  2.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 52/384 [00:23<02:31,  2.19it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 53/384 [00:23<02:27,  2.24it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 54/384 [00:24<02:28,  2.23it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 55/384 [00:24<02:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 56/384 [00:25<02:27,  2.23it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 57/384 [00:25<02:25,  2.25it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 58/384 [00:26<02:23,  2.28it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 59/384 [00:26<02:21,  2.29it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 60/384 [00:27<02:26,  2.21it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 61/384 [00:27<02:23,  2.24it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 62/384 [00:28<02:35,  2.07it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 63/384 [00:28<02:31,  2.12it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 64/384 [00:29<02:29,  2.15it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 65/384 [00:29<02:30,  2.12it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 66/384 [00:29<02:25,  2.19it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 67/384 [00:30<02:22,  2.22it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 68/384 [00:30<02:21,  2.24it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 69/384 [00:31<02:25,  2.16it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 70/384 [00:31<02:23,  2.19it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 71/384 [00:32<02:24,  2.17it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 72/384 [00:32<02:29,  2.09it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 73/384 [00:33<02:26,  2.12it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 74/384 [00:33<02:21,  2.20it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 75/384 [00:34<02:20,  2.20it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 76/384 [00:34<02:16,  2.26it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 77/384 [00:34<02:14,  2.28it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 78/384 [00:35<02:13,  2.29it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 79/384 [00:35<02:17,  2.22it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 80/384 [00:36<02:21,  2.14it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 81/384 [00:36<02:20,  2.15it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 82/384 [00:37<02:23,  2.11it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 83/384 [00:37<02:23,  2.10it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 84/384 [00:38<02:25,  2.06it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 85/384 [00:38<02:21,  2.11it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 86/384 [00:39<02:16,  2.18it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 87/384 [00:39<02:13,  2.22it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 88/384 [00:40<02:15,  2.19it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 89/384 [00:40<02:13,  2.21it/s]\u001b[A\n",
            "Predicting batches:  23%|                | 90/384 [00:40<02:17,  2.13it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 91/384 [00:41<02:15,  2.17it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 92/384 [00:41<02:12,  2.20it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 93/384 [00:42<02:10,  2.23it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 94/384 [00:42<02:11,  2.21it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 95/384 [00:43<02:08,  2.25it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 96/384 [00:43<02:08,  2.24it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 97/384 [00:44<02:09,  2.22it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 98/384 [00:44<02:07,  2.25it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 99/384 [00:44<02:08,  2.23it/s]\u001b[A\n",
            "Predicting batches:  26%|               | 100/384 [00:45<02:07,  2.22it/s]\u001b[A\n",
            "Predicting batches:  26%|               | 101/384 [00:45<02:06,  2.23it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 102/384 [00:46<02:04,  2.27it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 103/384 [00:46<02:04,  2.26it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 104/384 [00:47<02:04,  2.24it/s]\u001b[A\n",
            "Predicting batches:  27%|               | 105/384 [00:47<02:02,  2.28it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 106/384 [00:48<02:02,  2.27it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 107/384 [00:48<02:02,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 108/384 [00:48<02:01,  2.26it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 109/384 [00:49<02:02,  2.25it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 110/384 [00:49<02:03,  2.22it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 111/384 [00:50<02:02,  2.23it/s]\u001b[A\n",
            "Predicting batches:  29%|              | 112/384 [00:50<02:06,  2.15it/s]\u001b[A\n",
            "Predicting batches:  29%|              | 113/384 [00:51<02:09,  2.10it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 114/384 [00:51<02:06,  2.14it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 115/384 [00:52<02:05,  2.15it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 116/384 [00:52<02:02,  2.19it/s]\u001b[A\n",
            "Predicting batches:  30%|              | 117/384 [00:53<02:02,  2.18it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 118/384 [00:53<02:00,  2.20it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 119/384 [00:53<01:58,  2.24it/s]\u001b[A\n",
            "Predicting batches:  31%|              | 120/384 [00:54<01:58,  2.22it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 121/384 [00:54<01:59,  2.19it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 122/384 [00:55<01:57,  2.23it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 123/384 [00:55<01:55,  2.26it/s]\u001b[A\n",
            "Predicting batches:  32%|              | 124/384 [00:56<01:57,  2.22it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 125/384 [00:56<01:54,  2.26it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 126/384 [00:57<01:53,  2.27it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 127/384 [00:57<01:56,  2.21it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 128/384 [00:58<01:56,  2.20it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 129/384 [00:58<01:54,  2.23it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 130/384 [00:58<01:52,  2.26it/s]\u001b[A\n",
            "Predicting batches:  34%|             | 131/384 [00:59<01:51,  2.28it/s]\u001b[A\n",
            "Predicting batches:  34%|             | 132/384 [00:59<01:50,  2.27it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 133/384 [01:00<01:49,  2.29it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 134/384 [01:00<01:48,  2.31it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 135/384 [01:01<01:47,  2.31it/s]\u001b[A\n",
            "Predicting batches:  35%|             | 136/384 [01:01<01:48,  2.29it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 137/384 [01:01<01:48,  2.27it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 138/384 [01:02<01:50,  2.24it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 139/384 [01:02<01:48,  2.26it/s]\u001b[A\n",
            "Predicting batches:  36%|             | 140/384 [01:03<01:50,  2.20it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 141/384 [01:03<01:47,  2.26it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 142/384 [01:04<01:48,  2.23it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 143/384 [01:04<01:52,  2.14it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 144/384 [01:05<01:51,  2.15it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 145/384 [01:05<01:50,  2.17it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 146/384 [01:06<01:49,  2.17it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 147/384 [01:06<01:49,  2.17it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 148/384 [01:07<01:48,  2.18it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 149/384 [01:07<01:45,  2.23it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 150/384 [01:07<01:43,  2.26it/s]\u001b[A\n",
            "Predicting batches:  39%|            | 151/384 [01:08<01:45,  2.20it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 152/384 [01:08<01:45,  2.20it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 153/384 [01:09<01:45,  2.19it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 154/384 [01:09<01:47,  2.15it/s]\u001b[A\n",
            "Predicting batches:  40%|            | 155/384 [01:10<01:48,  2.11it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 156/384 [01:10<01:46,  2.14it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 157/384 [01:11<01:47,  2.12it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 158/384 [01:11<01:43,  2.17it/s]\u001b[A\n",
            "Predicting batches:  41%|            | 159/384 [01:12<01:46,  2.12it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 160/384 [01:12<01:43,  2.17it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 161/384 [01:12<01:40,  2.22it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 162/384 [01:13<01:41,  2.20it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 163/384 [01:13<01:40,  2.19it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 164/384 [01:14<01:42,  2.15it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 165/384 [01:14<01:40,  2.18it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 166/384 [01:15<01:40,  2.17it/s]\u001b[A\n",
            "Predicting batches:  43%|           | 167/384 [01:15<01:41,  2.13it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 168/384 [01:16<01:40,  2.16it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 169/384 [01:16<01:37,  2.22it/s]\u001b[A\n",
            "Predicting batches:  44%|           | 170/384 [01:17<01:38,  2.17it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 171/384 [01:17<01:36,  2.22it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 172/384 [01:18<01:35,  2.22it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 173/384 [01:18<01:36,  2.19it/s]\u001b[A\n",
            "Predicting batches:  45%|           | 174/384 [01:18<01:34,  2.21it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 175/384 [01:19<01:34,  2.22it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 176/384 [01:19<01:32,  2.26it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 177/384 [01:20<01:34,  2.20it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 178/384 [01:20<01:32,  2.23it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 179/384 [01:21<01:32,  2.22it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 180/384 [01:21<01:30,  2.25it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 181/384 [01:22<01:30,  2.24it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 182/384 [01:22<01:30,  2.24it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 183/384 [01:22<01:30,  2.23it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 184/384 [01:23<01:30,  2.20it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 185/384 [01:23<01:29,  2.22it/s]\u001b[A\n",
            "Predicting batches:  48%|          | 186/384 [01:24<01:28,  2.25it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 187/384 [01:24<01:27,  2.25it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 188/384 [01:25<01:29,  2.19it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 189/384 [01:25<01:28,  2.21it/s]\u001b[A\n",
            "Predicting batches:  49%|          | 190/384 [01:26<01:30,  2.15it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 191/384 [01:26<01:27,  2.20it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 192/384 [01:27<01:26,  2.22it/s]\u001b[A\n",
            "Predicting batches:  50%|          | 193/384 [01:27<01:28,  2.16it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 194/384 [01:27<01:27,  2.18it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 195/384 [01:28<01:25,  2.22it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 196/384 [01:28<01:22,  2.28it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 197/384 [01:29<01:25,  2.20it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 198/384 [01:29<01:24,  2.19it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 199/384 [01:30<01:23,  2.23it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 200/384 [01:30<01:25,  2.15it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 201/384 [01:31<01:24,  2.15it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 202/384 [01:31<01:22,  2.21it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 203/384 [01:32<01:21,  2.22it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 204/384 [01:32<01:22,  2.18it/s]\u001b[A\n",
            "Predicting batches:  53%|         | 205/384 [01:32<01:20,  2.22it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 206/384 [01:33<01:21,  2.20it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 207/384 [01:33<01:22,  2.15it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 208/384 [01:34<01:20,  2.20it/s]\u001b[A\n",
            "Predicting batches:  54%|         | 209/384 [01:34<01:19,  2.19it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 210/384 [01:35<01:17,  2.25it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 211/384 [01:35<01:16,  2.25it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 212/384 [01:36<01:17,  2.23it/s]\u001b[A\n",
            "Predicting batches:  55%|         | 213/384 [01:36<01:17,  2.21it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 214/384 [01:37<01:15,  2.24it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 215/384 [01:37<01:19,  2.13it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 216/384 [01:37<01:16,  2.19it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 217/384 [01:38<01:16,  2.19it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 218/384 [01:38<01:14,  2.23it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 219/384 [01:39<01:12,  2.28it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 220/384 [01:39<01:11,  2.30it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 221/384 [01:40<01:12,  2.26it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 222/384 [01:40<01:12,  2.24it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 223/384 [01:41<01:11,  2.26it/s]\u001b[A\n",
            "Predicting batches:  58%|        | 224/384 [01:41<01:09,  2.29it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 225/384 [01:41<01:11,  2.22it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 226/384 [01:42<01:11,  2.21it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 227/384 [01:42<01:10,  2.23it/s]\u001b[A\n",
            "Predicting batches:  59%|        | 228/384 [01:43<01:08,  2.28it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 229/384 [01:43<01:07,  2.31it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 230/384 [01:44<01:09,  2.23it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 231/384 [01:44<01:08,  2.24it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 232/384 [01:45<01:06,  2.27it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 233/384 [01:45<01:06,  2.28it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 234/384 [01:45<01:05,  2.27it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 235/384 [01:46<01:06,  2.25it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 236/384 [01:46<01:06,  2.24it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 237/384 [01:47<01:04,  2.27it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 238/384 [01:47<01:08,  2.12it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 239/384 [01:48<01:08,  2.11it/s]\u001b[A\n",
            "Predicting batches:  62%|       | 240/384 [01:48<01:07,  2.13it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 241/384 [01:49<01:06,  2.15it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 242/384 [01:49<01:05,  2.18it/s]\u001b[A\n",
            "Predicting batches:  63%|       | 243/384 [01:50<01:03,  2.22it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 244/384 [01:50<01:05,  2.14it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 245/384 [01:50<01:02,  2.22it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 246/384 [01:51<01:01,  2.23it/s]\u001b[A\n",
            "Predicting batches:  64%|       | 247/384 [01:51<01:01,  2.24it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 248/384 [01:52<01:00,  2.26it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 249/384 [01:52<01:00,  2.25it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 250/384 [01:53<01:03,  2.12it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 251/384 [01:53<01:02,  2.13it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 252/384 [01:54<01:03,  2.09it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 253/384 [01:54<01:00,  2.15it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 254/384 [01:55<01:01,  2.13it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 255/384 [01:55<00:59,  2.16it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 256/384 [01:56<01:00,  2.13it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 257/384 [01:56<00:58,  2.18it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 258/384 [01:56<00:56,  2.23it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 259/384 [01:57<00:56,  2.21it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 260/384 [01:57<00:55,  2.24it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 261/384 [01:58<00:55,  2.21it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 262/384 [01:58<00:54,  2.25it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 263/384 [01:59<00:53,  2.28it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 264/384 [01:59<00:53,  2.26it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 265/384 [02:00<00:53,  2.23it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 266/384 [02:00<00:53,  2.20it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 267/384 [02:01<00:55,  2.13it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 268/384 [02:01<00:54,  2.14it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 269/384 [02:01<00:54,  2.11it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 270/384 [02:02<00:53,  2.12it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 271/384 [02:02<00:52,  2.14it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 272/384 [02:03<00:51,  2.17it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 273/384 [02:03<00:52,  2.13it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 274/384 [02:04<00:51,  2.15it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 275/384 [02:04<00:50,  2.14it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 276/384 [02:05<00:49,  2.16it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 277/384 [02:05<00:49,  2.18it/s]\u001b[A\n",
            "Predicting batches:  72%|     | 278/384 [02:06<00:48,  2.19it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 279/384 [02:06<00:47,  2.20it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 280/384 [02:07<00:48,  2.15it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 281/384 [02:07<00:48,  2.14it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 282/384 [02:07<00:47,  2.16it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 283/384 [02:08<00:45,  2.20it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 284/384 [02:08<00:44,  2.25it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 285/384 [02:09<00:43,  2.29it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 286/384 [02:09<00:44,  2.18it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 287/384 [02:10<00:43,  2.23it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 288/384 [02:10<00:43,  2.20it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 289/384 [02:11<00:43,  2.19it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 290/384 [02:11<00:42,  2.24it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 291/384 [02:11<00:41,  2.26it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 292/384 [02:12<00:40,  2.26it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 293/384 [02:12<00:40,  2.27it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 294/384 [02:13<00:39,  2.28it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 295/384 [02:13<00:39,  2.26it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 296/384 [02:14<00:38,  2.28it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 297/384 [02:14<00:37,  2.31it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 298/384 [02:15<00:37,  2.29it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 299/384 [02:15<00:37,  2.30it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 300/384 [02:15<00:36,  2.30it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 301/384 [02:16<00:36,  2.26it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 302/384 [02:16<00:36,  2.27it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 303/384 [02:17<00:37,  2.13it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 304/384 [02:17<00:38,  2.07it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 305/384 [02:18<00:38,  2.06it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 306/384 [02:18<00:36,  2.16it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 307/384 [02:19<00:35,  2.19it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 308/384 [02:19<00:34,  2.23it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 309/384 [02:20<00:33,  2.24it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 310/384 [02:20<00:33,  2.20it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 311/384 [02:21<00:33,  2.20it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 312/384 [02:21<00:34,  2.11it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 313/384 [02:21<00:32,  2.15it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 314/384 [02:22<00:32,  2.18it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 315/384 [02:22<00:31,  2.22it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 316/384 [02:23<00:30,  2.26it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 317/384 [02:23<00:29,  2.28it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 318/384 [02:24<00:29,  2.25it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 319/384 [02:24<00:28,  2.27it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 320/384 [02:25<00:28,  2.25it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 321/384 [02:25<00:28,  2.23it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 322/384 [02:25<00:27,  2.22it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 323/384 [02:26<00:27,  2.25it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 324/384 [02:26<00:28,  2.07it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 325/384 [02:27<00:28,  2.05it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 326/384 [02:27<00:27,  2.10it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 327/384 [02:28<00:26,  2.19it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 328/384 [02:28<00:25,  2.21it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 329/384 [02:29<00:24,  2.22it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 330/384 [02:29<00:24,  2.20it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 331/384 [02:30<00:24,  2.18it/s]\u001b[A\n",
            "Predicting batches:  86%|  | 332/384 [02:30<00:24,  2.17it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 333/384 [02:31<00:24,  2.10it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 334/384 [02:31<00:23,  2.11it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 335/384 [02:32<00:22,  2.19it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 336/384 [02:32<00:22,  2.13it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 337/384 [02:33<00:22,  2.07it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 338/384 [02:33<00:21,  2.16it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 339/384 [02:33<00:20,  2.18it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 340/384 [02:34<00:20,  2.18it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 341/384 [02:34<00:19,  2.20it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 342/384 [02:35<00:18,  2.22it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 343/384 [02:35<00:18,  2.25it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 344/384 [02:36<00:17,  2.27it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 345/384 [02:36<00:17,  2.28it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 346/384 [02:37<00:17,  2.22it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 347/384 [02:37<00:16,  2.21it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 348/384 [02:37<00:16,  2.21it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 349/384 [02:38<00:15,  2.23it/s]\u001b[A\n",
            "Predicting batches:  91%| | 350/384 [02:38<00:15,  2.24it/s]\u001b[A\n",
            "Predicting batches:  91%| | 351/384 [02:39<00:14,  2.21it/s]\u001b[A\n",
            "Predicting batches:  92%| | 352/384 [02:39<00:14,  2.15it/s]\u001b[A\n",
            "Predicting batches:  92%| | 353/384 [02:40<00:14,  2.18it/s]\u001b[A\n",
            "Predicting batches:  92%| | 354/384 [02:40<00:13,  2.19it/s]\u001b[A\n",
            "Predicting batches:  92%| | 355/384 [02:41<00:13,  2.12it/s]\u001b[A\n",
            "Predicting batches:  93%| | 356/384 [02:41<00:12,  2.16it/s]\u001b[A\n",
            "Predicting batches:  93%| | 357/384 [02:42<00:12,  2.17it/s]\u001b[A\n",
            "Predicting batches:  93%| | 358/384 [02:42<00:11,  2.21it/s]\u001b[A\n",
            "Predicting batches:  93%| | 359/384 [02:42<00:11,  2.22it/s]\u001b[A\n",
            "Predicting batches:  94%| | 360/384 [02:43<00:10,  2.21it/s]\u001b[A\n",
            "Predicting batches:  94%| | 361/384 [02:43<00:10,  2.17it/s]\u001b[A\n",
            "Predicting batches:  94%| | 362/384 [02:44<00:09,  2.21it/s]\u001b[A\n",
            "Predicting batches:  95%| | 363/384 [02:44<00:09,  2.24it/s]\u001b[A\n",
            "Predicting batches:  95%| | 364/384 [02:45<00:08,  2.26it/s]\u001b[A\n",
            "Predicting batches:  95%| | 365/384 [02:45<00:08,  2.28it/s]\u001b[A\n",
            "Predicting batches:  95%| | 366/384 [02:46<00:07,  2.26it/s]\u001b[A\n",
            "Predicting batches:  96%| | 367/384 [02:46<00:07,  2.26it/s]\u001b[A\n",
            "Predicting batches:  96%|| 368/384 [02:46<00:07,  2.28it/s]\u001b[A\n",
            "Predicting batches:  96%|| 369/384 [02:47<00:06,  2.25it/s]\u001b[A\n",
            "Predicting batches:  96%|| 370/384 [02:47<00:06,  2.29it/s]\u001b[A\n",
            "Predicting batches:  97%|| 371/384 [02:48<00:05,  2.33it/s]\u001b[A\n",
            "Predicting batches:  97%|| 372/384 [02:48<00:05,  2.12it/s]\u001b[A\n",
            "Predicting batches:  97%|| 373/384 [02:49<00:05,  2.17it/s]\u001b[A\n",
            "Predicting batches:  97%|| 374/384 [02:49<00:04,  2.14it/s]\u001b[A\n",
            "Predicting batches:  98%|| 375/384 [02:50<00:04,  2.14it/s]\u001b[A\n",
            "Predicting batches:  98%|| 376/384 [02:50<00:03,  2.19it/s]\u001b[A\n",
            "Predicting batches:  98%|| 377/384 [02:51<00:03,  2.14it/s]\u001b[A\n",
            "Predicting batches:  98%|| 378/384 [02:51<00:02,  2.16it/s]\u001b[A\n",
            "Predicting batches:  99%|| 379/384 [02:51<00:02,  2.19it/s]\u001b[A\n",
            "Predicting batches:  99%|| 380/384 [02:52<00:01,  2.22it/s]\u001b[A\n",
            "Predicting batches:  99%|| 381/384 [02:52<00:01,  2.18it/s]\u001b[A\n",
            "Predicting batches:  99%|| 382/384 [02:53<00:00,  2.18it/s]\u001b[A\n",
            "Predicting batches: 100%|| 383/384 [02:53<00:00,  2.20it/s]\u001b[A\n",
            "Predicting batches: 100%|| 384/384 [02:54<00:00,  2.61it/s]\u001b[A\n",
            "Processing subjects:  86%|| 49/57 [09:35<08:14, 61.79s/it, professional medicin\u001b[A\n",
            "Formatting batches:   0%|                               | 0/272 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 43/272 [00:00<00:00, 422.06it/s]\u001b[A\n",
            "Formatting batches:  32%|              | 87/272 [00:00<00:00, 431.60it/s]\u001b[A\n",
            "Formatting batches:  48%|          | 131/272 [00:00<00:00, 433.74it/s]\u001b[A\n",
            "Formatting batches:  64%|       | 175/272 [00:00<00:00, 431.63it/s]\u001b[A\n",
            "Formatting batches:  81%|    | 219/272 [00:00<00:00, 427.78it/s]\u001b[A\n",
            "Formatting batches:  96%|| 262/272 [00:00<00:00, 427.39it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/68 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                       | 1/68 [00:00<00:18,  3.69it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/68 [00:00<00:18,  3.56it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 3/68 [00:00<00:18,  3.61it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/68 [00:01<00:17,  3.69it/s]\u001b[A\n",
            "Predicting batches:   7%|                      | 5/68 [00:01<00:18,  3.44it/s]\u001b[A\n",
            "Predicting batches:   9%|                      | 6/68 [00:01<00:18,  3.35it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 7/68 [00:02<00:17,  3.45it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 8/68 [00:02<00:17,  3.50it/s]\u001b[A\n",
            "Predicting batches:  13%|                    | 9/68 [00:02<00:17,  3.40it/s]\u001b[A\n",
            "Predicting batches:  15%|                   | 10/68 [00:02<00:16,  3.45it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 11/68 [00:03<00:16,  3.51it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 12/68 [00:03<00:15,  3.61it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 13/68 [00:03<00:15,  3.61it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 14/68 [00:03<00:15,  3.49it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 15/68 [00:04<00:15,  3.52it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 16/68 [00:04<00:14,  3.50it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 17/68 [00:04<00:14,  3.45it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 18/68 [00:05<00:14,  3.52it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 19/68 [00:05<00:13,  3.61it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 20/68 [00:05<00:12,  3.69it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 21/68 [00:05<00:12,  3.74it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 22/68 [00:06<00:11,  3.85it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 23/68 [00:06<00:11,  3.76it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 24/68 [00:06<00:11,  3.70it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 25/68 [00:07<00:11,  3.59it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 26/68 [00:07<00:11,  3.59it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 27/68 [00:07<00:11,  3.54it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 28/68 [00:07<00:11,  3.45it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 29/68 [00:08<00:11,  3.53it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 30/68 [00:08<00:10,  3.50it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 31/68 [00:08<00:10,  3.53it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 32/68 [00:08<00:09,  3.62it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 33/68 [00:09<00:09,  3.50it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 34/68 [00:09<00:09,  3.55it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 35/68 [00:09<00:09,  3.45it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 36/68 [00:10<00:09,  3.55it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 37/68 [00:10<00:08,  3.51it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 38/68 [00:10<00:08,  3.56it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 39/68 [00:10<00:08,  3.52it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 40/68 [00:11<00:08,  3.44it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 41/68 [00:11<00:07,  3.50it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 42/68 [00:11<00:07,  3.56it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 43/68 [00:12<00:06,  3.59it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 44/68 [00:12<00:06,  3.54it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 45/68 [00:12<00:06,  3.42it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 46/68 [00:13<00:06,  3.46it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 47/68 [00:13<00:06,  3.42it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 48/68 [00:13<00:05,  3.34it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 49/68 [00:13<00:05,  3.43it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 50/68 [00:14<00:05,  3.36it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 51/68 [00:14<00:05,  3.34it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 52/68 [00:14<00:04,  3.44it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 53/68 [00:15<00:04,  3.41it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 54/68 [00:15<00:04,  3.38it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 55/68 [00:15<00:03,  3.47it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 56/68 [00:15<00:03,  3.53it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 57/68 [00:16<00:03,  3.54it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 58/68 [00:16<00:02,  3.58it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 59/68 [00:16<00:02,  3.54it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 60/68 [00:17<00:02,  3.47it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 61/68 [00:17<00:02,  3.46it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 62/68 [00:17<00:01,  3.42it/s]\u001b[A\n",
            "Predicting batches:  93%| | 63/68 [00:17<00:01,  3.42it/s]\u001b[A\n",
            "Predicting batches:  94%| | 64/68 [00:18<00:01,  3.49it/s]\u001b[A\n",
            "Predicting batches:  96%| | 65/68 [00:18<00:00,  3.52it/s]\u001b[A\n",
            "Predicting batches:  97%|| 66/68 [00:18<00:00,  3.55it/s]\u001b[A\n",
            "Predicting batches:  99%|| 67/68 [00:19<00:00,  3.62it/s]\u001b[A\n",
            "Predicting batches: 100%|| 68/68 [00:19<00:00,  3.64it/s]\u001b[A\n",
            "Processing subjects:  88%|| 50/57 [09:55<05:44, 49.24s/it, professional psychol\u001b[A\n",
            "Formatting batches:   0%|                               | 0/612 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:   8%|                   | 50/612 [00:00<00:01, 494.47it/s]\u001b[A\n",
            "Formatting batches:  17%|                | 103/612 [00:00<00:00, 513.34it/s]\u001b[A\n",
            "Formatting batches:  25%|               | 156/612 [00:00<00:00, 519.49it/s]\u001b[A\n",
            "Formatting batches:  34%|             | 210/612 [00:00<00:00, 524.12it/s]\u001b[A\n",
            "Formatting batches:  43%|           | 264/612 [00:00<00:00, 529.42it/s]\u001b[A\n",
            "Formatting batches:  52%|         | 317/612 [00:00<00:00, 509.12it/s]\u001b[A\n",
            "Formatting batches:  60%|        | 369/612 [00:00<00:00, 508.63it/s]\u001b[A\n",
            "Formatting batches:  69%|      | 421/612 [00:00<00:00, 510.24it/s]\u001b[A\n",
            "Formatting batches:  77%|    | 474/612 [00:00<00:00, 513.29it/s]\u001b[A\n",
            "Formatting batches:  86%|  | 526/612 [00:01<00:00, 510.83it/s]\u001b[A\n",
            "Formatting batches:  94%| | 578/612 [00:01<00:00, 511.98it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                               | 0/153 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 1/153 [00:00<00:19,  7.65it/s]\u001b[A\n",
            "Predicting batches:   1%|                      | 2/153 [00:00<00:19,  7.77it/s]\u001b[A\n",
            "Predicting batches:   2%|                      | 3/153 [00:00<00:20,  7.20it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 4/153 [00:00<00:20,  7.22it/s]\u001b[A\n",
            "Predicting batches:   3%|                      | 5/153 [00:00<00:20,  7.11it/s]\u001b[A\n",
            "Predicting batches:   4%|                      | 6/153 [00:00<00:20,  7.09it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 7/153 [00:00<00:20,  7.13it/s]\u001b[A\n",
            "Predicting batches:   5%|                     | 8/153 [00:01<00:20,  7.21it/s]\u001b[A\n",
            "Predicting batches:   6%|                     | 9/153 [00:01<00:19,  7.45it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 10/153 [00:01<00:19,  7.38it/s]\u001b[A\n",
            "Predicting batches:   7%|                    | 11/153 [00:01<00:19,  7.24it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 12/153 [00:01<00:19,  7.15it/s]\u001b[A\n",
            "Predicting batches:   8%|                    | 13/153 [00:01<00:19,  7.16it/s]\u001b[A\n",
            "Predicting batches:   9%|                    | 14/153 [00:01<00:19,  7.23it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 15/153 [00:02<00:19,  7.16it/s]\u001b[A\n",
            "Predicting batches:  10%|                   | 16/153 [00:02<00:19,  6.97it/s]\u001b[A\n",
            "Predicting batches:  11%|                   | 17/153 [00:02<00:19,  7.10it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 18/153 [00:02<00:19,  6.94it/s]\u001b[A\n",
            "Predicting batches:  12%|                   | 19/153 [00:02<00:19,  6.94it/s]\u001b[A\n",
            "Predicting batches:  13%|                   | 20/153 [00:02<00:18,  7.15it/s]\u001b[A\n",
            "Predicting batches:  14%|                   | 21/153 [00:02<00:18,  7.31it/s]\u001b[A\n",
            "Predicting batches:  14%|                  | 22/153 [00:03<00:18,  7.21it/s]\u001b[A\n",
            "Predicting batches:  15%|                  | 23/153 [00:03<00:18,  7.21it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 24/153 [00:03<00:17,  7.34it/s]\u001b[A\n",
            "Predicting batches:  16%|                  | 25/153 [00:03<00:17,  7.29it/s]\u001b[A\n",
            "Predicting batches:  17%|                  | 26/153 [00:03<00:16,  7.50it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 27/153 [00:03<00:17,  7.35it/s]\u001b[A\n",
            "Predicting batches:  18%|                  | 28/153 [00:03<00:16,  7.48it/s]\u001b[A\n",
            "Predicting batches:  19%|                 | 29/153 [00:03<00:16,  7.58it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 30/153 [00:04<00:16,  7.65it/s]\u001b[A\n",
            "Predicting batches:  20%|                 | 31/153 [00:04<00:15,  7.67it/s]\u001b[A\n",
            "Predicting batches:  21%|                 | 32/153 [00:04<00:15,  7.68it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 33/153 [00:04<00:16,  7.28it/s]\u001b[A\n",
            "Predicting batches:  22%|                 | 34/153 [00:04<00:16,  7.20it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 35/153 [00:04<00:16,  7.26it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 36/153 [00:04<00:15,  7.39it/s]\u001b[A\n",
            "Predicting batches:  24%|                | 37/153 [00:05<00:15,  7.50it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 38/153 [00:05<00:15,  7.56it/s]\u001b[A\n",
            "Predicting batches:  25%|                | 39/153 [00:05<00:15,  7.44it/s]\u001b[A\n",
            "Predicting batches:  26%|                | 40/153 [00:05<00:15,  7.42it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 41/153 [00:05<00:14,  7.50it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 42/153 [00:05<00:14,  7.41it/s]\u001b[A\n",
            "Predicting batches:  28%|               | 43/153 [00:05<00:15,  7.25it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 44/153 [00:06<00:14,  7.29it/s]\u001b[A\n",
            "Predicting batches:  29%|               | 45/153 [00:06<00:14,  7.42it/s]\u001b[A\n",
            "Predicting batches:  30%|               | 46/153 [00:06<00:14,  7.52it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 47/153 [00:06<00:14,  7.42it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 48/153 [00:06<00:14,  7.25it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 49/153 [00:06<00:14,  7.22it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 50/153 [00:06<00:14,  7.27it/s]\u001b[A\n",
            "Predicting batches:  33%|              | 51/153 [00:06<00:13,  7.41it/s]\u001b[A\n",
            "Predicting batches:  34%|              | 52/153 [00:07<00:13,  7.25it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 53/153 [00:07<00:13,  7.28it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 54/153 [00:07<00:13,  7.31it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 55/153 [00:07<00:13,  7.28it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 56/153 [00:07<00:13,  7.20it/s]\u001b[A\n",
            "Predicting batches:  37%|             | 57/153 [00:07<00:13,  7.25it/s]\u001b[A\n",
            "Predicting batches:  38%|             | 58/153 [00:07<00:13,  7.30it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 59/153 [00:08<00:12,  7.27it/s]\u001b[A\n",
            "Predicting batches:  39%|             | 60/153 [00:08<00:12,  7.46it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 61/153 [00:08<00:12,  7.43it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 62/153 [00:08<00:12,  7.42it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 63/153 [00:08<00:12,  7.42it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 64/153 [00:08<00:11,  7.53it/s]\u001b[A\n",
            "Predicting batches:  42%|            | 65/153 [00:08<00:11,  7.49it/s]\u001b[A\n",
            "Predicting batches:  43%|            | 66/153 [00:09<00:11,  7.55it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 67/153 [00:09<00:11,  7.61it/s]\u001b[A\n",
            "Predicting batches:  44%|            | 68/153 [00:09<00:11,  7.65it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 69/153 [00:09<00:11,  7.40it/s]\u001b[A\n",
            "Predicting batches:  46%|            | 70/153 [00:09<00:11,  7.33it/s]\u001b[A\n",
            "Predicting batches:  46%|           | 71/153 [00:09<00:11,  7.09it/s]\u001b[A\n",
            "Predicting batches:  47%|           | 72/153 [00:09<00:11,  7.26it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 73/153 [00:09<00:11,  7.16it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 74/153 [00:10<00:10,  7.32it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 75/153 [00:10<00:10,  7.44it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 76/153 [00:10<00:10,  7.43it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 77/153 [00:10<00:10,  7.52it/s]\u001b[A\n",
            "Predicting batches:  51%|          | 78/153 [00:10<00:10,  7.35it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 79/153 [00:10<00:10,  7.07it/s]\u001b[A\n",
            "Predicting batches:  52%|          | 80/153 [00:10<00:10,  7.28it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 81/153 [00:11<00:09,  7.31it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 82/153 [00:11<00:09,  7.33it/s]\u001b[A\n",
            "Predicting batches:  54%|          | 83/153 [00:11<00:10,  6.85it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 84/153 [00:11<00:09,  7.10it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 85/153 [00:11<00:09,  7.30it/s]\u001b[A\n",
            "Predicting batches:  56%|         | 86/153 [00:11<00:09,  7.43it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 87/153 [00:11<00:08,  7.35it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 88/153 [00:12<00:08,  7.29it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 89/153 [00:12<00:08,  7.31it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 90/153 [00:12<00:08,  7.33it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 91/153 [00:12<00:08,  7.34it/s]\u001b[A\n",
            "Predicting batches:  60%|        | 92/153 [00:12<00:08,  7.45it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 93/153 [00:12<00:07,  7.51it/s]\u001b[A\n",
            "Predicting batches:  61%|        | 94/153 [00:12<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 95/153 [00:12<00:07,  7.41it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 96/153 [00:13<00:07,  7.50it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 97/153 [00:13<00:07,  7.55it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 98/153 [00:13<00:07,  7.49it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 99/153 [00:13<00:07,  7.46it/s]\u001b[A\n",
            "Predicting batches:  65%|       | 100/153 [00:13<00:07,  7.52it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 101/153 [00:13<00:06,  7.65it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 102/153 [00:13<00:06,  7.69it/s]\u001b[A\n",
            "Predicting batches:  67%|      | 103/153 [00:14<00:06,  7.68it/s]\u001b[A\n",
            "Predicting batches:  68%|      | 104/153 [00:14<00:06,  7.59it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 105/153 [00:14<00:06,  7.53it/s]\u001b[A\n",
            "Predicting batches:  69%|      | 106/153 [00:14<00:06,  6.99it/s]\u001b[A\n",
            "Predicting batches:  70%|      | 107/153 [00:14<00:06,  7.17it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 108/153 [00:14<00:06,  7.23it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 109/153 [00:14<00:06,  7.15it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 110/153 [00:14<00:05,  7.31it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 111/153 [00:15<00:05,  7.22it/s]\u001b[A\n",
            "Predicting batches:  73%|     | 112/153 [00:15<00:05,  7.13it/s]\u001b[A\n",
            "Predicting batches:  74%|     | 113/153 [00:15<00:05,  7.05it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 114/153 [00:15<00:05,  7.02it/s]\u001b[A\n",
            "Predicting batches:  75%|     | 115/153 [00:15<00:05,  7.21it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 116/153 [00:15<00:05,  7.34it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 117/153 [00:15<00:04,  7.23it/s]\u001b[A\n",
            "Predicting batches:  77%|    | 118/153 [00:16<00:04,  7.36it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 119/153 [00:16<00:04,  7.30it/s]\u001b[A\n",
            "Predicting batches:  78%|    | 120/153 [00:16<00:04,  7.44it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 121/153 [00:16<00:04,  7.16it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 122/153 [00:16<00:04,  7.22it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 123/153 [00:16<00:04,  7.35it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 124/153 [00:16<00:03,  7.36it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 125/153 [00:17<00:03,  7.31it/s]\u001b[A\n",
            "Predicting batches:  82%|   | 126/153 [00:17<00:03,  7.27it/s]\u001b[A\n",
            "Predicting batches:  83%|   | 127/153 [00:17<00:03,  7.16it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 128/153 [00:17<00:03,  7.33it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 129/153 [00:17<00:03,  7.35it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 130/153 [00:17<00:03,  7.46it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 131/153 [00:17<00:02,  7.43it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 132/153 [00:18<00:02,  7.29it/s]\u001b[A\n",
            "Predicting batches:  87%|  | 133/153 [00:18<00:02,  7.25it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 134/153 [00:18<00:02,  7.29it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 135/153 [00:18<00:02,  7.30it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 136/153 [00:18<00:02,  7.28it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 137/153 [00:18<00:02,  7.26it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 138/153 [00:18<00:02,  7.30it/s]\u001b[A\n",
            "Predicting batches:  91%|  | 139/153 [00:18<00:01,  7.41it/s]\u001b[A\n",
            "Predicting batches:  92%| | 140/153 [00:19<00:01,  7.49it/s]\u001b[A\n",
            "Predicting batches:  92%| | 141/153 [00:19<00:01,  7.33it/s]\u001b[A\n",
            "Predicting batches:  93%| | 142/153 [00:19<00:01,  7.06it/s]\u001b[A\n",
            "Predicting batches:  93%| | 143/153 [00:19<00:01,  7.26it/s]\u001b[A\n",
            "Predicting batches:  94%| | 144/153 [00:19<00:01,  7.47it/s]\u001b[A\n",
            "Predicting batches:  95%| | 145/153 [00:19<00:01,  7.55it/s]\u001b[A\n",
            "Predicting batches:  95%| | 146/153 [00:19<00:00,  7.61it/s]\u001b[A\n",
            "Predicting batches:  96%|| 147/153 [00:20<00:00,  7.16it/s]\u001b[A\n",
            "Predicting batches:  97%|| 148/153 [00:20<00:00,  7.23it/s]\u001b[A\n",
            "Predicting batches:  97%|| 149/153 [00:20<00:00,  7.16it/s]\u001b[A\n",
            "Predicting batches:  98%|| 150/153 [00:20<00:00,  7.10it/s]\u001b[A\n",
            "Predicting batches:  99%|| 151/153 [00:20<00:00,  7.17it/s]\u001b[A\n",
            "Predicting batches:  99%|| 152/153 [00:20<00:00,  7.33it/s]\u001b[A\n",
            "Predicting batches: 100%|| 153/153 [00:20<00:00,  7.42it/s]\u001b[A\n",
            "Processing subjects:  89%|| 51/57 [10:17<04:06, 41.09s/it, public relations]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  52%|          | 57/110 [00:00<00:00, 565.01it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 1/28 [00:00<00:03,  8.55it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 3/28 [00:00<00:02, 10.10it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 5/28 [00:00<00:02,  9.51it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 6/28 [00:00<00:02,  9.46it/s]\u001b[A\n",
            "Predicting batches:  29%|                 | 8/28 [00:00<00:02,  9.98it/s]\u001b[A\n",
            "Predicting batches:  36%|              | 10/28 [00:01<00:01, 10.12it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 12/28 [00:01<00:01, 10.07it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 14/28 [00:01<00:01, 10.31it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 16/28 [00:01<00:01, 10.21it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 18/28 [00:01<00:00, 10.36it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 20/28 [00:01<00:00, 10.48it/s]\u001b[A\n",
            "Predicting batches:  79%|     | 22/28 [00:02<00:00, 10.32it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 24/28 [00:02<00:00, 10.43it/s]\u001b[A\n",
            "Predicting batches:  93%| | 26/28 [00:02<00:00, 10.53it/s]\u001b[A\n",
            "Predicting batches: 100%|| 28/28 [00:02<00:00, 11.46it/s]\u001b[A\n",
            "Processing subjects:  91%|| 52/57 [10:20<02:28, 29.63s/it, security studies]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/245 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  16%|                 | 40/245 [00:00<00:00, 397.92it/s]\u001b[A\n",
            "Formatting batches:  33%|              | 82/245 [00:00<00:00, 410.02it/s]\u001b[A\n",
            "Formatting batches:  51%|          | 124/245 [00:00<00:00, 408.65it/s]\u001b[A\n",
            "Formatting batches:  67%|      | 165/245 [00:00<00:00, 406.26it/s]\u001b[A\n",
            "Formatting batches:  84%|   | 206/245 [00:00<00:00, 406.35it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/62 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   2%|                       | 1/62 [00:00<00:18,  3.35it/s]\u001b[A\n",
            "Predicting batches:   3%|                       | 2/62 [00:00<00:18,  3.17it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 3/62 [00:00<00:18,  3.26it/s]\u001b[A\n",
            "Predicting batches:   6%|                      | 4/62 [00:01<00:17,  3.29it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 5/62 [00:01<00:17,  3.18it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 6/62 [00:01<00:17,  3.21it/s]\u001b[A\n",
            "Predicting batches:  11%|                     | 7/62 [00:02<00:16,  3.25it/s]\u001b[A\n",
            "Predicting batches:  13%|                     | 8/62 [00:02<00:16,  3.22it/s]\u001b[A\n",
            "Predicting batches:  15%|                    | 9/62 [00:02<00:16,  3.21it/s]\u001b[A\n",
            "Predicting batches:  16%|                   | 10/62 [00:03<00:16,  3.20it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 11/62 [00:03<00:15,  3.22it/s]\u001b[A\n",
            "Predicting batches:  19%|                  | 12/62 [00:03<00:15,  3.26it/s]\u001b[A\n",
            "Predicting batches:  21%|                  | 13/62 [00:04<00:15,  3.24it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 14/62 [00:04<00:15,  3.10it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 15/62 [00:04<00:14,  3.14it/s]\u001b[A\n",
            "Predicting batches:  26%|                 | 16/62 [00:04<00:14,  3.18it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 17/62 [00:05<00:14,  3.14it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 18/62 [00:05<00:13,  3.22it/s]\u001b[A\n",
            "Predicting batches:  31%|                | 19/62 [00:05<00:13,  3.23it/s]\u001b[A\n",
            "Predicting batches:  32%|               | 20/62 [00:06<00:12,  3.34it/s]\u001b[A\n",
            "Predicting batches:  34%|               | 21/62 [00:06<00:12,  3.34it/s]\u001b[A\n",
            "Predicting batches:  35%|              | 22/62 [00:06<00:12,  3.32it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 23/62 [00:07<00:12,  3.23it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 24/62 [00:07<00:11,  3.26it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 25/62 [00:07<00:11,  3.19it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 26/62 [00:08<00:11,  3.24it/s]\u001b[A\n",
            "Predicting batches:  44%|             | 27/62 [00:08<00:10,  3.27it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 28/62 [00:08<00:10,  3.24it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 29/62 [00:08<00:10,  3.22it/s]\u001b[A\n",
            "Predicting batches:  48%|           | 30/62 [00:09<00:09,  3.24it/s]\u001b[A\n",
            "Predicting batches:  50%|           | 31/62 [00:09<00:09,  3.30it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 32/62 [00:09<00:09,  3.29it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 33/62 [00:10<00:08,  3.25it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 34/62 [00:10<00:08,  3.35it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 35/62 [00:10<00:07,  3.42it/s]\u001b[A\n",
            "Predicting batches:  58%|         | 36/62 [00:11<00:07,  3.43it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 37/62 [00:11<00:07,  3.35it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 38/62 [00:11<00:07,  3.25it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 39/62 [00:11<00:06,  3.30it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 40/62 [00:12<00:06,  3.32it/s]\u001b[A\n",
            "Predicting batches:  66%|       | 41/62 [00:12<00:06,  3.23it/s]\u001b[A\n",
            "Predicting batches:  68%|       | 42/62 [00:12<00:06,  3.29it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 43/62 [00:13<00:05,  3.26it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 44/62 [00:13<00:05,  3.26it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 45/62 [00:13<00:05,  3.31it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 46/62 [00:14<00:04,  3.27it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 47/62 [00:14<00:04,  3.27it/s]\u001b[A\n",
            "Predicting batches:  77%|     | 48/62 [00:14<00:04,  3.29it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 49/62 [00:15<00:03,  3.28it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 50/62 [00:15<00:03,  3.27it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 51/62 [00:15<00:03,  3.29it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 52/62 [00:15<00:03,  3.26it/s]\u001b[A\n",
            "Predicting batches:  85%|   | 53/62 [00:16<00:02,  3.25it/s]\u001b[A\n",
            "Predicting batches:  87%|   | 54/62 [00:16<00:02,  3.25it/s]\u001b[A\n",
            "Predicting batches:  89%|  | 55/62 [00:16<00:02,  3.24it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 56/62 [00:17<00:01,  3.24it/s]\u001b[A\n",
            "Predicting batches:  92%| | 57/62 [00:17<00:01,  3.34it/s]\u001b[A\n",
            "Predicting batches:  94%| | 58/62 [00:17<00:01,  3.35it/s]\u001b[A\n",
            "Predicting batches:  95%| | 59/62 [00:18<00:00,  3.23it/s]\u001b[A\n",
            "Predicting batches:  97%|| 60/62 [00:18<00:00,  3.21it/s]\u001b[A\n",
            "Predicting batches:  98%|| 61/62 [00:18<00:00,  3.22it/s]\u001b[A\n",
            "Processing subjects:  93%|| 53/57 [10:40<01:46, 26.56s/it, sociology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/201 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  26%|               | 53/201 [00:00<00:00, 524.99it/s]\u001b[A\n",
            "Formatting batches:  54%|         | 109/201 [00:00<00:00, 540.02it/s]\u001b[A\n",
            "Formatting batches:  83%|   | 167/201 [00:00<00:00, 555.64it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   4%|                       | 2/51 [00:00<00:04, 10.34it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 4/51 [00:00<00:04,  9.74it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 5/51 [00:00<00:04,  9.77it/s]\u001b[A\n",
            "Predicting batches:  12%|                     | 6/51 [00:00<00:04,  9.51it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 7/51 [00:00<00:04,  9.61it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 8/51 [00:00<00:04,  9.69it/s]\u001b[A\n",
            "Predicting batches:  18%|                   | 9/51 [00:00<00:04,  9.75it/s]\u001b[A\n",
            "Predicting batches:  20%|                  | 10/51 [00:01<00:04,  9.77it/s]\u001b[A\n",
            "Predicting batches:  22%|                  | 11/51 [00:01<00:04,  9.80it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 12/51 [00:01<00:04,  9.63it/s]\u001b[A\n",
            "Predicting batches:  25%|                 | 13/51 [00:01<00:03,  9.72it/s]\u001b[A\n",
            "Predicting batches:  27%|                | 14/51 [00:01<00:03,  9.47it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 15/51 [00:01<00:03,  9.42it/s]\u001b[A\n",
            "Predicting batches:  31%|               | 16/51 [00:01<00:03,  9.39it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 17/51 [00:01<00:03,  9.53it/s]\u001b[A\n",
            "Predicting batches:  35%|               | 18/51 [00:01<00:03,  9.44it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 19/51 [00:01<00:03,  9.38it/s]\u001b[A\n",
            "Predicting batches:  39%|              | 20/51 [00:02<00:03,  9.35it/s]\u001b[A\n",
            "Predicting batches:  41%|             | 21/51 [00:02<00:03,  9.33it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 22/51 [00:02<00:03,  9.30it/s]\u001b[A\n",
            "Predicting batches:  45%|            | 23/51 [00:02<00:02,  9.48it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 24/51 [00:02<00:02,  9.41it/s]\u001b[A\n",
            "Predicting batches:  49%|           | 25/51 [00:02<00:02,  9.37it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 26/51 [00:02<00:02,  9.50it/s]\u001b[A\n",
            "Predicting batches:  53%|          | 27/51 [00:02<00:02,  9.44it/s]\u001b[A\n",
            "Predicting batches:  55%|          | 28/51 [00:02<00:02,  9.58it/s]\u001b[A\n",
            "Predicting batches:  57%|          | 29/51 [00:03<00:02,  9.69it/s]\u001b[A\n",
            "Predicting batches:  59%|         | 30/51 [00:03<00:02,  9.77it/s]\u001b[A\n",
            "Predicting batches:  61%|         | 31/51 [00:03<00:02,  9.83it/s]\u001b[A\n",
            "Predicting batches:  63%|        | 32/51 [00:03<00:01,  9.66it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 33/51 [00:03<00:01,  9.73it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 34/51 [00:03<00:01,  9.76it/s]\u001b[A\n",
            "Predicting batches:  69%|       | 35/51 [00:03<00:01,  9.78it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 36/51 [00:03<00:01,  9.82it/s]\u001b[A\n",
            "Predicting batches:  73%|      | 37/51 [00:03<00:01,  9.80it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 39/51 [00:04<00:01, 10.00it/s]\u001b[A\n",
            "Predicting batches:  78%|     | 40/51 [00:04<00:01,  9.81it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 41/51 [00:04<00:01,  9.85it/s]\u001b[A\n",
            "Predicting batches:  82%|    | 42/51 [00:04<00:00,  9.69it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 43/51 [00:04<00:00,  9.74it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 44/51 [00:04<00:00,  9.60it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 45/51 [00:04<00:00,  9.70it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 46/51 [00:04<00:00,  9.73it/s]\u001b[A\n",
            "Predicting batches:  92%| | 47/51 [00:04<00:00,  9.58it/s]\u001b[A\n",
            "Predicting batches:  94%| | 48/51 [00:04<00:00,  9.49it/s]\u001b[A\n",
            "Predicting batches:  96%| | 49/51 [00:05<00:00,  9.58it/s]\u001b[A\n",
            "Predicting batches:  98%|| 50/51 [00:05<00:00,  9.64it/s]\u001b[A\n",
            "Processing subjects:  95%|| 54/57 [10:45<01:00, 20.27s/it, us foreign policy]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  56%|         | 56/100 [00:00<00:00, 551.90it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   8%|                      | 2/25 [00:00<00:02, 10.30it/s]\u001b[A\n",
            "Predicting batches:  16%|                    | 4/25 [00:00<00:02, 10.30it/s]\u001b[A\n",
            "Predicting batches:  24%|                  | 6/25 [00:00<00:01, 10.17it/s]\u001b[A\n",
            "Predicting batches:  32%|                | 8/25 [00:00<00:01, 10.10it/s]\u001b[A\n",
            "Predicting batches:  40%|             | 10/25 [00:00<00:01, 10.17it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 12/25 [00:01<00:01, 10.18it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 14/25 [00:01<00:01, 10.07it/s]\u001b[A\n",
            "Predicting batches:  64%|        | 16/25 [00:01<00:00, 10.22it/s]\u001b[A\n",
            "Predicting batches:  72%|      | 18/25 [00:01<00:00, 10.33it/s]\u001b[A\n",
            "Predicting batches:  80%|    | 20/25 [00:01<00:00, 10.19it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 22/25 [00:02<00:00, 10.23it/s]\u001b[A\n",
            "Predicting batches:  96%| | 24/25 [00:02<00:00, 10.13it/s]\u001b[A\n",
            "Processing subjects:  96%|| 55/57 [10:48<00:29, 14.99s/it, virology]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/166 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  36%|             | 59/166 [00:00<00:00, 584.04it/s]\u001b[A\n",
            "Formatting batches:  71%|     | 118/166 [00:00<00:00, 582.31it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                      | 2/42 [00:00<00:02, 13.47it/s]\u001b[A\n",
            "Predicting batches:  10%|                     | 4/42 [00:00<00:02, 13.32it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/42 [00:00<00:02, 12.95it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/42 [00:00<00:02, 12.85it/s]\u001b[A\n",
            "Predicting batches:  24%|                 | 10/42 [00:00<00:02, 12.89it/s]\u001b[A\n",
            "Predicting batches:  29%|                | 12/42 [00:00<00:02, 12.45it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/42 [00:01<00:02, 12.34it/s]\u001b[A\n",
            "Predicting batches:  38%|              | 16/42 [00:01<00:02, 12.54it/s]\u001b[A\n",
            "Predicting batches:  43%|             | 18/42 [00:01<00:01, 12.52it/s]\u001b[A\n",
            "Predicting batches:  48%|            | 20/42 [00:01<00:01, 12.49it/s]\u001b[A\n",
            "Predicting batches:  52%|           | 22/42 [00:01<00:01, 12.65it/s]\u001b[A\n",
            "Predicting batches:  57%|         | 24/42 [00:01<00:01, 12.74it/s]\u001b[A\n",
            "Predicting batches:  62%|        | 26/42 [00:02<00:01, 12.74it/s]\u001b[A\n",
            "Predicting batches:  67%|       | 28/42 [00:02<00:01, 12.43it/s]\u001b[A\n",
            "Predicting batches:  71%|      | 30/42 [00:02<00:00, 12.59it/s]\u001b[A\n",
            "Predicting batches:  76%|     | 32/42 [00:02<00:00, 12.54it/s]\u001b[A\n",
            "Predicting batches:  81%|    | 34/42 [00:02<00:00, 12.29it/s]\u001b[A\n",
            "Predicting batches:  86%|   | 36/42 [00:02<00:00, 12.44it/s]\u001b[A\n",
            "Predicting batches:  90%|  | 38/42 [00:03<00:00, 12.60it/s]\u001b[A\n",
            "Predicting batches:  95%| | 40/42 [00:03<00:00, 11.05it/s]\u001b[A\n",
            "Predicting batches: 100%|| 42/42 [00:03<00:00, 12.07it/s]\u001b[A\n",
            "Processing subjects:  98%|| 56/57 [10:52<00:11, 11.59s/it, world religions]\u001b[A\n",
            "Formatting batches:   0%|                               | 0/171 [00:00<?, ?it/s]\u001b[A\n",
            "Formatting batches:  37%|             | 63/171 [00:00<00:00, 620.64it/s]\u001b[A\n",
            "Formatting batches:  74%|     | 126/171 [00:00<00:00, 621.11it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "Predicting batches:   0%|                                | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Predicting batches:   5%|                       | 2/43 [00:00<00:02, 16.99it/s]\u001b[A\n",
            "Predicting batches:   9%|                     | 4/43 [00:00<00:02, 16.69it/s]\u001b[A\n",
            "Predicting batches:  14%|                    | 6/43 [00:00<00:02, 16.51it/s]\u001b[A\n",
            "Predicting batches:  19%|                   | 8/43 [00:00<00:02, 15.97it/s]\u001b[A\n",
            "Predicting batches:  23%|                 | 10/43 [00:00<00:01, 16.55it/s]\u001b[A\n",
            "Predicting batches:  28%|                | 12/43 [00:00<00:01, 16.44it/s]\u001b[A\n",
            "Predicting batches:  33%|               | 14/43 [00:00<00:01, 16.41it/s]\u001b[A\n",
            "Predicting batches:  37%|              | 16/43 [00:00<00:01, 16.00it/s]\u001b[A\n",
            "Predicting batches:  42%|             | 18/43 [00:01<00:01, 16.11it/s]\u001b[A\n",
            "Predicting batches:  47%|            | 20/43 [00:01<00:01, 15.79it/s]\u001b[A\n",
            "Predicting batches:  51%|           | 22/43 [00:01<00:01, 15.58it/s]\u001b[A\n",
            "Predicting batches:  56%|          | 24/43 [00:01<00:01, 15.47it/s]\u001b[A\n",
            "Predicting batches:  60%|         | 26/43 [00:01<00:01, 15.70it/s]\u001b[A\n",
            "Predicting batches:  65%|        | 28/43 [00:01<00:00, 16.28it/s]\u001b[A\n",
            "Predicting batches:  70%|       | 30/43 [00:01<00:00, 16.29it/s]\u001b[A\n",
            "Predicting batches:  74%|      | 32/43 [00:01<00:00, 15.92it/s]\u001b[A\n",
            "Predicting batches:  79%|    | 34/43 [00:02<00:00, 16.39it/s]\u001b[A\n",
            "Predicting batches:  84%|   | 36/43 [00:02<00:00, 16.34it/s]\u001b[A\n",
            "Predicting batches:  88%|  | 38/43 [00:02<00:00, 15.99it/s]\u001b[A\n",
            "Predicting batches:  93%| | 40/43 [00:02<00:00, 16.09it/s]\u001b[A\n",
            "Predicting batches:  98%|| 42/43 [00:02<00:00, 16.16it/s]\u001b[A\n",
            "Processing subjects: 100%|| 57/57 [10:55<00:00, 11.49s/it, world religions]\u001b[A\n",
            "        Average: 58.41\n",
            "           STEM: 49.30\n",
            "Social Sciences: 67.21\n",
            "     Humanities: 54.79\n",
            "          Other: 63.79\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli eval instructiontuning_lora_eval.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "116c038fc0764a118b3381aac5e0828d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17639ae42259456dbca3ae2be58619cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5407a70de8419693df1d22d5c8bdbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_c20cbf9318794caa97433b70a49a723b"
          }
        },
        "1e294998453a46eba8ae9382fd63e4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34959170cecd47ba945da1d7fd052e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cbc9094d6744182ab2ea61740393088": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e294998453a46eba8ae9382fd63e4ee",
            "placeholder": "",
            "style": "IPY_MODEL_c7239048f09e42b9b6ac6b45a9f50d8b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "62fa9b71e2e548b3a4cced2ff123aaac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a713851005ff41a8948d03631ac19577",
            "placeholder": "",
            "style": "IPY_MODEL_8af622609f484e9499e85e3f7acfe2c6",
            "value": ""
          }
        },
        "7984b2e1c4e04fb38a23bd06c6b920ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802dbb4314f246218d59a6fd4863d4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_116c038fc0764a118b3381aac5e0828d",
            "style": "IPY_MODEL_34959170cecd47ba945da1d7fd052e82",
            "value": true
          }
        },
        "8af622609f484e9499e85e3f7acfe2c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a713851005ff41a8948d03631ac19577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac45a9297c2a4c618cd5c8562faabdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c0af717f5e7048079f07f59c76288646": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17639ae42259456dbca3ae2be58619cd",
            "placeholder": "",
            "style": "IPY_MODEL_fc263c7861dd4feaa6bb4e9e2080c481",
            "value": "Connecting..."
          }
        },
        "c20cbf9318794caa97433b70a49a723b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c682d4a8f72f409c8be7343fb8599dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7239048f09e42b9b6ac6b45a9f50d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8d6e5dd15cb462da8d97c99dcd13fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c682d4a8f72f409c8be7343fb8599dcd",
            "style": "IPY_MODEL_ac45a9297c2a4c618cd5c8562faabdfc",
            "tooltip": ""
          }
        },
        "fb79b552a71f460c890e843a8b692fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7984b2e1c4e04fb38a23bd06c6b920ef",
            "placeholder": "",
            "style": "IPY_MODEL_ff4c6269b65f4433906a447e5d420c3a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "fc263c7861dd4feaa6bb4e9e2080c481": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff4c6269b65f4433906a447e5d420c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
